Video Title: Building makemore Part 4: Becoming a Backprop Ninja
Video ID: q8SA3rM6ckI
URL: https://www.youtube.com/watch?v=q8SA3rM6ckI
View Count: 289,517

[00:00] hi everyone so today we are once again
[00:02] continuing our implementation of make
[00:04] more now so far we've come up to here
[00:07] montalia perceptrons and our neural net
[00:09] looked like this and we were
[00:11] implementing this over the last few
[00:12] lectures
[00:13] now I'm sure everyone is very excited to
[00:15] go into recurring neural networks and
[00:16] all of their variants and how they work
[00:18] and the diagrams look cool and it's very
[00:20] exciting and interesting and we're going
[00:21] to get a better result but unfortunately
[00:23] I think we have to remain here for one
[00:25] more lecture and the reason for that is
[00:28] we've already trained this multilio
[00:30] perceptron right and we are getting
[00:31] pretty good loss and I think we have a
[00:33] pretty decent understanding of the
[00:34] architecture and how it works but the
[00:37] line of code here that I take an issue
[00:39] with is here lost up backward that is we
[00:42] are taking a pytorch auto grad and using
[00:45] it to calculate all of our gradients
[00:46] along the way and I would like to remove
[00:48] the use of lost at backward and I would
[00:50] like us to write our backward pass
[00:52] manually on the level of tensors and I
[00:55] think that this is a very useful
[00:56] exercise for the following reasons
[00:58] I actually have an entire blog post on
[01:00] this topic but I'd like to call back
[01:02] propagation a leaky abstraction
[01:05] and what I mean by that is back
[01:07] propagation does doesn't just make your
[01:09] neural networks just work magically it's
[01:11] not the case they can just Stack Up
[01:12] arbitrary Lego blocks of differentiable
[01:14] functions and just cross your fingers
[01:16] and back propagate and everything is
[01:17] great things don't just work
[01:19] automatically it is a leaky abstraction
[01:22] in the sense that you can shoot yourself
[01:23] in the foot if you do not understanding
[01:25] its internals it will magically not work
[01:28] or not work optimally and you will need
[01:31] to understand how it works under the
[01:32] hood if you're hoping to debug it and if
[01:34] you are hoping to address it in your
[01:36] neural nut
[01:37] um so this blog post here from a while
[01:39] ago goes into some of those examples so
[01:42] for example we've already covered them
[01:43] some of them already for example the
[01:46] flat tails of these functions and how
[01:48] you do not want to saturate them too
[01:51] much because your gradients will die the
[01:53] case of dead neurons which I've already
[01:55] covered as well
[01:56] the case of exploding or Vanishing
[01:58] gradients in the case of repair neural
[02:00] networks which we are about to cover
[02:02] and then also you will often come across
[02:05] some examples in the wild
[02:07] this is a snippet that I found uh in a
[02:10] random code base on the internet where
[02:11] they actually have like a very subtle
[02:13] but pretty major bug in their
[02:15] implementation and the bug points at the
[02:18] fact that the author of this code does
[02:20] not actually understand by propagation
[02:21] so they're trying to do here is they're
[02:23] trying to clip the loss at a certain
[02:25] maximum value but actually what they're
[02:27] trying to do is they're trying to
[02:28] collect the gradients to have a maximum
[02:30] value instead of trying to clip the loss
[02:32] at a maximum value and
[02:34] um indirectly they're basically causing
[02:36] some of the outliers to be actually
[02:38] ignored because when you clip a loss of
[02:41] an outlier you are setting its gradient
[02:43] to zero and so have a look through this
[02:46] and read through it but there's
[02:48] basically a bunch of subtle issues that
[02:50] you're going to avoid if you actually
[02:51] know what you're doing and that's why I
[02:53] don't think it's the case that because
[02:55] pytorch or other Frameworks offer
[02:56] autograd it is okay for us to ignore how
[02:59] it works
[03:00] now we've actually already covered
[03:02] covered autograd and we wrote micrograd
[03:04] but micrograd was an autograd engine
[03:07] only on the level of individual scalars
[03:09] so the atoms were single individual
[03:11] numbers and uh you know I don't think
[03:13] it's enough and I'd like us to basically
[03:14] think about back propagation on level of
[03:16] tensors as well and so in a summary I
[03:19] think it's a good exercise I think it is
[03:21] very very valuable you're going to
[03:23] become better at debugging neural
[03:25] networks and making sure that you
[03:27] understand what you're doing it is going
[03:28] to make everything fully explicit so
[03:30] you're not going to be nervous about
[03:31] what is hidden away from you and
[03:33] basically in general we're going to
[03:34] emerge stronger and so let's get into it
[03:37] a bit of a fun historical note here is
[03:40] that today writing your backward pass by
[03:42] hand and manually is not recommended and
[03:43] no one does it except for the purposes
[03:45] of exercise but about 10 years ago in
[03:48] deep learning this was fairly standard
[03:49] and in fact pervasive so at the time
[03:52] everyone used to write their own
[03:53] backward pass by hand manually including
[03:55] myself and it's just what you would do
[03:57] so we used to ride backward pass by hand
[03:59] and now everyone just calls lost that
[04:01] backward uh we've lost something I want
[04:04] to give you a few examples of this so
[04:07] here's a 2006 paper from Jeff Hinton and
[04:11] Russell selectinov in science that was
[04:13] influential at the time and this was
[04:15] training some architectures called
[04:17] restricted bolstery machines and
[04:19] basically it's an auto encoder trained
[04:22] here and this is from roughly 2010 I had
[04:26] a library for training researchable
[04:27] machines and this was at the time
[04:30] written in Matlab so python was not used
[04:32] for deep learning pervasively it was all
[04:34] Matlab and Matlab was this a scientific
[04:36] Computing package that everyone would
[04:39] use so we would write Matlab which is
[04:41] barely a programming language as well
[04:44] but I've had a very convenient tensor
[04:46] class and was this a Computing
[04:48] environment and you would run here it
[04:49] would all run on a CPU of course but you
[04:51] would have very nice plots to go with it
[04:53] and a built-in debugger and it was
[04:54] pretty nice now the code in this package
[04:57] in 2010 that I wrote for fitting
[05:00] research multiple machines to a large
[05:03] extent is recognizable but I wanted to
[05:05] show you how you would well I'm creating
[05:07] the data in the XY batches I'm
[05:09] initializing the neural nut so it's got
[05:11] weights and biases just like we're used
[05:13] to and then this is the training Loop
[05:15] where we actually do the forward pass
[05:17] and then here at this time they didn't
[05:19] even necessarily use back propagation to
[05:21] train neural networks so this in
[05:23] particular implements contrastive
[05:25] Divergence which estimates a gradient
[05:28] and then here we take that gradient and
[05:30] use it for a parameter update along the
[05:32] lines that we're used to
[05:34] um yeah here
[05:36] but you can see that basically people
[05:38] are meddling with these gradients uh
[05:39] directly and inline and themselves uh it
[05:41] wasn't that common to use an auto grad
[05:43] engine here's one more example from a
[05:45] paper of mine from 2014
[05:47] um called the fragmented embeddings
[05:49] and here what I was doing is I was
[05:51] aligning images and text
[05:53] um and so it's kind of like a clip if
[05:55] you're familiar with it but instead of
[05:56] working on the level of entire images
[05:58] and entire sentences it was working on
[06:00] the level of individual objects and
[06:01] little pieces of sentences and I was
[06:03] embedding them and then calculating very
[06:05] much like a clip-like loss and I dig up
[06:08] the code from 2014 of how I implemented
[06:10] this and it was already in numpy and
[06:13] python
[06:14] and here I'm planting the cost function
[06:16] and it was standard to implement not
[06:19] just the cost but also the backward pass
[06:20] manually so here I'm calculating the
[06:23] image embeddings sentence embeddings the
[06:26] loss function I calculate this course
[06:28] this is the loss function and then once
[06:31] I have the loss function I do the
[06:32] backward pass right here so I backward
[06:34] through the loss function and through
[06:36] the neural nut and I append
[06:38] regularization so everything was done by
[06:41] hand manually and you were just right
[06:42] out the backward pass and then you would
[06:44] use a gradient Checker to make sure that
[06:46] your numerical estimate of the gradient
[06:47] agrees with the one you calculated
[06:49] during back propagation so this was very
[06:51] standard for a long time but today of
[06:53] course it is standard to use an auto
[06:55] grad engine
[06:56] um but it was definitely useful and I
[06:58] think people sort of understood how
[06:59] these neural networks work on a very
[07:01] intuitive level and so I think it's a
[07:03] good exercise again and this is where we
[07:04] want to be okay so just as a reminder
[07:06] from our previous lecture this is The
[07:08] jupyter Notebook that we implemented at
[07:09] the time and
[07:11] we're going to keep everything the same
[07:13] so we're still going to have a two layer
[07:15] multiplayer perceptron with a batch
[07:16] normalization layer so the forward pass
[07:18] will be basically identical to this
[07:20] lecture but here we're going to get rid
[07:22] of lost and backward and instead we're
[07:23] going to write the backward pass
[07:24] manually
[07:26] now here's the starter code for this
[07:27] lecture we are becoming a back prop
[07:29] ninja in this notebook
[07:31] and the first few cells here are
[07:34] identical to what we are used to so we
[07:36] are doing some imports loading the data
[07:37] set and processing the data set none of
[07:40] this changed
[07:41] now here I'm introducing a utility
[07:43] function that we're going to use later
[07:44] to compare the gradients so in
[07:46] particular we are going to have the
[07:47] gradients that we estimate manually
[07:49] ourselves and we're going to have
[07:50] gradients that Pi torch calculates and
[07:53] we're going to be checking for
[07:54] correctness assuming of course that
[07:55] pytorch is correct
[07:58] um then here we have the initialization
[08:00] that we are quite used to so we have our
[08:03] embedding table for the characters the
[08:05] first layer second layer and the batch
[08:06] normalization in between
[08:08] and here's where we create all the
[08:09] parameters now you will note that I
[08:11] changed the initialization a little bit
[08:13] uh to be small numbers so normally you
[08:16] would set the biases to be all zero here
[08:18] I am setting them to be small random
[08:20] numbers and I'm doing this because
[08:22] if your variables are initialized to
[08:24] exactly zero sometimes what can happen
[08:26] is that can mask an incorrect
[08:28] implementation of a gradient
[08:30] um because uh when everything is zero it
[08:32] sort of like simplifies and gives you a
[08:34] much simpler expression of the gradient
[08:35] than you would otherwise get and so by
[08:37] making it small numbers I'm trying to
[08:39] unmask those potential errors in these
[08:41] calculations
[08:43] you also notice that I'm using uh B1 in
[08:46] the first layer I'm using a bias despite
[08:48] batch normalization right afterwards
[08:50] um so this would typically not be what
[08:52] you do because we talked about the fact
[08:54] that you don't need the bias but I'm
[08:55] doing this here just for fun
[08:57] um because we're going to have a
[08:58] gradient with respect to it and we can
[09:00] check that we are still calculating it
[09:01] correctly even though this bias is
[09:03] asparious
[09:05] so here I'm calculating a single batch
[09:07] and then here I'm doing a forward pass
[09:10] now you'll notice that the forward pass
[09:11] is significantly expanded from what we
[09:13] are used to here the forward pass was
[09:15] just
[09:16] um here
[09:17] now the reason that the forward pass is
[09:19] longer is for two reasons number one
[09:22] here we just had an F dot cross entropy
[09:24] but here I am bringing back a explicit
[09:26] implementation of the loss function
[09:28] and number two
[09:29] I've broken up the implementation into
[09:32] manageable chunks so we have a lot a lot
[09:35] more intermediate tensors along the way
[09:37] in the forward pass and that's because
[09:38] we are about to go backwards and
[09:40] calculate the gradients in this back
[09:42] propagation from the bottom to the top
[09:45] so we're going to go upwards and just
[09:48] like we have for example the lock props
[09:49] tensor in a forward pass in the backward
[09:51] pass we're going to have a d-lock probes
[09:53] which is going to store the derivative
[09:55] of the loss with respect to the lock
[09:56] props tensor and so we're going to be
[09:58] prepending D to every one of these
[10:00] tensors and calculating it along the way
[10:02] of this back propagation
[10:04] so as an example we have a b and raw
[10:07] here we're going to be calculating a DB
[10:09] in raw so here I'm telling pytorch that
[10:12] we want to retain the grad of all these
[10:14] intermediate values because here in
[10:16] exercise one we're going to calculate
[10:18] the backward pass so we're going to
[10:20] calculate all these D values D variables
[10:22] and use the CNP function I've introduced
[10:25] above to check our correctness with
[10:26] respect to what pi torch is telling us
[10:29] this is going to be exercise one uh
[10:31] where we sort of back propagate through
[10:32] this entire graph
[10:34] now just to give you a very quick
[10:36] preview of what's going to happen in
[10:37] exercise two and below here we have
[10:40] fully broken up the loss and back
[10:43] propagated through it manually in all
[10:45] the little Atomic pieces that make it up
[10:47] but here we're going to collapse the
[10:49] laws into a single cross-entropy call
[10:50] and instead we're going to analytically
[10:53] derive using math and paper and pencil
[10:56] the gradient of the loss with respect to
[10:59] the logits and instead of back
[11:01] propagating through all of its little
[11:02] chunks one at a time we're just going to
[11:04] analytically derive what that gradient
[11:05] is and we're going to implement that
[11:07] which is much more efficient as we'll
[11:09] see in the in a bit
[11:10] then we're going to do the exact same
[11:12] thing for patch normalization so instead
[11:14] of breaking up bass drum into all the
[11:16] old tiny components we're going to use
[11:18] uh pen and paper and Mathematics and
[11:20] calculus to derive the gradient through
[11:22] the bachelor Bachelor layer so we're
[11:25] going to calculate the backward
[11:27] passthrough bathroom layer in a much
[11:28] more efficient expression instead of
[11:30] backward propagating through all of its
[11:31] little pieces independently
[11:33] so there's going to be exercise three
[11:36] and then in exercise four we're going to
[11:38] put it all together and this is the full
[11:40] code of training this two layer MLP and
[11:42] we're going to basically insert our
[11:44] manual back prop and we're going to take
[11:46] out lost it backward and you will
[11:48] basically see that you can get all the
[11:50] same results using fully your own code
[11:53] and the only thing we're using from
[11:55] pytorch is the torch.tensor to make the
[11:59] calculations efficient but otherwise you
[12:01] will understand fully what it means to
[12:03] forward and backward and neural net and
[12:04] train it and I think that'll be awesome
[12:06] so let's get to it
[12:08] okay so I read all the cells of this
[12:10] notebook all the way up to here and I'm
[12:13] going to erase this and I'm going to
[12:14] start implementing backward pass
[12:15] starting with d lock problems so we want
[12:18] to understand what should go here to
[12:20] calculate the gradient of the loss with
[12:22] respect to all the elements of the log
[12:23] props tensor
[12:25] now I'm going to give away the answer
[12:26] here but I wanted to put a quick note
[12:28] here that I think would be most
[12:30] pedagogically useful for you is to
[12:32] actually go into the description of this
[12:34] video and find the link to this Jupiter
[12:36] notebook you can find it both on GitHub
[12:38] but you can also find Google collab with
[12:40] it so you don't have to install anything
[12:41] you'll just go to a website on Google
[12:43] collab and you can try to implement
[12:45] these derivatives or gradients yourself
[12:47] and then if you are not able to come to
[12:50] my video and see me do it and so work in
[12:53] Tandem and try it first yourself and
[12:55] then see me give away the answer and I
[12:57] think that'll be most valuable to you
[12:59] and that's how I recommend you go
[13:00] through this lecture
[13:01] so we are starting here with d-log props
[13:03] now d-lock props will hold the
[13:06] derivative of the loss with respect to
[13:08] all the elements of log props
[13:11] what is inside log blobs the shape of
[13:13] this is 32 by 27. so it's not going to
[13:18] surprise you that D log props should
[13:19] also be an array of size 32 by 27
[13:21] because we want the derivative loss with
[13:23] respect to all of its elements so the
[13:26] sizes of those are always going to be
[13:27] equal
[13:29] now how how does log props influence the
[13:33] loss okay loss is negative block probes
[13:36] indexed with range of N and YB and then
[13:40] the mean of that now just as a reminder
[13:42] YB is just a basically an array of all
[13:47] the correct indices
[13:51] um so what we're doing here is we're
[13:52] taking the lock props array of size 32
[13:54] by 27.
[13:57] right
[13:58] and then we are going in every single
[14:00] row and in each row we are plugging
[14:03] plucking out the index eight and then 14
[14:06] and 15 and so on so we're going down the
[14:07] rows that's the iterator range of N and
[14:10] then we are always plucking out the
[14:12] index of the column specified by this
[14:15] tensor YB so in the zeroth row we are
[14:17] taking the eighth column in the first
[14:20] row we're taking the 14th column Etc and
[14:23] so log props at this plugs out
[14:26] all those
[14:28] log probabilities of the correct next
[14:30] character in a sequence
[14:32] so that's what that does and the shape
[14:34] of this or the size of it is of course
[14:36] 32 because our batch size is 32.
[14:40] so these elements get plugged out and
[14:43] then their mean and the negative of that
[14:45] becomes loss
[14:47] so I always like to work with simpler
[14:49] examples to understand the numerical
[14:52] form of derivative what's going on here
[14:55] is once we've plucked out these examples
[14:58] um we're taking the mean and then the
[15:00] negative so the loss basically
[15:02] I can write it this way is the negative
[15:04] of say a plus b plus c
[15:07] and the mean of those three numbers
[15:09] would be say negative would divide three
[15:11] that would be how we achieve the mean of
[15:13] three numbers ABC although we actually
[15:15] have 32 numbers here
[15:16] and so what is basically the loss by say
[15:20] like d a right
[15:22] well if we simplify this expression
[15:24] mathematically this is negative one over
[15:26] three of A and negative plus negative
[15:28] one over three of B
[15:30] plus negative 1 over 3 of c and so what
[15:33] is D loss by D A it's just negative one
[15:35] over three
[15:36] and so you can see that if we don't just
[15:38] have a b and c but we have 32 numbers
[15:40] then D loss by D
[15:43] um you know every one of those numbers
[15:45] is going to be one over N More generally
[15:47] because n is the um the size of the
[15:50] batch 32 in this case
[15:53] so D loss by
[15:55] um D Lock probs is negative 1 over n
[15:59] in all these places
[16:02] now what about the other elements inside
[16:04] lock problems because lock props is
[16:05] large array you see that lock problems
[16:07] at shape is 32 by 27. but only 32 of
[16:11] them participate in the loss calculation
[16:13] so what's the derivative of all the
[16:15] other most of the elements that do not
[16:18] get plucked out here
[16:20] while their loss intuitively is zero
[16:22] sorry they're gradient intuitively is
[16:24] zero and that's because they did not
[16:25] participate in the loss
[16:27] so most of these numbers inside this
[16:29] tensor does not feed into the loss and
[16:32] so if we were to change these numbers
[16:33] then the loss doesn't change which is
[16:36] the equivalent of way of saying that the
[16:38] derivative of the loss with respect to
[16:39] them is zero they don't impact it
[16:43] so here's a way to implement this
[16:45] derivative then we start out with
[16:47] torch.zeros of shape 32 by 27 or let's
[16:50] just say instead of doing this because
[16:52] we don't want to hard code numbers let's
[16:54] do torch.zeros like
[16:57] block probs so basically this is going
[16:59] to create an array of zeros exactly in
[17:00] the shape of log probs
[17:02] and then we need to set the derivative
[17:05] of negative 1 over n inside exactly
[17:07] these locations so here's what we can do
[17:09] the lock props indexed in The Identical
[17:12] way
[17:14] will be just set to negative one over
[17:16] zero divide n
[17:19] right just like we derived here
[17:22] so now let me erase all this reasoning
[17:25] and then this is the candidate
[17:27] derivative for D log props let's
[17:29] uncomment the first line and check that
[17:31] this is correct
[17:34] okay so CMP ran and let's go back to CMP
[17:39] and you see that what it's doing is it's
[17:41] calculating if
[17:42] the calculated value by us which is DT
[17:46] is exactly equal to T dot grad as
[17:48] calculated by pi torch and then this is
[17:51] making sure that all the elements are
[17:52] exactly equal and then converting this
[17:54] to a single Boolean value because we
[17:57] don't want the Boolean tensor we just
[17:58] want to Boolean value
[18:00] and then here we are making sure that
[18:02] okay if they're not exactly equal maybe
[18:04] they are approximately equal because of
[18:06] some floating Point issues but they're
[18:07] very very close
[18:09] so here we are using torch.allclose
[18:10] which has a little bit of a wiggle
[18:13] available because sometimes you can get
[18:15] very very close but if you use a
[18:17] slightly different calculation because a
[18:19] floating Point arithmetic you can get a
[18:22] slightly different result so this is
[18:24] checking if you get an approximately
[18:25] close result
[18:27] and then here we are checking the
[18:28] maximum uh basically the value that has
[18:31] the highest difference and what is the
[18:34] difference in the absolute value
[18:35] difference between those two and so we
[18:37] are printing whether we have an exact
[18:39] equality an approximate equality and
[18:42] what is the largest difference
[18:45] and so here
[18:46] we see that we actually have exact
[18:48] equality and so therefore of course we
[18:50] also have an approximate equality and
[18:52] the maximum difference is exactly zero
[18:54] so basically our d-log props is exactly
[18:57] equal to what pytors calculated to be
[19:00] lockprops.grad in its back propagation
[19:03] so so far we're working pretty well okay
[19:06] so let's now continue our back
[19:07] propagation
[19:08] we have that lock props depends on
[19:10] probes through a log
[19:12] so all the elements of probes are being
[19:14] element wise applied log to
[19:17] now if we want deep props then then
[19:19] remember your micrograph training
[19:22] we have like a log node it takes in
[19:24] probs and creates log probs and the
[19:27] props will be the local derivative of
[19:30] that individual Operation Log times the
[19:33] derivative loss with respect to its
[19:34] output which in this case is D log props
[19:37] so what is the local derivative of this
[19:39] operation well we are taking log element
[19:41] wise and we can come here and we can see
[19:43] well from alpha is your friend that d by
[19:45] DX of log of x is just simply one of our
[19:47] X
[19:48] so therefore in this case X is problems
[19:51] so we have d by DX is one over X which
[19:54] is one of our probes and then this is
[19:56] the local derivative and then times we
[19:58] want to chain it
[20:00] so this is chain rule
[20:01] times do log props
[20:03] let me uncomment this and let me run the
[20:06] cell in place and we see that the
[20:08] derivative of props as we calculated
[20:10] here is exactly correct
[20:12] and so notice here how this works probes
[20:15] that are props is going to be inverted
[20:18] and then element was multiplied here
[20:20] so if your probes is very very close to
[20:23] one that means you are your network is
[20:25] currently predicting the character
[20:26] correctly then this will become one over
[20:28] one and D log probes just gets passed
[20:30] through
[20:31] but if your probabilities are
[20:33] incorrectly assigned so if the correct
[20:35] character here is getting a very low
[20:37] probability then 1.0 dividing by it will
[20:41] boost this
[20:43] and then multiply by the log props so
[20:45] basically what this line is doing
[20:46] intuitively is it's taking the examples
[20:49] that have a very low probability
[20:50] currently assigned and it's boosting
[20:52] their gradient uh you can you can look
[20:55] at it that way next up is Count some imp
[20:59] so we want the river of this now let me
[21:02] just pause here and kind of introduce
[21:05] What's Happening Here in general because
[21:06] I know it's a little bit confusing we
[21:08] have the locusts that come out of the
[21:09] neural nut here what I'm doing is I'm
[21:11] finding the maximum in each row and I'm
[21:15] subtracting it for the purposes of
[21:16] numerical stability and we talked about
[21:18] how if you do not do this you run
[21:20] numerical issues if some of the logits
[21:22] take on two large values because we end
[21:24] up exponentiating them
[21:26] so this is done just for safety
[21:28] numerically then here's the
[21:30] exponentiation of all the sort of like
[21:32] logits to create our accounts and then
[21:35] we want to take the some of these counts
[21:38] and normalize so that all of the probes
[21:40] sum to one
[21:41] now here instead of using one over count
[21:43] sum I use uh raised to the power of
[21:46] negative one mathematically they are
[21:47] identical I just found that there's
[21:49] something wrong with the pytorch
[21:50] implementation of the backward pass of
[21:52] division
[21:53] um and it gives like a real result but
[21:55] that doesn't happen for star star native
[21:58] one that's why I'm using this formula
[21:59] instead but basically all that's
[22:01] happening here is we got the logits
[22:04] we're going to exponentiate all of them
[22:05] and want to normalize the counts to
[22:07] create our probabilities it's just that
[22:09] it's happening across multiple lines
[22:12] so now
[22:14] here
[22:17] we want to First Take the derivative we
[22:20] want to back propagate into account
[22:21] sumiv and then into counts as well
[22:24] so what should be the count sum M now we
[22:28] actually have to be careful here because
[22:29] we have to scrutinize and be careful
[22:32] with the shapes so counts that shape and
[22:35] then count some inverse shape
[22:39] are different
[22:40] so in particular counts as 32 by 27 but
[22:43] this count sum m is 32 by 1. and so in
[22:47] this multiplication here we also have an
[22:49] implicit broadcasting that pytorch will
[22:52] do because it needs to take this column
[22:53] tensor of 32 numbers and replicate it
[22:55] horizontally 27 times to align these two
[22:58] tensors so it can do an element twice
[23:00] multiply
[23:01] so really what this looks like is the
[23:03] following using a toy example again
[23:06] what we really have here is just props
[23:08] is counts times conservative so it's a C
[23:10] equals a times B
[23:11] but a is 3 by 3 and b is just three by
[23:15] one a column tensor and so pytorch
[23:17] internally replicated this elements of B
[23:19] and it did that across all the columns
[23:22] so for example B1 which is the first
[23:24] element of B would be replicated here
[23:26] across all the columns in this
[23:27] multiplication
[23:29] and now we're trying to back propagate
[23:31] through this operation to count some m
[23:34] so when we're calculating this
[23:35] derivative
[23:37] it's important to realize that these two
[23:39] this looks like a single operation but
[23:41] actually is two operations applied
[23:44] sequentially the first operation that
[23:46] pytorch did is it took this column
[23:48] tensor and replicated it across all the
[23:52] um across all the columns basically 27
[23:54] times so that's the first operation it's
[23:55] a replication and then the second
[23:57] operation is the multiplication so let's
[23:59] first background through the
[24:01] multiplication
[24:02] if these two arrays are of the same size
[24:05] and we just have a and b of both of them
[24:08] three by three then how do we mult how
[24:11] do we back propagate through a
[24:12] multiplication so if we just have
[24:14] scalars and not tensors then if you have
[24:16] C equals a times B then what is uh the
[24:19] order of the of C with respect to B well
[24:21] it's just a and so that's the local
[24:23] derivative
[24:24] so here in our case undoing the
[24:27] multiplication and back propagating
[24:29] through just the multiplication itself
[24:30] which is element wise is going to be the
[24:33] local derivative which in this case is
[24:36] simply counts because counts is the a
[24:40] so this is the local derivative and then
[24:42] times because the chain rule D props
[24:46] so this here is the derivative or the
[24:48] gradient but with respect to replicated
[24:50] B
[24:52] but we don't have a replicated B we just
[24:54] have a single B column so how do we now
[24:56] back propagate through the replication
[24:59] and intuitively this B1 is the same
[25:02] variable and it's just reused multiple
[25:04] times
[25:04] and so you can look at it
[25:07] as being equivalent to a case we've
[25:09] encountered in micrograd
[25:10] and so here I'm just pulling out a
[25:12] random graph we used in micrograd we had
[25:14] an example where a single node
[25:17] has its output feeding into two branches
[25:19] of basically the graph until the last
[25:22] function and we're talking about how the
[25:25] correct thing to do in the backward pass
[25:26] is we need to sum all the gradients that
[25:29] arrive at any one node so across these
[25:31] different branches the gradients would
[25:33] sum
[25:34] so if a node is used multiple times the
[25:37] gradients for all of its uses sum during
[25:39] back propagation
[25:41] so here B1 is used multiple times in all
[25:44] these columns and therefore the right
[25:45] thing to do here is to sum
[25:48] horizontally across all the rows so I'm
[25:51] going to sum in
[25:52] Dimension one but we want to retain this
[25:55] Dimension so that the uh so that counts
[25:58] some end and its gradient are going to
[26:00] be exactly the same shape so we want to
[26:02] make sure that we keep them as true so
[26:04] we don't lose this dimension and this
[26:07] will make the count sum M be exactly
[26:08] shape 32 by 1.
[26:11] so revealing this comparison as well and
[26:14] running this we see that we get an exact
[26:17] match
[26:18] so this derivative is exactly correct
[26:22] and let me erase
[26:24] this now let's also back propagate into
[26:26] counts which is the other variable here
[26:29] to create probes so from props to count
[26:32] some INF we just did that let's go into
[26:33] counts as well
[26:35] so decounts will be
[26:39] the chances are a so DC by d a is just B
[26:43] so therefore it's count summative
[26:47] um and then times chain rule the props
[26:51] now councilman is three two by One D
[26:54] probs is 32 by 27.
[26:57] so
[26:59] um those will broadcast fine and will
[27:02] give us decounts there's no additional
[27:04] summation required here
[27:06] um there will be a broadcasting that
[27:08] happens in this multiply here because
[27:11] count some M needs to be replicated
[27:12] again to correctly multiply D props but
[27:16] that's going to give the correct result
[27:18] so as far as the single operation is
[27:20] concerned so we back probably go from
[27:23] props to counts but we can't actually
[27:25] check the derivative counts uh I have it
[27:29] much later on and the reason for that is
[27:31] because count sum in depends on counts
[27:34] and so there's a second Branch here that
[27:36] we have to finish because can't summon
[27:38] back propagates into account sum and
[27:40] count sum will buy properly into counts
[27:42] and so counts is a node that is being
[27:44] used twice it's used right here in two
[27:46] props and it goes through this other
[27:48] Branch through count summative
[27:50] so even though we've calculated the
[27:52] first contribution of it we still have
[27:54] to calculate the second contribution of
[27:55] it later
[27:57] okay so we're continuing with this
[27:58] Branch we have the derivative for count
[28:00] sum if now we want the derivative of
[28:02] count sum so D count sum equals what is
[28:05] the local derivative of this operation
[28:07] so this is basically an element wise one
[28:09] over counts sum
[28:11] so count sum raised to the power of
[28:13] negative one is the same as one over
[28:15] count sum if we go to all from alpha we
[28:17] see that x to the negative one D by D by
[28:20] D by DX of it is basically Negative X to
[28:23] the negative 2. right one negative one
[28:25] over squared is the same as Negative X
[28:27] to the negative two
[28:29] so D count sum here will be local
[28:32] derivative is going to be negative
[28:35] um
[28:36] counts sum to the negative two that's
[28:39] the local derivative times chain rule
[28:41] which is D count sum in
[28:46] so that's D count sum
[28:49] let's uncomment this and check that I am
[28:51] correct okay so we have perfect equality
[28:55] and there's no sketchiness going on here
[28:58] with any shapes because these are of the
[28:59] same shape okay next up we want to back
[29:02] propagate through this line we have that
[29:04] count sum it's count.sum along the rows
[29:07] so I wrote out
[29:09] um some help here we have to keep in
[29:11] mind that counts of course is 32 by 27
[29:13] and count sum is 32 by 1. so in this
[29:17] back propagation we need to take this
[29:19] column of derivatives and transform it
[29:22] into a array of derivatives
[29:24] two-dimensional array
[29:26] so what is this operation doing we're
[29:28] taking in some kind of an input like say
[29:31] a three by three Matrix a and we are
[29:32] summing up the rows into a column tells
[29:36] her B1 b2b3 that is basically this
[29:39] so now we have the derivatives of the
[29:41] loss with respect to B all the elements
[29:44] of B
[29:45] and now we want to derivative loss with
[29:47] respect to all these little A's
[29:49] so how do the B's depend on the ace is
[29:52] basically what we're after what is the
[29:54] local derivative of this operation
[29:56] well we can see here that B1 only
[29:58] depends on these elements here the
[30:01] derivative of B1 with respect to all of
[30:03] these elements down here is zero but for
[30:06] these elements here like a11 a12 Etc the
[30:09] local derivative is one right so DB 1 by
[30:13] D A 1 1 for example is one so it's one
[30:16] one and one
[30:18] so when we have the derivative of loss
[30:19] with respect to B1
[30:21] did a local derivative of B1 with
[30:23] respect to these inputs is zeros here
[30:25] but it's one on these guys
[30:27] so in the chain rule
[30:29] we have the local derivative uh times
[30:32] sort of the derivative of B1 and so
[30:35] because the local derivative is one on
[30:37] these three elements the look of them
[30:39] are multiplying the derivative of B1
[30:41] will just be the derivative of B1 and so
[30:45] you can look at it as a router basically
[30:47] an addition is a router of gradient
[30:50] whatever gradient comes from above it
[30:52] just gets routed equally to all the
[30:53] elements that participate in that
[30:55] addition
[30:56] so in this case the derivative of B1
[30:58] will just flow equally to the derivative
[31:00] of a11 a12 and a13
[31:03] . so if we have a derivative of all the
[31:05] elements of B and in this column tensor
[31:07] which is D counts sum that we've
[31:10] calculated just now
[31:11] we basically see that what that amounts
[31:14] to is all of these are now flowing to
[31:17] all these elements of a and they're
[31:19] doing that horizontally
[31:21] so basically what we want is we want to
[31:22] take the decount sum of size 30 by 1 and
[31:26] we just want to replicate it 27 times
[31:28] horizontally to create 32 by 27 array
[31:32] so there's many ways to implement this
[31:33] operation you could of course just
[31:35] replicate the tensor but I think maybe
[31:37] one clean one is that the counts is
[31:40] simply torch dot once like
[31:43] so just an two-dimensional arrays of
[31:45] ones in the shape of counts so 32 by 27
[31:49] times D counts sum so this way we're
[31:53] letting the broadcasting here basically
[31:56] implement the replication you can look
[31:58] at it that way
[31:59] but then we have to also be careful
[32:02] because decounts was already calculated
[32:05] we calculated earlier here and that was
[32:08] just the first branch and we're now
[32:09] finishing the second Branch so we need
[32:11] to make sure that these gradients add so
[32:13] plus equals
[32:14] and then here
[32:16] um let's comment out the comparison and
[32:20] let's make sure crossing fingers that we
[32:23] have the correct result so pytorch
[32:25] agrees with us on this gradient as well
[32:28] okay hopefully we're getting a hang of
[32:29] this now counts as an element-wise X of
[32:32] Norm legits so now we want D Norm logits
[32:36] and because it's an element price
[32:38] operation everything is very simple what
[32:40] is the local derivative of e to the X
[32:41] it's famously just e to the x so this is
[32:45] the local derivative
[32:48] that is the local derivative now we
[32:50] already calculated it and it's inside
[32:51] counts so we may as well potentially
[32:53] just reuse counts that is the local
[32:55] derivative
[32:56] times uh D counts
[33:01] funny as that looks constant decount is
[33:04] derivative on the normal objects and now
[33:07] let's erase this and let's verify and it
[33:10] looks good
[33:12] so that's uh normal agents
[33:14] okay so we are here on this line now the
[33:17] normal objects
[33:18] we have that and we're trying to
[33:20] calculate the logits and deloget Maxes
[33:22] so back propagating through this line
[33:25] now we have to be careful here because
[33:26] the shapes again are not the same and so
[33:29] there's an implicit broadcasting
[33:30] Happening Here
[33:32] so normal jits has this shape 32 by 27
[33:34] logist does as well but logit Maxis is
[33:37] only 32 by one so there's a broadcasting
[33:40] here in the minus
[33:42] now here I try to sort of write out a
[33:45] two example again we basically have that
[33:48] this is our C equals a minus B
[33:50] and we see that because of the shape
[33:52] these are three by three but this one is
[33:54] just a column
[33:55] and so for example every element of C we
[33:57] have to look at how it uh came to be and
[34:00] every element of C is just the
[34:01] corresponding element of a minus uh
[34:04] basically that associated b
[34:08] so it's very clear now that the
[34:10] derivatives of every one of these c's
[34:13] with respect to their inputs are one for
[34:16] the corresponding a
[34:18] and it's a negative one for the
[34:20] corresponding B
[34:22] and so therefore
[34:24] um
[34:25] the derivatives on the C will flow
[34:27] equally to the corresponding Ace and
[34:30] then also to the corresponding base but
[34:33] then in addition to that the B's are
[34:35] broadcast so we'll have to do the
[34:36] additional sum just like we did before
[34:39] and of course the derivatives for B's
[34:41] will undergo a minus because the local
[34:43] derivative here is uh negative one
[34:46] so DC three two by D B3 is negative one
[34:50] so let's just Implement that basically
[34:52] delugits will be uh exactly copying the
[34:56] derivative on normal objects
[34:58] so
[34:59] delugits equals the norm logits and I'll
[35:03] do a DOT clone for safety so we're just
[35:05] making a copy
[35:06] and then we have that the loaded Maxis
[35:09] will be the negative of the non-legits
[35:13] because of the negative sign
[35:15] and then we have to be careful because
[35:17] logic Maxis is a column
[35:20] and so just like we saw before because
[35:23] we keep replicating the same elements
[35:26] across all the columns
[35:28] then in the backward pass because we
[35:31] keep reusing this these are all just
[35:33] like separate branches of use of that
[35:35] one variable and so therefore we have to
[35:37] do a Sum along one would keep them
[35:39] equals true so that we don't destroy
[35:42] this dimension
[35:43] and then the logic Maxes will be the
[35:45] same shape now we have to be careful
[35:47] because this deloaches is not the final
[35:49] deloaches and that's because not only do
[35:52] we get gradient signal into logits
[35:54] through here but the logic Maxes as a
[35:56] function of logits and that's a second
[35:58] Branch into logits so this is not yet
[36:01] our final derivative for logits we will
[36:03] come back later for the second branch
[36:05] for now the logic Maxis is the final
[36:07] derivative so let me uncomment this CMP
[36:10] here and let's just run this
[36:12] and logit Maxes hit by torch agrees with
[36:15] us
[36:16] so that was the derivative into through
[36:19] this line
[36:21] now before we move on I want to pause
[36:22] here briefly and I want to look at these
[36:24] logic Maxes and especially their
[36:26] gradients
[36:27] we've talked previously in the previous
[36:28] lecture that the only reason we're doing
[36:31] this is for the numerical stability of
[36:33] the softmax that we are implementing
[36:34] here and we talked about how if you take
[36:37] these logents for any one of these
[36:39] examples so one row of this logit's
[36:41] tensor if you add or subtract any value
[36:44] equally to all the elements then the
[36:47] value of the probes will be unchanged
[36:49] you're not changing soft Max the only
[36:51] thing that this is doing is it's making
[36:53] sure that X doesn't overflow and the
[36:55] reason we're using a Max is because then
[36:57] we are guaranteed that each row of
[36:58] logits the highest number is zero and so
[37:01] this will be safe
[37:03] and so
[37:05] um
[37:06] basically what that has repercussions
[37:09] if it is the case that changing logit
[37:11] Maxis does not change the props and
[37:13] therefore there's not change the loss
[37:15] then the gradient on logic masses should
[37:17] be zero right because saying those two
[37:20] things is the same
[37:21] so indeed we hope that this is very very
[37:23] small numbers so indeed we hope this is
[37:25] zero now because of floating Point uh
[37:28] sort of wonkiness
[37:30] um this doesn't come out exactly zero
[37:31] only in some of the rows it does but we
[37:33] get extremely small values like one e
[37:35] negative nine or ten and so this is
[37:37] telling us that the values of loaded
[37:39] Maxes are not impacting the loss as they
[37:42] shouldn't
[37:43] it feels kind of weird to back propagate
[37:44] through this branch honestly because
[37:48] if you have any implementation of like f
[37:50] dot cross entropy and pytorch and you
[37:52] you block together all these elements
[37:54] and you're not doing the back
[37:54] propagation piece by piece then you
[37:57] would probably assume that the
[37:59] derivative through here is exactly zero
[38:01] uh so you would be sort of
[38:03] um skipping this branch because it's
[38:07] only done for numerical stability but
[38:09] it's interesting to see that even if you
[38:10] break up everything into the full atoms
[38:13] and you still do the computation as
[38:14] you'd like with respect to numerical
[38:16] stability uh the correct thing happens
[38:17] and you still get a very very small
[38:20] gradients here
[38:21] um basically reflecting the fact that
[38:23] the values of these do not matter with
[38:26] respect to the final loss
[38:27] okay so let's now continue back
[38:29] propagation through this line here we've
[38:31] just calculated the logit Maxis and now
[38:33] we want to back prop into logits through
[38:35] this second branch
[38:36] now here of course we took legits and we
[38:38] took the max along all the rows and then
[38:41] we looked at its values here now the way
[38:43] this works is that in pytorch
[38:47] this thing here
[38:49] the max returns both the values and it
[38:52] Returns the indices at which those
[38:53] values to count the maximum value
[38:55] now in the forward pass we only used
[38:57] values because that's all we needed but
[39:00] in the backward pass it's extremely
[39:01] useful to know about where those maximum
[39:04] values occurred and we have the indices
[39:06] at which they occurred and this will of
[39:08] course helps us to help us do the back
[39:10] propagation because what should the
[39:12] backward pass be here in this case we
[39:14] have the largest tensor which is 32 by
[39:16] 27 and in each row we find the maximum
[39:18] value and then that value gets plucked
[39:20] out into loaded Maxis and so intuitively
[39:24] um basically the derivative flowing
[39:27] through here then should be one
[39:31] times the look of derivatives is 1 for
[39:34] the appropriate entry that was plucked
[39:35] out
[39:36] and then times the global derivative of
[39:39] the logic axis
[39:40] so really what we're doing here if you
[39:42] think through it is we need to take the
[39:44] deloachet Maxis and we need to scatter
[39:46] it to the correct positions in these
[39:50] logits from where the maximum values
[39:52] came
[39:53] and so
[39:54] um
[39:56] I came up with one line of code sort of
[39:58] that does that let me just erase a bunch
[39:59] of stuff here so the line of uh you
[40:02] could do it kind of very similar to what
[40:03] we've done here where we create a zeros
[40:05] and then we populate uh the correct
[40:07] elements uh so we use the indices here
[40:10] and we would set them to be one but you
[40:13] can also use one hot
[40:15] so F dot one hot and then I'm taking the
[40:18] lowest of Max over the First Dimension
[40:21] dot indices and I'm telling uh pytorch
[40:24] that the dimension of every one of these
[40:27] tensors should be
[40:29] um
[40:29] 27 and so what this is going to do
[40:33] is okay I apologize this is crazy filthy
[40:37] that I am sure of this
[40:39] it's really just a an array of where the
[40:41] Maxes came from in each row and that
[40:44] element is one and the all the other
[40:45] elements are zero so it's a one-half
[40:47] Vector in each row and these indices are
[40:50] now populating a single one in the
[40:53] proper place
[40:54] and then what I'm doing here is I'm
[40:56] multiplying by the logit Maxis and keep
[40:58] in mind that this is a column
[41:01] of 32 by 1. and so when I'm doing this
[41:05] times the logic Maxis the logic Maxes
[41:08] will broadcast and that column will you
[41:10] know get replicated and in an element
[41:12] wise multiply will ensure that each of
[41:15] these just gets routed to whichever one
[41:17] of these bits is turned on
[41:19] and so that's another way to implement
[41:21] uh this kind of a this kind of a
[41:23] operation and both of these can be used
[41:26] I just thought I would show an
[41:28] equivalent way to do it and I'm using
[41:30] plus equals because we already
[41:31] calculated the logits here and this is
[41:33] not the second branch
[41:35] so let's
[41:37] look at logits and make sure that this
[41:39] is correct
[41:40] and we see that we have exactly the
[41:42] correct answer
[41:44] next up we want to continue with logits
[41:46] here that is an outcome of a matrix
[41:49] multiplication and a bias offset in this
[41:51] linear layer
[41:53] so I've printed out the shapes of all
[41:56] these intermediate tensors we see that
[41:58] logits is of course 32 by 27 as we've
[42:00] just seen
[42:01] then the H here is 32 by 64. so these
[42:05] are 64 dimensional hidden States and
[42:08] then this W Matrix projects those 64
[42:10] dimensional vectors into 27 dimensions
[42:12] and then there's a 27 dimensional offset
[42:15] which is a one-dimensional vector
[42:18] now we should note that this plus here
[42:20] actually broadcasts because H multiplied
[42:23] by by W2 will give us a 32 by 27. and so
[42:27] then this plus B2 is a 27 dimensional
[42:31] lecture here
[42:32] now in the rules of broadcasting what's
[42:33] going to happen with this bias Vector is
[42:35] that this one-dimensional Vector of 27
[42:37] will get aligned with a padded dimension
[42:41] of one on the left and it will basically
[42:43] become a row vector and then it will get
[42:45] replicated vertically 32 times to make
[42:48] it 32 by 27 and then there's an
[42:50] element-wise multiply
[42:52] now
[42:54] the question is how do we back propagate
[42:56] from logits to the hidden States the
[42:59] weight Matrix W2 and the bias B2
[43:02] and you might think that we need to go
[43:03] to some Matrix calculus and then we have
[43:07] to look up the derivative for a matrix
[43:09] multiplication but actually you don't
[43:11] have to do any of that and you can go
[43:12] back to First principles and derive this
[43:14] yourself on a piece of paper and
[43:17] specifically what I like to do and I
[43:18] what I find works well for me is you
[43:20] find a specific small example that you
[43:23] then fully write out and then in the
[43:25] process of analyzing how that individual
[43:27] small example works you will understand
[43:28] the broader pattern and you'll be able
[43:30] to generalize and write out the full
[43:32] general formula for what how these
[43:35] derivatives flow in an expression like
[43:37] this so let's try that out
[43:39] so pardon the low budget production here
[43:41] but what I've done here is I'm writing
[43:43] it out on a piece of paper really what
[43:45] we are interested in is we have a
[43:46] multiply B plus C and that creates a d
[43:50] and we have the derivative of the loss
[43:53] with respect to D and we'd like to know
[43:54] what the derivative of the losses with
[43:55] respect to a b and c
[43:57] now these here are little
[44:00] two-dimensional examples of a matrix
[44:01] multiplication Two by Two Times a two by
[44:03] two
[44:04] plus a 2 a vector of just two elements
[44:07] C1 and C2 gives me a two by two
[44:10] now notice here that I have a bias
[44:14] Vector here called C and the bisex
[44:17] vector is C1 and C2 but as I described
[44:19] over here that bias Vector will become a
[44:21] row Vector in the broadcasting and will
[44:23] replicate vertically so that's what's
[44:24] happening here as well C1 C2 is
[44:27] replicated vertically and we see how we
[44:29] have two rows of C1 C2 as a result
[44:33] so now when I say write it out I just
[44:35] mean like this basically break up this
[44:37] matrix multiplication into the actual
[44:40] thing that that's going on under the
[44:41] hood so as a result of matrix
[44:44] multiplication and how it works d11 is
[44:46] the result of a DOT product between the
[44:48] first row of a and the First Column of B
[44:51] so a11 b11 plus a12 B21 plus C1
[44:57] and so on so forth for all the other
[44:59] elements of D and once you actually
[45:02] write it out it becomes obvious this is
[45:03] just a bunch of multipliers and
[45:06] um adds and we know from micrograd how
[45:09] to differentiate multiplies and adds and
[45:11] so this is not scary anymore it's not
[45:13] just matrix multiplication it's just uh
[45:15] tedious unfortunately but this is
[45:17] completely tractable we have DL by D for
[45:20] all of these and we want DL by uh all
[45:23] these little other variables so how do
[45:25] we achieve that and how do we actually
[45:26] get the gradients okay so the low budget
[45:29] production continues here
[45:30] so let's for example derive the
[45:32] derivative of the loss with respect to
[45:34] a11
[45:36] we see here that a11 occurs twice in our
[45:38] simple expression right here right here
[45:40] and influences d11 and D12
[45:43] . so this is so what is DL by d a one
[45:46] one well it's DL by d11 times the local
[45:51] derivative of d11 which in this case is
[45:53] just b11 because that's what's
[45:55] multiplying a11 here
[45:57] so uh and likewise here the local
[46:00] derivative of D12 with respect to a11 is
[46:02] just B12 and so B12 well in the chain
[46:05] rule therefore multiply the L by d 1 2.
[46:08] and then because a11 is used both to
[46:11] produce d11 and D12 we need to add up
[46:15] the contributions of both of those sort
[46:18] of chains that are running in parallel
[46:20] and that's why we get a plus just adding
[46:22] up those two
[46:24] um those two contributions and that
[46:26] gives us DL by d a one one we can do the
[46:29] exact same analysis for the other one
[46:31] for all the other elements of a and when
[46:34] you simply write it out it's just super
[46:36] simple
[46:37] um taking of gradients on you know
[46:40] expressions like this
[46:42] you find that
[46:44] this Matrix DL by D A that we're after
[46:47] right if we just arrange all the all of
[46:49] them in the same shape as a takes so a
[46:52] is just too much Matrix so d l by D A
[46:55] here will be also just the same shape
[46:59] tester with the derivatives now so deal
[47:03] by D a11 Etc
[47:05] and we see that actually we can express
[47:06] what we've written out here as a matrix
[47:09] multiplied
[47:10] and so it just so happens that D all by
[47:13] that all of these formulas that we've
[47:15] derived here by taking gradients can
[47:17] actually be expressed as a matrix
[47:19] multiplication and in particular we see
[47:21] that it is the matrix multiplication of
[47:22] these two array matrices
[47:25] so it is the um DL by D and then Matrix
[47:30] multiplying B but B transpose actually
[47:32] so you see that B21 and b12 have changed
[47:37] place
[47:38] whereas before we had of course b11 B12
[47:41] B2 on B22 so you see that this other
[47:45] Matrix B is transposed
[47:47] and so basically what we have long story
[47:49] short just by doing very simple
[47:50] reasoning here by breaking up the
[47:52] expression in the case of a very simple
[47:54] example is that DL by d a is which is
[47:58] this is simply equal to DL by DD Matrix
[48:02] multiplied with B transpose
[48:05] so that is what we have so far now we
[48:08] also want the derivative with respect to
[48:10] um B and C now
[48:13] for B I'm not actually doing the full
[48:15] derivation because honestly it's um it's
[48:18] not deep it's just uh annoying it's
[48:20] exhausting you can actually do this
[48:22] analysis yourself you'll also find that
[48:24] if you take this these expressions and
[48:26] you differentiate with respect to b
[48:27] instead of a you will find that DL by DB
[48:30] is also a matrix multiplication in this
[48:33] case you have to take the Matrix a and
[48:35] transpose it and Matrix multiply that
[48:37] with bl by DD
[48:39] and that's what gives you a deal by DB
[48:42] and then here for the offsets C1 and C2
[48:46] if you again just differentiate with
[48:47] respect to C1 you will find an
[48:50] expression like this
[48:52] and C2 an expression like this
[48:55] and basically you'll find the DL by DC
[48:57] is simply because they're just
[48:59] offsetting these Expressions you just
[49:01] have to take the deal by DD Matrix
[49:04] of the derivatives of D and you just
[49:07] have to sum across the columns and that
[49:11] gives you the derivatives for C
[49:13] so long story short
[49:15] the backward Paths of a matrix multiply
[49:18] is a matrix multiply
[49:20] and instead of just like we had D equals
[49:22] a times B plus C in the scalar case uh
[49:25] we sort of like arrive at something very
[49:27] very similar but now uh with a matrix
[49:29] multiplication instead of a scalar
[49:31] multiplication
[49:32] so the derivative of D with respect to a
[49:36] is
[49:37] DL by DD Matrix multiplied B trespose
[49:41] and here it's a transpose multiply deal
[49:44] by DD but in both cases it's a matrix
[49:46] multiplication with the derivative and
[49:49] the other term in the multiplication
[49:53] and for C it is a sum
[49:55] now I'll tell you a secret I can never
[49:58] remember the formulas that we just
[50:00] arrived for back proper gain information
[50:01] multiplication and I can back propagate
[50:03] through these Expressions just fine and
[50:05] the reason this works is because the
[50:07] dimensions have to work out
[50:09] uh so let me give you an example say I
[50:11] want to create DH
[50:13] then what should the H be number one I
[50:16] have to know that the shape of DH must
[50:19] be the same as the shape of H
[50:21] and the shape of H is 32 by 64. and then
[50:24] the other piece of information I know is
[50:26] that DH must be some kind of matrix
[50:28] multiplication of the logits with W2
[50:32] and delojits is 32 by 27 and W2 is a 64
[50:37] by 27. there is only a single way to
[50:40] make the shape work out in this case and
[50:43] it is indeed the correct result in
[50:45] particular here H needs to be 32 by 64.
[50:48] the only way to achieve that is to take
[50:50] a deluges
[50:52] and Matrix multiply it with you see how
[50:55] I have to take W2 but I have to
[50:57] transpose it to make the dimensions work
[50:58] out
[50:59] so w to transpose and it's the only way
[51:02] to make these to Matrix multiply those
[51:04] two pieces to make the shapes work out
[51:06] and that turns out to be the correct
[51:08] formula so if we come here we want DH
[51:11] which is d a and we see that d a is DL
[51:15] by DD Matrix multiply B transpose
[51:18] so that's Delo just multiply and B is W2
[51:21] so W2 transpose which is exactly what we
[51:24] have here so there's no need to remember
[51:26] these formulas similarly now if I want
[51:30] dw2 well I know that it must be a matrix
[51:33] multiplication of D logits and H
[51:37] and maybe there's a few transpose like
[51:39] there's one transpose in there as well
[51:40] and I don't know which way it is so I
[51:42] have to come to W2 and I see that its
[51:44] shape is 64 by 27
[51:47] and that has to come from some interest
[51:49] multiplication of these two
[51:51] and so to get a 64 by 27 I need to take
[51:55] um
[51:56] H I need to transpose it
[51:59] and then I need to Matrix multiply it
[52:01] um so that will become 64 by 32 and then
[52:04] I need to make sure to multiply with the
[52:05] 32 by 27 and that's going to give me a
[52:07] 64 by 27. so I need to make sure it's
[52:09] multiplied this with the logist that
[52:11] shape just like that that's the only way
[52:13] to make the dimensions work out and just
[52:15] use matrix multiplication and if we come
[52:17] here we see that that's exactly what's
[52:19] here so a transpose a for us is H
[52:23] multiplied with deloaches
[52:25] so that's W2 and then db2
[52:30] is just the um
[52:33] vertical sum and actually in the same
[52:35] way there's only one way to make the
[52:37] shapes work out I don't have to remember
[52:38] that it's a vertical Sum along the zero
[52:40] axis because that's the only way that
[52:42] this makes sense because B2 shape is 27
[52:45] so in order to get a um delugits
[52:50] here is 30 by 27 so knowing that it's
[52:54] just sum over deloaches in some
[52:56] Direction
[52:59] that direction must be zero because I
[53:02] need to eliminate this Dimension so it's
[53:04] this
[53:06] so this is so let's kind of like the
[53:08] hacky way let me copy paste and delete
[53:10] that and let me swing over here and this
[53:13] is our backward pass for the linear
[53:14] layer uh hopefully
[53:17] so now let's uncomment
[53:19] these three and we're checking that we
[53:21] got all the three derivatives correct
[53:24] and run
[53:26] and we see that h wh and B2 are all
[53:30] exactly correct so we back propagated
[53:33] through a linear layer
[53:36] now next up we have derivative for the h
[53:39] already and we need to back propagate
[53:41] through 10h into h preact
[53:43] so we want to derive DH preact
[53:47] and here we have to back propagate
[53:48] through a 10 H and we've already done
[53:50] this in micrograd and we remember that
[53:52] 10h has a very simple backward formula
[53:54] now unfortunately if I just put in D by
[53:56] DX of 10 h of X into both from alpha it
[53:59] lets us down it tells us that it's a
[54:00] hyperbolic secant function squared of X
[54:03] it's not exactly helpful but luckily
[54:06] Google image search does not let us down
[54:08] and it gives us the simpler formula and
[54:10] in particular if you have that a is
[54:12] equal to 10 h of Z then d a by DZ by
[54:16] propagating through 10 H is just one
[54:17] minus a square and take note that 1
[54:21] minus a square a here is the output of
[54:23] the 10h not the input to the 10h Z so
[54:27] the D A by DZ is here formulated in
[54:29] terms of the output of that 10h
[54:31] and here also in Google image search we
[54:34] have the full derivation if you want to
[54:35] actually take the actual definition of
[54:38] 10h and work through the math to figure
[54:39] out 1 minus standard square of Z
[54:42] so 1 minus a square is the local
[54:45] derivative in our case that is 1 minus
[54:49] uh the output of 10 H squared which here
[54:52] is H
[54:53] so it's h squared and that is the local
[54:56] derivative and then times the chain rule
[54:58] DH
[55:00] so that is going to be our candidate
[55:02] implementation so if we come here
[55:05] and then uncomment this let's hope for
[55:08] the best
[55:09] and we have the right answer
[55:12] okay next up we have DH preact and we
[55:15] want to back propagate into the gain the
[55:17] B and raw and the B and bias
[55:19] so here this is the bathroom parameters
[55:21] being gained in bias inside the bash
[55:23] term that take the B and raw that is
[55:25] exact unit caution and then scale it and
[55:28] shift it
[55:29] and these are the parameters of The
[55:30] Bachelor now here we have a
[55:33] multiplication but it's worth noting
[55:35] that this multiply is very very
[55:36] different from this Matrix multiply here
[55:38] Matrix multiply are DOT products between
[55:41] rows and Columns of these matrices
[55:43] involved this is an element twice
[55:45] multiply so things are quite a bit
[55:46] simpler
[55:47] now we do have to be careful with some
[55:49] of the broadcasting happening in this
[55:51] line of code though so you see how BN
[55:53] gain and B and bias are 1 by 64. but H
[55:58] preact and B and raw are 32 by 64.
[56:02] so we have to be careful with that and
[56:04] make sure that all the shapes work out
[56:05] fine and that the broadcasting is
[56:06] correctly back propagated
[56:08] so in particular let's start with the B
[56:10] and Gain so DB and gain should be
[56:14] and here this is again elementorized
[56:17] multiply and whenever we have a times b
[56:19] equals c we saw that the local
[56:21] derivative here is just if this is a the
[56:23] local derivative is just the B the other
[56:25] one so the local derivative is just B
[56:27] and raw and then times chain rule
[56:31] so DH preact
[56:34] so this is the candidate gradient now
[56:38] again we have to be careful because B
[56:40] and Gain Is of size 1 by 64. but this
[56:44] here would be 32 by 64.
[56:48] and so
[56:49] um the correct thing to do in this case
[56:51] of course is that b and gain here is a
[56:53] rule Vector of 64 numbers it gets
[56:55] replicated vertically in this operation
[56:58] and so therefore the correct thing to do
[57:00] is to sum because it's being replicated
[57:03] and therefore all the gradients in each
[57:06] of the rows that are now flowing
[57:07] backwards need to sum up to that same
[57:10] tensor DB and Gain so we have to sum
[57:13] across all the zero all the examples
[57:16] basically
[57:17] which is the direction in which this
[57:19] gets replicated
[57:20] and now we have to be also careful
[57:21] because we
[57:23] um being gain is of shape 1 by 64. so in
[57:26] fact I need to keep them as true
[57:29] otherwise I would just get 64.
[57:31] now I don't actually really remember why
[57:34] the being gain and the BN bias I made
[57:36] them be 1 by 64.
[57:40] um
[57:41] but the biases B1 and B2 I just made
[57:44] them be one-dimensional vectors they're
[57:45] not two-dimensional tensors so I can't
[57:47] recall exactly why I left the gain and
[57:51] the bias as two-dimensional but it
[57:53] doesn't really matter as long as you are
[57:54] consistent and you're keeping it the
[57:55] same
[57:56] so in this case we want to keep the
[57:58] dimension so that the tensor shapes work
[58:01] next up we have B and raw so DB and raw
[58:05] will be BN gain
[58:09] multiplying
[58:11] dhreact that's our chain rule now what
[58:15] about the
[58:17] um
[58:18] dimensions of this we have to be careful
[58:20] right so DH preact is 32 by 64. B and
[58:24] gain is 1 by 64. so it will just get
[58:27] replicated and to create this
[58:29] multiplication which is the correct
[58:31] thing because in a forward pass it also
[58:33] gets replicated in just the same way
[58:35] so in fact we don't need the brackets
[58:37] here we're done
[58:38] and the shapes are already correct
[58:40] and finally for the bias
[58:43] very similar this bias here is very very
[58:46] similar to the bias we saw when you
[58:47] layer in the linear layer and we see
[58:49] that the gradients from each preact will
[58:51] simply flow into the biases and add up
[58:54] because these are just these are just
[58:55] offsets
[58:57] and so basically we want this to be DH
[58:59] preact but it needs to Sum along the
[59:01] right Dimension and in this case similar
[59:04] to the gain we need to sum across the
[59:06] zeroth dimension the examples because of
[59:09] the way that the bias gets replicated
[59:10] vertically
[59:11] and we also want to have keep them as
[59:14] true
[59:15] and so this will basically take this and
[59:17] sum it up and give us a 1 by 64.
[59:20] so this is the candidate implementation
[59:23] it makes all the shapes work
[59:25] let me bring it up down here and then
[59:28] let me uncomment these three lines
[59:32] to check that we are getting the correct
[59:33] result for all the three tensors and
[59:36] indeed we see that all of that got back
[59:38] propagated correctly so now we get to
[59:40] the batch Norm layer we see how here
[59:42] being gay and being bias are the
[59:44] parameters so the back propagation ends
[59:46] but B and raw now is the output of the
[59:50] standardization
[59:51] so here what I'm doing of course is I'm
[59:53] breaking up the batch form into
[59:54] manageable pieces so we can back
[59:55] propagate through each line individually
[59:57] but basically what's happening is BN
[60:00] mean I is the sum
[60:03] so this is the B and mean I I apologize
[60:06] for the variable naming B and diff is x
[60:10] minus mu
[60:11] B and div 2 is x minus mu squared here
[60:15] inside the variance
[60:16] B and VAR is the variance so uh Sigma
[60:20] Square this is B and bar and it's
[60:22] basically the sum of squares
[60:25] so this is the x minus mu squared and
[60:28] then the sum now you'll notice one
[60:30] departure here
[60:32] here it is normalized as 1 over m
[60:34] uh which is number of examples here I'm
[60:37] normalizing as one over n minus 1
[60:39] instead of N and this is deliberate and
[60:42] I'll come back to that in a bit when we
[60:43] are at this line it is something called
[60:45] the bezels correction
[60:47] but this is how I want it in our case
[60:51] bienvar inv then becomes basically
[60:53] bienvar plus Epsilon Epsilon is one
[60:56] negative five and then it's one over
[60:58] square root
[60:59] is the same as raising to the power of
[61:02] negative 0.5 right because 0.5 is square
[61:05] root and then negative makes it one over
[61:07] square root
[61:08] so BM Bar M is a one over this uh
[61:12] denominator here and then we can see
[61:14] that b and raw which is the X hat here
[61:16] is equal to the BN diff the numerator
[61:19] multiplied by the
[61:22] um BN bar in
[61:24] and this line here that creates pre-h
[61:27] pre-act was the last piece we've already
[61:29] back propagated through it
[61:31] so now what we want to do is we are here
[61:34] and we have B and raw and we have to
[61:35] first back propagate into B and diff and
[61:38] B and Bar M
[61:40] so now we're here and we have DB and raw
[61:43] and we need to back propagate through
[61:45] this line
[61:46] now I've written out the shapes here and
[61:49] indeed bien VAR m is a shape 1 by 64. so
[61:53] there is a broadcasting happening here
[61:55] that we have to be careful with but it
[61:57] is just an element-wise simple
[61:58] multiplication by now we should be
[62:00] pretty comfortable with that to get DB
[62:02] and diff we know that this is just B and
[62:05] varm
[62:06] multiplied with
[62:08] DP and raw
[62:11] and conversely to get dbmring
[62:15] we need to take the end if
[62:17] and multiply that by DB and raw
[62:22] so this is the candidate but of course
[62:24] we need to make sure that broadcasting
[62:26] is obeyed so in particular B and VAR M
[62:29] multiplying with DB and raw
[62:31] will be okay and give us 32 by 64 as we
[62:35] expect
[62:36] but dbm VAR inv would be taking a 32 by
[62:40] 64.
[62:42] multiplying it by 32 by 64. so this is a
[62:45] 32 by 64. but of course DB this uh B and
[62:49] VAR in is only 1 by 64. so the second
[62:52] line here needs a sum across the
[62:55] examples and because there's this
[62:57] Dimension here we need to make sure that
[63:00] keep them is true
[63:02] so this is the candidate
[63:04] let's erase this and let's swing down
[63:07] here
[63:09] and implement it and then let's comment
[63:11] out dbm barif and DB and diff
[63:16] now we'll actually notice that DB and
[63:18] diff by the way is going to be incorrect
[63:22] so when I run this
[63:24] BMR m is correct B and diff is not
[63:27] correct and this is actually expected
[63:30] because we're not done with b and diff
[63:34] so in particular when we slide here we
[63:36] see here that b and raw as a function of
[63:37] B and diff but actually B and far of is
[63:40] a function of B of R which is a function
[63:42] of B and df2 which is a function of B
[63:44] and diff
[63:45] so it comes here so bdn diff
[63:48] um these variable names are crazy I'm
[63:50] sorry it branches out into two branches
[63:53] and we've only done one branch of it we
[63:55] have to continue our back propagation
[63:57] and eventually come back to B and diff
[63:58] and then we'll be able to do a plus
[64:00] equals and get the actual card gradient
[64:02] for now it is good to verify that CMP
[64:05] also works it doesn't just lie to us and
[64:07] tell us that everything is always
[64:08] correct it can in fact detect when your
[64:11] gradient is not correct so it's that's
[64:13] good to see as well okay so now we have
[64:15] the derivative here and we're trying to
[64:17] back propagate through this line
[64:18] and because we're raising to a power of
[64:21] negative 0.5 I brought up the power rule
[64:23] and we see that basically we have that
[64:25] the BM bar will now be we bring down the
[64:28] exponent so negative 0.5 times
[64:31] uh X which is this
[64:34] and now raised to the power of negative
[64:36] 0.5 minus 1 which is negative 1.5
[64:39] now we would have to also apply a small
[64:42] chain rule here in our head because we
[64:45] need to take further the derivative of B
[64:48] and VAR with respect to this expression
[64:49] here inside the bracket but because this
[64:51] is an elementalized operation and
[64:53] everything is fairly simple that's just
[64:54] one and so there's nothing to do there
[64:57] so this is the local derivative and then
[65:00] times the global derivative to create
[65:01] the chain rule this is just times the BM
[65:04] bar have
[65:05] so this is our candidate let me bring
[65:08] this down
[65:10] and uncommon to the check
[65:14] and we see that we have the correct
[65:16] result
[65:17] now before we propagate through the next
[65:19] line I want to briefly talk about the
[65:20] note here where I'm using the bezels
[65:22] correction dividing by n minus 1 instead
[65:24] of dividing by n when I normalize here
[65:27] the sum of squares
[65:29] now you'll notice that this is departure
[65:31] from the paper which uses one over n
[65:33] instead not one over n minus one their m
[65:36] is RN
[65:38] and
[65:39] um so it turns out that there are two
[65:40] ways of estimating variance of an array
[65:43] one is the biased estimate which is one
[65:46] over n and the other one is the unbiased
[65:49] estimate which is one over n minus one
[65:51] now confusingly in the paper this is uh
[65:54] not very clearly described and also it's
[65:56] a detail that kind of matters I think
[65:58] um they are using the biased version
[66:00] training time but later when they are
[66:02] talking about the inference they are
[66:04] mentioning that when they do the
[66:06] inference they are using the unbiased
[66:08] estimate which is the n minus one
[66:10] version in
[66:12] um
[66:12] basically for inference
[66:15] and to calibrate the running mean and
[66:18] the running variance basically and so
[66:20] they they actually introduce a trained
[66:22] test mismatch where in training they use
[66:24] the biased version and in the in test
[66:26] time they use the unbiased version I
[66:28] find this extremely confusing you can
[66:30] read more about the bezels correction
[66:32] and why uh dividing by n minus one gives
[66:35] you a better estimate of the variance in
[66:37] a case where you have population size or
[66:39] samples for the population
[66:41] that are very small and that is indeed
[66:44] the case for us because we are dealing
[66:46] with many patches and these mini matches
[66:48] are a small sample of a larger
[66:50] population which is the entire training
[66:52] set and so it just turns out that if you
[66:55] just estimate it using one over n that
[66:57] actually almost always underestimates
[66:58] the variance and it is a biased
[67:00] estimator and it is advised that you use
[67:02] the unbiased version and divide by n
[67:04] minus one and you can go through this
[67:06] article here that I liked that actually
[67:08] describes the full reasoning and I'll
[67:09] link it in the video description
[67:12] now when you calculate the torture
[67:13] variance
[67:15] you'll notice that they take the
[67:16] unbiased flag whether or not you want to
[67:18] divide by n or n minus one confusingly
[67:21] they do not mention what the default is
[67:24] for unbiased but I believe unbiased by
[67:26] default is true I'm not sure why the
[67:29] docs here don't cite that
[67:31] now in The Bachelor
[67:33] 1D the documentation again is kind of
[67:35] wrong and confusing it says that the
[67:38] standard deviation is calculated via the
[67:39] biased estimator
[67:41] but this is actually not exactly right
[67:43] and people have pointed out that it is
[67:44] not right in a number of issues since
[67:46] then because actually the rabbit hole is
[67:49] deeper and they follow the paper exactly
[67:52] and they use the biased version for
[67:54] training but when they're estimating the
[67:56] running standard deviation we are using
[67:58] the unbiased version so again there's
[68:00] the train test mismatch so long story
[68:02] short I'm not a fan of trained test
[68:05] discrepancies I basically kind of
[68:07] consider
[68:08] the fact that we use the bias version
[68:10] the training time and the unbiased test
[68:13] time I basically consider this to be a
[68:14] bug and I don't think that there's a
[68:16] good reason for that it's not really
[68:18] they don't really go into the detail of
[68:19] the reasoning behind it in this paper so
[68:22] that's why I basically prefer to use the
[68:24] bestless correction in my own work
[68:26] unfortunately Bastion does not take a
[68:29] keyword argument that tells you whether
[68:30] or not you want to use the unbiased
[68:33] version of the bias version in both
[68:34] train and test and so therefore anyone
[68:36] using batch normalization basically in
[68:38] my view has a bit of a bug in the code
[68:41] um
[68:42] and this turns out to be much less of a
[68:44] problem if your batch mini batch sizes
[68:46] are a bit larger but still I just might
[68:48] kind of uh unpardable so maybe someone
[68:51] can explain why this is okay but for now
[68:53] I prefer to use the unbiased version
[68:55] consistently both during training and at
[68:57] this time and that's why I'm using one
[69:00] over n minus one here
[69:01] okay so let's now actually back
[69:03] propagate through this line
[69:05] so
[69:07] the first thing that I always like to do
[69:08] is I like to scrutinize the shapes first
[69:10] so in particular here looking at the
[69:12] shapes of what's involved I see that b
[69:14] and VAR shape is 1 by 64. so it's a row
[69:18] vector and BND if two dot shape is 32 by
[69:21] 64.
[69:22] so clearly here we're doing a sum over
[69:25] the zeroth axis to squash the first
[69:28] dimension of of the shapes here using a
[69:32] sum so that right away actually hints to
[69:35] me that there will be some kind of a
[69:36] replication or broadcasting in the
[69:38] backward pass and maybe you're noticing
[69:40] the pattern here but basically anytime
[69:42] you have a sum in the forward pass that
[69:45] turns into a replication or broadcasting
[69:47] in the backward pass along the same
[69:49] Dimension and conversely when we have a
[69:52] replication or a broadcasting in the
[69:54] forward pass that indicates a variable
[69:57] reuse and so in the backward pass that
[69:59] turns into a sum over the exact same
[70:01] dimension
[70:02] and so hopefully you're noticing that
[70:04] Duality that those two are kind of like
[70:06] the opposite of each other in the
[70:07] forward and backward pass
[70:09] now once we understand the shapes the
[70:11] next thing I like to do always is I like
[70:12] to look at a toy example in my head to
[70:15] sort of just like understand roughly how
[70:16] uh the variable the variable
[70:18] dependencies go in the mathematical
[70:19] formula
[70:21] so here we have a two-dimensional array
[70:24] of the end of two which we are scaling
[70:26] by a constant and then we are summing uh
[70:29] vertically over the columns so if we
[70:32] have a two by two Matrix a and then we
[70:33] sum over the columns and scale we would
[70:36] get a row Vector B1 B2 and B1 depends on
[70:39] a in this way whereas just sum they're
[70:42] scaled of a and B2 in this way where
[70:45] it's the second column sump and scale
[70:48] and so looking at this basically
[70:52] what we want to do now is we have the
[70:53] derivatives on B1 and B2 and we want to
[70:55] back propagate them into Ace and so it's
[70:58] clear that just differentiating in your
[70:59] head the local derivative here is one
[71:01] over n minus 1 times uh one
[71:05] uh for each one of these A's and um
[71:09] basically the derivative of B1 has to
[71:11] flow through The Columns of a
[71:13] scaled by one over n minus one
[71:16] and that's roughly What's Happening Here
[71:18] so intuitively the derivative flow tells
[71:21] us that DB and diff2
[71:24] will be the local derivative of this
[71:27] operation and there are many ways to do
[71:29] this by the way but I like to do
[71:31] something like this torch dot once like
[71:33] of bndf2 so I'll create a large array
[71:37] two-dimensional of ones
[71:39] and then I will scale it so 1.0 divided
[71:42] by n minus 1.
[71:44] so this is a array of
[71:46] um one over n minus one and that's sort
[71:49] of like the local derivative
[71:50] and now for the chain rule I will simply
[71:53] just multiply it by dbm bar
[71:58] and notice here what's going to happen
[72:00] this is 32 by 64 and this is just 1 by
[72:02] 64. so I'm letting the broadcasting do
[72:06] the replication because internally in
[72:08] pytorch basically dbnbar which is 1 by
[72:11] 64 row vector
[72:13] well in this multiplication get
[72:15] um copied vertically until the two are
[72:18] of the same shape and then there will be
[72:19] an element wise multiply and so that uh
[72:22] so that the broadcasting is basically
[72:23] doing the replication
[72:25] and I will end up with the derivatives
[72:27] of DB and diff2 here
[72:30] so this is the candidate solution let's
[72:32] bring it down here
[72:33] let's uncomment this line where we check
[72:36] it and let's hope for the best
[72:39] and indeed we see that this is the
[72:41] correct formula next up let's
[72:43] differentiate here and to be in this
[72:45] so here we have that b and diff is
[72:48] element y squared to create B and F2
[72:50] so this is a relatively simple
[72:52] derivative because it's a simple element
[72:54] wise operation so it's kind of like the
[72:56] scalar case and we have that DB and div
[72:59] should be if this is x squared then the
[73:02] derivative of this is 2x right so it's
[73:04] simply 2 times B and if that's the local
[73:07] derivative
[73:08] and then times chain Rule and the shape
[73:11] of these is the same they are of the
[73:13] same shape so times this
[73:15] so that's the backward pass for this
[73:17] variable let me bring that down here
[73:20] and now we have to be careful because we
[73:22] already calculated dbm depth right so
[73:24] this is just the end of the other uh you
[73:27] know other Branch coming back to B and
[73:30] diff
[73:30] because B and diff was already back
[73:32] propagated to way over here
[73:34] from being raw so we now completed the
[73:37] second branch and so that's why I have
[73:39] to do plus equals and if you recall we
[73:42] had an incorrect derivative for being
[73:43] diff before and I'm hoping that once we
[73:46] append this last missing piece we have
[73:48] the exact correctness so let's run
[73:51] ambient to be in div now actually shows
[73:55] the exact correct derivative
[73:57] um so that's comforting okay so let's
[74:00] now back propagate through this line
[74:01] here
[74:03] um the first thing we do of course is we
[74:04] check the shapes and I wrote them out
[74:07] here and basically the shape of this is
[74:08] 32 by 64. hpbn is the same shape
[74:12] but B and mean I is a row Vector 1 by
[74:15] 64. so this minus here will actually do
[74:17] broadcasting and so we have to be
[74:19] careful with that and as a hint to us
[74:21] again because of The Duality a
[74:23] broadcasting and the forward pass means
[74:25] a variable reuse and therefore there
[74:27] will be a sum in the backward pass
[74:30] so let's write out the backward pass
[74:31] here now
[74:33] um
[74:34] back propagate into the hpbn
[74:37] because this is these are the same shape
[74:39] then the local derivative for each one
[74:41] of the elements here is just one for the
[74:43] corresponding element in here
[74:45] so basically what this means is that the
[74:47] gradient just simply copies it's just a
[74:50] variable assignment it's quality so I'm
[74:52] just going to clone this tensor just for
[74:54] safety to create an exact copy of DB and
[74:58] div
[75:00] and then here to back propagate into
[75:01] this one what I'm inclined to do here is
[75:07] will basically be
[75:09] uh what is the local derivative well
[75:12] it's negative torch.1's like
[75:16] of the shape of uh B and diff
[75:19] right
[75:22] and then times
[75:24] the um
[75:27] the derivative here dbf
[75:32] and this here is the back propagation
[75:34] for the replicated B and mean I
[75:37] so I still have to back propagate
[75:39] through the uh replication in the
[75:42] broadcasting and I do that by doing a
[75:43] sum so I'm going to take this whole
[75:45] thing and I'm going to do a sum over the
[75:47] zeroth dimension which was the
[75:49] replication
[75:53] so if you scrutinize this by the way
[75:55] you'll notice that this is the same
[75:57] shape as that and so what I'm doing uh
[76:00] what I'm doing here doesn't actually
[76:01] make that much sense because it's just a
[76:03] array of ones multiplying DP and diff so
[76:06] in fact I can just do this
[76:10] um and that is equivalent
[76:12] so this is the candidate backward pass
[76:15] let me copy it here and then let me
[76:18] comment out this one and this one
[76:22] enter
[76:24] and it's wrong
[76:27] damn
[76:29] actually sorry this is supposed to be
[76:31] wrong and it's supposed to be wrong
[76:33] because
[76:34] we are back propagating from a b and
[76:36] diff into hpbn and but we're not done
[76:39] because B and mean I depends on hpbn and
[76:43] there will be a second portion of that
[76:44] derivative coming from this second
[76:46] Branch so we're not done yet and we
[76:48] expect it to be incorrect so there you
[76:50] go
[76:50] uh so let's now back propagate from uh B
[76:53] and mean I into hpbn
[76:56] um
[76:57] and so here again we have to be careful
[76:58] because there's a broadcasting along
[77:01] um or there's a Sum along the zeroth
[77:03] dimension so this will turn into
[77:04] broadcasting in the backward pass now
[77:06] and I'm going to go a little bit faster
[77:08] on this line because it is very similar
[77:10] to the line that we had before and
[77:12] multiplies in the past in fact
[77:14] so the hpbn
[77:18] will be
[77:20] the gradient will be scaled by 1 over n
[77:22] and then basically this gradient here on
[77:25] dbn mean I
[77:27] is going to be scaled by 1 over n and
[77:30] then it's going to flow across all the
[77:32] columns and deposit itself into the hpvn
[77:35] so what we want is this thing scaled by
[77:38] 1 over n
[77:39] only put the constant up front here
[77:43] um
[77:45] so scale down the gradient and now we
[77:47] need to replicate it across all the um
[77:51] across all the rows here so we I like to
[77:55] do that by torch.lunslike of basically
[78:00] um hpbn
[78:03] and I will let the broadcasting do the
[78:05] work of replication
[78:09] so
[78:14] like that
[78:16] so this is uh the hppn and hopefully
[78:21] we can plus equals that
[78:27] so this here is broadcasting
[78:30] um and then this is the scaling so this
[78:32] should be current
[78:33] okay
[78:35] so that completes the back propagation
[78:37] of the bathroom layer and we are now
[78:38] here let's back propagate through the
[78:40] linear layer one here now because
[78:43] everything is getting a little
[78:44] vertically crazy I copy pasted the line
[78:46] here and let's just back properly
[78:48] through this one line
[78:50] so first of course we inspect the shapes
[78:52] and we see that this is 32 by 64. MCAT
[78:56] is 32 by 30.
[78:58] W1 is 30 30 by 64 and B1 is just 64. so
[79:04] as I mentioned back propagating through
[79:06] linear layers is fairly easy just by
[79:08] matching the shapes so let's do that we
[79:11] have that dmcat
[79:14] should be
[79:15] um some matrix multiplication of dhbn
[79:18] with uh W1 and one transpose thrown in
[79:21] there so to make uh MCAT be 32 by 30
[79:28] I need to take dhpn
[79:32] 32 by 64 and multiply it by w1.
[79:36] transpose
[79:39] to get the only one I need to end up
[79:43] with 30 by 64.
[79:45] so to get that I need to take uh MCAT
[79:48] transpose
[79:51] and multiply that by
[79:53] uh dhpion
[79:58] and finally to get DB1
[80:01] this is a addition and we saw that
[80:04] basically I need to just sum the
[80:06] elements in dhpbn along some Dimension
[80:09] and to make the dimensions work out I
[80:12] need to Sum along the zeroth axis here
[80:14] to eliminate this Dimension and we do
[80:17] not keep dims
[80:19] uh so that we want to just get a single
[80:21] one-dimensional lecture of 64.
[80:23] so these are the claimed derivatives
[80:27] let me put that here and let me
[80:29] uncomment three lines and cross our
[80:32] fingers
[80:34] everything is great okay so we now
[80:36] continue almost there we have the
[80:37] derivative of MCAT and we want to
[80:39] derivative we want to back propagate
[80:41] into m
[80:43] so I again copied this line over here
[80:46] so this is the forward pass and then
[80:48] this is the shapes so remember that the
[80:51] shape here was 32 by 30 and the original
[80:53] shape of M plus 32 by 3 by 10. so this
[80:57] layer in the forward pass as you recall
[80:58] did the concatenation of these three
[81:01] 10-dimensional character vectors
[81:04] and so now we just want to undo that
[81:06] so this is actually relatively
[81:08] straightforward operation because uh the
[81:11] backward pass of the what is the view
[81:12] view is just a representation of the
[81:15] array it's just a logical form of how
[81:17] you interpret the array so let's just
[81:18] reinterpret it to be what it was before
[81:21] so in other words the end is not uh 32
[81:25] by 30. it is basically dmcat
[81:29] but if you view it as the original shape
[81:34] so just m dot shape
[81:37] uh you can you can pass in tuples into
[81:39] view
[81:40] and so this should just be okay
[81:44] we just re-represent that view and then
[81:47] we uncomment this line here and
[81:49] hopefully
[81:51] yeah so the derivative of M is correct
[81:55] so in this case we just have to
[81:56] re-represent the shape of those
[81:57] derivatives into the original View
[81:59] so now we are at the final line and the
[82:01] only thing that's left to back propagate
[82:02] through is this indexing operation here
[82:05] MSC at xB so as I did before I copy
[82:09] pasted this line here and let's look at
[82:11] the shapes of everything that's involved
[82:12] and remind ourselves how this worked
[82:15] so m.shape was 32 by 3 by 10.
[82:19] it says 32 examples and then we have
[82:22] three characters each one of them has a
[82:24] 10 dimensional embedding
[82:26] and this was achieved by taking the
[82:28] lookup table C which have 27 possible
[82:31] characters
[82:32] each of them 10 dimensional and we
[82:34] looked up
[82:35] at the rows that were specified inside
[82:39] this tensor xB
[82:41] so XB is 32 by 3 and it's basically
[82:43] giving us for each example the Identity
[82:45] or the index of which character is part
[82:49] of that example
[82:50] and so here I'm showing the first five
[82:52] rows of three of this tensor xB
[82:57] and so we can see that for example here
[82:58] it was the first example in this batch
[83:00] is that the first character and the
[83:02] first character and the fourth character
[83:04] comes into the neural net
[83:06] and then we want to predict the next
[83:08] character in a sequence after the
[83:10] character is one one four
[83:12] so basically What's Happening Here is
[83:14] there are integers inside XB and each
[83:18] one of these integers is specifying
[83:19] which row of C we want to pluck out
[83:22] right and then we arrange those rows
[83:25] that we've plucked out into 32 by 3 by
[83:28] 10 tensor and we just package them in we
[83:30] just package them into the sensor
[83:33] and now what's happening is that we have
[83:35] D amp
[83:36] so for every one of these uh basically
[83:39] plucked out rows we have their gradients
[83:41] now
[83:42] but they're arranged inside this 32 by 3
[83:45] by 10 tensor so all we have to do now is
[83:48] we just need to Route this gradient
[83:49] backwards through this assignment so we
[83:52] need to find which row of C that every
[83:54] one of these
[83:56] um 10 dimensional embeddings come from
[83:59] and then we need to deposit them into DC
[84:03] so we just need to undo the indexing and
[84:06] of course if any of these rows of C was
[84:08] used multiple times which almost
[84:10] certainly is the case like the row one
[84:11] and one was used multiple times then we
[84:13] have to remember that the gradients that
[84:15] arrive there have to add
[84:18] so for each occurrence we have to have
[84:19] an addition
[84:21] so let's now write this out and I don't
[84:23] actually know if like a much better way
[84:24] to do this than a for Loop unfortunately
[84:26] in Python
[84:28] um so maybe someone can come up with a
[84:29] vectorized efficient operation but for
[84:32] now let's just use for loops so let me
[84:34] create a torch.zeros like
[84:37] C to initialize uh just uh 27 by 10
[84:40] tensor of all zeros
[84:43] and then honestly 4K in range XB dot
[84:46] shape at zero
[84:49] maybe someone has a better way to do
[84:51] this but for J and range
[84:53] be that shape at one
[84:55] this is going to iterate over all the
[84:58] um all the elements of XB all these
[85:01] integers
[85:03] and then let's get the index at this
[85:05] position
[85:06] so the index is basically x b at KJ
[85:11] so that an example of that like is 11 or
[85:14] 14 and so on
[85:16] and now in the forward pass we took
[85:19] and we basically took um
[85:24] the row of C at index and we deposited
[85:27] it into M at K of J
[85:30] that's what happened that's where they
[85:32] are packaged so now we need to go
[85:34] backwards and we just need to route
[85:36] DM at the position KJ
[85:39] we now have these derivatives
[85:42] for each position and it's 10
[85:44] dimensional
[85:45] and you just need to go into the correct
[85:47] row of C
[85:49] so DC rather at IX is this but plus
[85:54] equals
[85:55] because there could be multiple
[85:56] occurrences uh like the same row could
[85:58] have been used many many times and so
[86:00] all of those derivatives will just go
[86:04] backwards through the indexing and they
[86:06] will add
[86:07] so this is my candidate solution
[86:12] let's copy it here
[86:16] let's uncomment this and cross our
[86:19] fingers
[86:20] hey
[86:21] so that's it we've back propagated
[86:24] through
[86:25] this entire Beast
[86:28] so there we go totally makes sense
[86:31] so now we come to exercise two it
[86:33] basically turns out that in this first
[86:34] exercise we were doing way too much work
[86:36] uh we were back propagating way too much
[86:39] and it was all good practice and so on
[86:40] but it's not what you would do in
[86:42] practice and the reason for that is for
[86:44] example here I separated out this loss
[86:47] calculation over multiple lines and I
[86:49] broke it up all all to like its smallest
[86:51] atomic pieces and we back propagated
[86:53] through all of those individually
[86:55] but it turns out that if you just look
[86:56] at the mathematical expression for the
[86:58] loss
[87:00] um then actually you can do the
[87:02] differentiation on pen and paper and a
[87:04] lot of terms cancel and simplify and the
[87:06] mathematical expression you end up with
[87:07] can be significantly shorter and easier
[87:10] to implement than back propagating
[87:11] through all the little pieces of
[87:12] everything you've done
[87:13] so before we had this complicated
[87:16] forward paths going from logits to the
[87:18] loss
[87:19] but in pytorch everything can just be
[87:21] glued together into a single call at
[87:22] that cross entropy you just pass in
[87:24] logits and the labels and you get the
[87:26] exact same loss as I verify here so our
[87:28] previous loss and the fast loss coming
[87:31] from the chunk of operations as a single
[87:33] mathematical expression is the same but
[87:36] it's much much faster in a forward pass
[87:38] it's also much much faster in backward
[87:40] pass and the reason for that is if you
[87:42] just look at the mathematical form of
[87:43] this and differentiate again you will
[87:45] end up with a very small and short
[87:46] expression so that's what we want to do
[87:48] here we want to in a single operation or
[87:51] in a single go or like very quickly go
[87:54] directly to delojits
[87:56] and we need to implement the logits as a
[87:59] function of logits and yb's
[88:02] but it will be significantly shorter
[88:04] than whatever we did here where to get
[88:06] to deluggets we had to go all the way
[88:08] here
[88:10] so all of this work can be skipped in a
[88:12] much much simpler mathematical
[88:13] expression that you can Implement here
[88:16] so you can give it a shot yourself
[88:18] basically look at what exactly is the
[88:21] mathematical expression of loss and
[88:23] differentiate with respect to the logits
[88:26] so let me show you a hint you can of
[88:29] course try it fully yourself but if not
[88:31] I can give you some hint of how to get
[88:33] started mathematically
[88:36] so basically What's Happening Here is we
[88:38] have logits then there's a softmax that
[88:41] takes the logits and gives you
[88:42] probabilities then we are using the
[88:44] identity of the correct next character
[88:46] to pluck out a row of probabilities take
[88:50] the negative log of it to get our
[88:51] negative block probability and then we
[88:54] average up all the log probabilities or
[88:56] negative block probabilities to get our
[88:58] loss
[88:59] so basically what we have is for a
[89:01] single individual example rather we have
[89:04] that loss is equal to negative log
[89:06] probability uh where P here is kind of
[89:09] like thought of as a vector of all the
[89:11] probabilities so at the Y position where
[89:14] Y is the label
[89:16] and we have that P here of course is the
[89:19] softmax so the ith component of P of
[89:23] this probability Vector is just the
[89:25] softmax function so raising all the
[89:28] logits uh basically to the power of E
[89:31] and normalizing so everything comes to
[89:34] 1.
[89:35] now if you write out P of Y here you can
[89:38] just write out the soft Max and then
[89:40] basically what we're interested in is
[89:41] we're interested in the derivative of
[89:43] the loss with respect to the I logit
[89:47] and so basically it's a d by DLI of this
[89:51] expression here
[89:52] where we have L indexed with the
[89:54] specific label Y and on the bottom we
[89:56] have a sum over J of e to the L J and
[89:58] the negative block of all that so
[90:00] potentially give it a shot pen and paper
[90:02] and see if you can actually derive the
[90:04] expression for the loss by DLI and then
[90:07] we're going to implement it here okay so
[90:09] I'm going to give away the result here
[90:11] so this is some of the math I did to
[90:13] derive the gradients analytically and so
[90:17] we see here that I'm just applying the
[90:19] rules of calculus from your first or
[90:20] second year of bachelor's degree if you
[90:22] took it and we see that the expression
[90:24] is actually simplify quite a bit you
[90:26] have to separate out the analysis in the
[90:27] case where the ith index that you're
[90:30] interested in inside logits is either
[90:32] equal to the label or it's not equal to
[90:34] the label and then the expression
[90:35] simplify and cancel in a slightly
[90:37] different way and what we end up with is
[90:39] something very very simple
[90:41] and we either end up with basically
[90:43] pirai where p is again this Vector of
[90:46] probabilities after a soft Max or P at I
[90:49] minus 1 where we just simply subtract a
[90:51] one but in any case we just need to
[90:53] calculate the soft Max p e and then in
[90:56] the correct Dimension we need to
[90:58] subtract one and that's the gradient the
[91:00] form that it takes analytically so let's
[91:03] implement this basically and we have to
[91:04] keep in mind that this is only done for
[91:06] a single example but here we are working
[91:08] with batches of examples
[91:09] so we have to be careful of that and
[91:12] then the loss for a batch is the average
[91:14] loss over all the examples so in other
[91:17] words is the example for all the
[91:18] individual examples is the loss for each
[91:20] individual example summed up and then
[91:22] divided by n and we have to back
[91:24] propagate through that as well and be
[91:26] careful with it
[91:28] so deluggets is going to be of that soft
[91:30] Max
[91:32] uh pytorch has a softmax function that
[91:35] you can call and we want to apply the
[91:36] softmax on the logits and we want to go
[91:39] in the dimension that is one so
[91:42] basically we want to do the softmax
[91:44] along the rows of these logits
[91:47] then at the correct positions we need to
[91:49] subtract a 1. so delugits at iterating
[91:52] over all the rows
[91:54] and indexing into the columns
[91:57] provided by the correct labels inside YB
[92:00] we need to subtract one
[92:03] and then finally it's the average loss
[92:05] that is the loss and in the average
[92:07] there's a one over n of all the losses
[92:09] added up and so we need to also
[92:12] propagate through that division
[92:14] so the gradient has to be scaled down by
[92:16] by n as well because of the mean
[92:19] but this otherwise should be the result
[92:22] so now if we verify this
[92:24] we see that we don't get an exact match
[92:26] but at the same time the maximum
[92:30] difference from logits from pytorch and
[92:33] RD logits here is uh on the order of 5e
[92:37] negative 9. so it's a tiny tiny number
[92:39] so because of floating point wantiness
[92:41] we don't get the exact bitwise result
[92:44] but we basically get the correct answer
[92:47] approximately
[92:49] now I'd like to pause here briefly
[92:51] before we move on to the next exercise
[92:52] because I'd like us to get an intuitive
[92:54] sense of what the logits is because it
[92:56] has a beautiful and very simple
[92:58] explanation honestly
[93:00] um so here I'm taking the logits and I'm
[93:03] visualizing it and we can see that we
[93:05] have a batch of 32 examples of 27
[93:07] characters
[93:08] and what is the logits intuitively right
[93:10] the logits is the probabilities that the
[93:13] properties Matrix in the forward pass
[93:15] but then here these black squares are
[93:17] the positions of the correct indices
[93:19] where we subtracted a one
[93:21] and so uh what is this doing right these
[93:24] are the derivatives on the logits and so
[93:27] let's look at just the first row here
[93:31] so that's what I'm doing here I'm
[93:33] clocking the probabilities of these
[93:34] logits and then I'm taking just the
[93:36] first row and this is the probability
[93:38] row and then the logits of the first row
[93:41] and multiplying by n just for us so that
[93:43] we don't have the scaling by n in here
[93:46] and everything is more interpretable we
[93:48] see that it's exactly equal to the
[93:50] probability of course but then the
[93:52] position of the correct index has a
[93:53] minus equals one so minus one on that
[93:56] position
[93:57] and so notice that
[93:59] um if you take Delo Jets at zero and you
[94:01] sum it
[94:03] it actually sums to zero and so you
[94:06] should think of these uh gradients here
[94:08] at each cell as like a force
[94:12] um we are going to be basically pulling
[94:15] down on the probabilities of the
[94:17] incorrect characters and we're going to
[94:19] be pulling up on the probability at the
[94:22] correct index and that's what's
[94:24] basically happening in each row and thus
[94:29] the amount of push and pull is exactly
[94:31] equalized because the sum is zero so the
[94:34] amount to which we pull down in the
[94:36] probabilities and the demand that we
[94:37] push up on the probability of the
[94:39] correct character is equal
[94:41] so sort of the the repulsion and the
[94:43] attraction are equal and think of the
[94:45] neural app now as a like a massive uh
[94:48] pulley system or something like that
[94:50] we're up here on top of the logits and
[94:52] we're pulling up we're pulling down the
[94:54] properties of Incorrect and pulling up
[94:55] the property of the correct and in this
[94:57] complicated pulley system because
[94:59] everything is mathematically uh just
[95:01] determined just think of it as sort of
[95:03] like this tension translating to this
[95:05] complicating pulling mechanism and then
[95:07] eventually we get a tug on the weights
[95:09] and the biases and basically in each
[95:11] update we just kind of like tug in the
[95:13] direction that we like for each of these
[95:15] elements and the parameters are slowly
[95:17] given in to the tug and that's what
[95:19] training in neural net kind of like
[95:20] looks like on a high level
[95:22] and so I think the the forces of push
[95:24] and pull in these gradients are actually
[95:26] uh very intuitive here we're pushing and
[95:29] pulling on the correct answer and the
[95:31] incorrect answers and the amount of
[95:33] force that we're applying is actually
[95:34] proportional to uh the probabilities
[95:37] that came out in the forward pass
[95:39] and so for example if our probabilities
[95:41] came out exactly correct so they would
[95:43] have had zero everywhere except for one
[95:45] at the correct uh position then the the
[95:48] logits would be all a row of zeros for
[95:51] that example there would be no push and
[95:52] pull so the amount to which your
[95:55] prediction is incorrect is exactly the
[95:58] amount by which you're going to get a
[95:59] pull or a push in that dimension
[96:01] so if you have for example a very
[96:04] confidently mispredicted element here
[96:05] then
[96:07] um what's going to happen is that
[96:08] element is going to be pulled down very
[96:10] heavily and the correct answer is going
[96:12] to be pulled up to the same amount
[96:14] and the other characters are not going
[96:16] to be influenced too much
[96:19] so the amounts to which you mispredict
[96:21] is then proportional to the strength of
[96:23] the pole and that's happening
[96:25] independently in all the dimensions of
[96:27] this of this tensor and it's sort of
[96:29] very intuitive and varies to think
[96:30] through and that's basically the magic
[96:32] of the cross-entropy loss and what it's
[96:34] doing dynamically in the backward pass
[96:36] of the neural net so now we get to
[96:38] exercise number three which is a very
[96:41] fun exercise
[96:42] um depending on your definition of fun
[96:43] and we are going to do for batch
[96:45] normalization exactly what we did for
[96:47] cross entropy loss in exercise number
[96:49] two that is we are going to consider it
[96:51] as a glued single mathematical
[96:52] expression and back propagate through it
[96:54] in a very efficient manner because we
[96:56] are going to derive a much simpler
[96:58] formula for the backward path of batch
[96:59] normalization
[97:01] and we're going to do that using pen and
[97:02] paper
[97:03] so previously we've broken up
[97:05] bastionalization into all of the little
[97:06] intermediate pieces and all the atomic
[97:08] operations inside it and then we back
[97:10] propagate it through it one by one
[97:13] now we just have a single sort of
[97:15] forward pass of a batch form and it's
[97:18] all glued together
[97:20] and we see that we get the exact same
[97:21] result as before
[97:23] now for the backward pass we'd like to
[97:25] also Implement a single formula
[97:27] basically for back propagating through
[97:29] this entire operation that is the
[97:30] bachelorization
[97:32] so in the forward pass previously we
[97:34] took hpvn the hidden states of the
[97:37] pre-batch realization and created H
[97:39] preact which is the hidden States just
[97:42] before the activation
[97:44] in the bachelorization paper each pbn is
[97:46] X and each preact is y
[97:49] so in the backward pass what we'd like
[97:51] to do now is we have DH preact and we'd
[97:54] like to produce d h previous
[97:56] and we'd like to do that in a very
[97:57] efficient manner so that's the name of
[98:00] the game calculate the H previan given
[98:02] DH preact and for the purposes of this
[98:05] exercise we're going to ignore gamma and
[98:07] beta and their derivatives because they
[98:09] take on a very simple form in a very
[98:11] similar way to what we did up above
[98:14] so let's calculate this given that right
[98:18] here
[98:18] so to help you a little bit like I did
[98:20] before I started off the implementation
[98:23] here on pen and paper and I took two
[98:26] sheets of paper to derive the
[98:28] mathematical formulas for the backward
[98:29] pass
[98:30] and basically to set up the problem uh
[98:33] just write out the MU Sigma Square
[98:35] variance x i hat and Y I exactly as in
[98:39] the paper except for the bezel
[98:40] correction
[98:41] and then
[98:42] in a backward pass we have the
[98:44] derivative of the loss with respect to
[98:46] all the elements of Y and remember that
[98:48] Y is a vector there's there's multiple
[98:50] numbers here
[98:52] so we have all the derivatives with
[98:54] respect to all the Y's
[98:56] and then there's a demo and a beta and
[98:59] this is kind of like the compute graph
[99:01] the gamma and the beta there's the X hat
[99:03] and then the MU and the sigma squared
[99:06] and the X so we have DL by DYI and we
[99:10] won't DL by d x i for all the I's in
[99:13] these vectors
[99:15] so this is the compute graph and you
[99:17] have to be careful because I'm trying to
[99:19] note here that these are vectors so
[99:22] there's many nodes here inside x x hat
[99:25] and Y but mu and sigma sorry Sigma
[99:29] Square are just individual scalars
[99:30] single numbers so you have to be careful
[99:33] with that you have to imagine there's
[99:34] multiple nodes here or you're going to
[99:35] get your math wrong
[99:38] um so as an example I would suggest that
[99:40] you go in the following order one two
[99:43] three four in terms of the back
[99:44] propagation so back propagating to X hat
[99:46] then into Sigma Square then into mu and
[99:49] then into X
[99:52] um just like in a topological sort in
[99:54] micrograd we would go from right to left
[99:55] you're doing the exact same thing except
[99:57] you're doing it with symbols and on a
[99:59] piece of paper
[100:01] so for number one uh I'm not giving away
[100:05] too much if you want DL of d x i hat
[100:09] then we just take DL by DYI and multiply
[100:12] it by gamma because of this expression
[100:15] here where any individual Yi is just
[100:17] gamma times x i hat plus beta so it
[100:21] doesn't help you too much there but this
[100:23] gives you basically the derivatives for
[100:25] all the X hats and so now try to go
[100:28] through this computational graph and
[100:31] derive what is DL by D Sigma Square
[100:35] and then what is DL by B mu and then one
[100:38] is D L by DX
[100:39] eventually so give it a go and I'm going
[100:42] to be revealing the answer one piece at
[100:44] a time okay so to get DL by D Sigma
[100:46] Square we have to remember again like I
[100:48] mentioned that there are many excess X
[100:51] hats here
[100:52] and remember that Sigma square is just a
[100:54] single individual number here
[100:55] so when we look at the expression
[100:59] for the L by D Sigma Square
[101:01] we have that we have to actually
[101:03] consider all the possible paths that um
[101:08] we basically have that there's many X
[101:10] hats and they all feed off from they all
[101:13] depend on Sigma Square so Sigma square
[101:15] has a large fan out there's lots of
[101:17] arrows coming out from Sigma square into
[101:19] all the X hats
[101:20] and then there's a back propagating
[101:22] signal from each X hat into Sigma square
[101:24] and that's why we actually need to sum
[101:26] over all those I's from I equal to 1 to
[101:29] m
[101:30] of the DL by d x i hat which is the
[101:35] global gradient
[101:36] times the x i Hat by D Sigma Square
[101:40] which is the local gradient
[101:42] of this operation here
[101:44] and then mathematically I'm just working
[101:46] it out here and I'm simplifying and you
[101:48] get a certain expression for DL by D
[101:51] Sigma square and we're going to be using
[101:52] this expression when we back propagate
[101:53] into mu and then eventually into X so
[101:56] now let's continue our back propagation
[101:58] into mu so what is D L by D mu now again
[102:01] be careful that mu influences X hat and
[102:04] X hat is actually lots of values so for
[102:07] example if our mini batch size is 32 as
[102:09] it is in our example that we were
[102:10] working on then this is 32 numbers and
[102:13] 32 arrows going back to mu and then mu
[102:16] going to Sigma square is just a single
[102:18] Arrow because Sigma square is a scalar
[102:19] so in total there are 33 arrows
[102:22] emanating from you and then all of them
[102:25] have gradients coming into mu and they
[102:27] all need to be summed up
[102:29] and so that's why when we look at the
[102:31] expression for DL by D mu I am summing
[102:34] up over all the gradients of DL by d x i
[102:37] hat times the x i Hat by being mu
[102:40] uh so that's the that's this arrow and
[102:43] that's 32 arrows here and then plus the
[102:45] one Arrow from here which is the L by
[102:47] the sigma Square Times the sigma squared
[102:49] by D mu
[102:50] so now we have to work out that
[102:52] expression and let me just reveal the
[102:54] rest of it
[102:55] uh simplifying here is not complicated
[102:58] the first term and you just get an
[103:00] expression here
[103:01] for the second term though there's
[103:02] something really interesting that
[103:03] happens
[103:04] when we look at the sigma squared by D
[103:06] mu and we simplify
[103:08] at one point if we assume that in a
[103:11] special case where mu is actually the
[103:14] average of X I's as it is in this case
[103:17] then if we plug that in then actually
[103:20] the gradient vanishes and becomes
[103:22] exactly zero and that makes the entire
[103:24] second term cancel
[103:26] and so these uh if you just have a
[103:29] mathematical expression like this and
[103:30] you look at D Sigma Square by D mu you
[103:33] would get some mathematical formula for
[103:35] how mu impacts Sigma Square
[103:37] but if it is the special case that Nu is
[103:39] actually equal to the average as it is
[103:42] in the case of pastoralization that
[103:43] gradient will actually vanish and become
[103:45] zero so the whole term cancels and we
[103:48] just get a fairly straightforward
[103:49] expression here for DL by D mu okay and
[103:52] now we get to the craziest part which is
[103:54] uh deriving DL by dxi which is
[103:57] ultimately what we're after
[103:59] now let's count
[104:00] first of all how many numbers are there
[104:03] inside X as I mentioned there are 32
[104:05] numbers there are 32 Little X I's and
[104:08] let's count the number of arrows
[104:09] emanating from each x i
[104:11] there's an arrow going to Mu an arrow
[104:13] going to Sigma Square
[104:14] and then there's an arrow going to X hat
[104:16] but this Arrow here let's scrutinize
[104:19] that a little bit
[104:20] each x i hat is just a function of x i
[104:23] and all the other scalars so x i hat
[104:27] only depends on x i and none of the
[104:29] other X's
[104:30] and so therefore there are actually in
[104:32] this single Arrow there are 32 arrows
[104:34] but those 32 arrows are going exactly
[104:37] parallel they don't interfere and
[104:39] they're just going parallel between x
[104:40] and x hat you can look at it that way
[104:42] and so how many arrows are emanating
[104:44] from each x i there are three arrows mu
[104:47] Sigma squared and the associated X hat
[104:50] and so in back propagation we now need
[104:53] to apply the chain rule and we need to
[104:55] add up those three contributions
[104:57] so here's what that looks like if I just
[104:59] write that out
[105:02] we have uh we're going through we're
[105:04] chaining through mu Sigma square and
[105:06] through X hat and those three terms are
[105:09] just here
[105:10] now we already have three of these we
[105:13] have d l by d x i hat
[105:15] we have DL by D mu which we derived here
[105:17] and we have DL by D Sigma Square which
[105:19] we derived here but we need three other
[105:22] terms here
[105:23] the this one this one and this one so I
[105:26] invite you to try to derive them it's
[105:28] not that complicated you're just looking
[105:29] at these Expressions here and
[105:31] differentiating with respect to x i
[105:34] so give it a shot but here's the result
[105:39] or at least what I got
[105:41] um
[105:42] yeah I'm just I'm just differentiating
[105:44] with respect to x i for all these
[105:45] expressions and honestly I don't think
[105:47] there's anything too tricky here it's
[105:48] basic calculus
[105:50] now it gets a little bit more tricky is
[105:52] we are now going to plug everything
[105:53] together so all of these terms
[105:55] multiplied with all of these terms and
[105:57] add it up according to this formula and
[105:59] that gets a little bit hairy so what
[106:01] ends up happening is
[106:04] uh
[106:05] you get a large expression and the thing
[106:08] to be very careful with here of course
[106:09] is we are working with a DL by dxi for
[106:12] specific I here but when we are plugging
[106:15] in some of these terms
[106:17] like say
[106:18] um
[106:19] this term here deal by D signal squared
[106:22] you see how the L by D Sigma squared I
[106:24] end up with an expression and I'm
[106:26] iterating over little I's here but I
[106:29] can't use I as the variable when I plug
[106:31] in here because this is a different I
[106:33] from this eye
[106:35] this I here is just a place or like a
[106:37] local variable for for a for Loop in
[106:39] here so here when I plug that in you
[106:41] notice that I rename the I to a j
[106:43] because I need to make sure that this J
[106:45] is not that this J is not this I this J
[106:48] is like like a little local iterator
[106:50] over 32 terms and so you have to be
[106:53] careful with that when you're plugging
[106:54] in the expressions from here to here you
[106:56] may have to rename eyes into J's and you
[106:58] have to be very careful what is actually
[107:00] an I with respect to the L by t x i
[107:04] so some of these are J's some of these
[107:07] are I's
[107:08] and then we simplify this expression
[107:11] and I guess like the big thing to notice
[107:13] here is a bunch of terms just kind of
[107:15] come out to the front and you can
[107:16] refactor them there's a sigma squared
[107:18] plus Epsilon raised to the power of
[107:19] negative three over two uh this Sigma
[107:21] squared plus Epsilon can be actually
[107:23] separated out into three terms each of
[107:25] them are Sigma squared plus Epsilon to
[107:28] the negative one over two so the three
[107:30] of them multiplied is equal to this and
[107:33] then those three terms can go different
[107:35] places because of the multiplication so
[107:37] one of them actually comes out to the
[107:39] front and will end up here outside one
[107:42] of them joins up with this term and one
[107:45] of them joins up with this other term
[107:47] and then when you simplify the
[107:49] expression you'll notice that some of
[107:51] these terms that are coming out are just
[107:52] the x i hats
[107:54] so you can simplify just by rewriting
[107:56] that
[107:57] and what we end up with at the end is a
[107:58] fairly simple mathematical expression
[108:00] over here that I cannot simplify further
[108:02] but basically you'll notice that it only
[108:05] uses the stuff we have and it derives
[108:06] the thing we need so we have the L by d
[108:10] y for all the I's and those are used
[108:13] plenty of times here and also in
[108:15] addition what we're using is these x i
[108:17] hats and XJ hats and they just come from
[108:19] the forward pass
[108:20] and otherwise this is a simple
[108:22] expression and it gives us DL by d x i
[108:25] for all the I's and that's ultimately
[108:27] what we're interested in
[108:29] so that's the end of Bachelor backward
[108:32] pass analytically let's now implement
[108:34] this final result
[108:36] okay so I implemented the expression
[108:38] into a single line of code here and you
[108:41] can see that the max diff is Tiny so
[108:43] this is the correct implementation of
[108:44] this formula now I'll just uh
[108:48] basically tell you that getting this
[108:50] formula here from this mathematical
[108:52] expression was not trivial and there's a
[108:54] lot going on packed into this one
[108:56] formula and this is a whole exercise by
[108:58] itself because you have to consider the
[109:00] fact that this formula here is just for
[109:03] a single neuron and a batch of 32
[109:05] examples but what I'm doing here is I'm
[109:07] actually we actually have 64 neurons and
[109:10] so this expression has to in parallel
[109:11] evaluate the bathroom backward pass for
[109:14] all of those 64 neurons in parallel
[109:16] independently so this has to happen
[109:18] basically in every single
[109:20] um
[109:20] column of the inputs here
[109:24] and in addition to that you see how
[109:26] there are a bunch of sums here and we
[109:28] need to make sure that when I do those
[109:29] sums that they broadcast correctly onto
[109:31] everything else that's here
[109:33] and so getting this expression is just
[109:35] like highly non-trivial and I invite you
[109:36] to basically look through it and step
[109:37] through it and it's a whole exercise to
[109:39] make sure that this this checks out but
[109:43] once all the shapes are green and once
[109:45] you convince yourself that it's correct
[109:46] you can also verify that Patrick's gets
[109:48] the exact same answer as well and so
[109:50] that gives you a lot of peace of mind
[109:51] that this mathematical formula is
[109:53] correctly implemented here and
[109:55] broadcasted correctly and replicated in
[109:57] parallel for all of the 64 neurons
[110:00] inside this bastrum layer okay and
[110:03] finally exercise number four asks you to
[110:05] put it all together and uh here we have
[110:08] a redefinition of the entire problem so
[110:10] you see that we reinitialize the neural
[110:11] nut from scratch and everything and then
[110:13] here instead of calling loss that
[110:15] backward we want to have the manual back
[110:18] propagation here as we derived It Up
[110:20] Above so go up copy paste all the chunks
[110:23] of code that we've already derived put
[110:25] them here and drive your own gradients
[110:26] and then optimize this neural nut
[110:28] basically using your own gradients all
[110:31] the way to the calibration of The
[110:33] Bachelor and the evaluation of the loss
[110:34] and I was able to achieve quite a good
[110:36] loss basically the same loss you would
[110:38] achieve before and that shouldn't be
[110:40] surprising because all we've done is
[110:41] we've really gotten to Lost That
[110:44] backward and we've pulled out all the
[110:45] code
[110:46] and inserted it here but those gradients
[110:49] are identical and everything is
[110:50] identical and the results are identical
[110:52] it's just that we have full visibility
[110:54] on exactly what goes on under the hood
[110:56] I'll plot that backward in this specific
[110:58] case and this is all of our code this is
[111:02] the full backward pass using basically
[111:04] the simplified backward pass for the
[111:06] cross entropy loss and the mass
[111:08] generalization so back propagating
[111:10] through cross entropy the second layer
[111:13] the 10 H nonlinearity the batch
[111:15] normalization
[111:16] uh through the first layer and through
[111:19] the embedding and so you see that this
[111:21] is only maybe what is this 20 lines of
[111:23] code or something like that and that's
[111:25] what gives us gradients and now we can
[111:27] potentially erase losses backward so the
[111:30] way I have the code set up is you should
[111:31] be able to run this entire cell once you
[111:33] fill this in and this will run for only
[111:36] 100 iterations and then break
[111:37] and it breaks because it gives you an
[111:39] opportunity to check your gradients
[111:41] against pytorch
[111:43] so here our gradients we see are not
[111:46] exactly equal they are approximately
[111:48] equal and the differences are tiny
[111:51] wanting negative 9 or so and I don't
[111:52] exactly know where they're coming from
[111:54] to be honest
[111:56] um so once we have some confidence that
[111:57] the gradients are basically correct we
[111:59] can take out the gradient tracking
[112:01] we can disable this breaking statement
[112:05] and then we can
[112:07] basically disable lost of backward we
[112:10] don't need it anymore it feels amazing
[112:13] to say that
[112:14] and then here when we are doing the
[112:16] update we're not going to use P dot grad
[112:18] this is the old way of pytorch we don't
[112:21] have that anymore because we're not
[112:22] doing backward we are going to use this
[112:25] update where we you see that I'm
[112:27] iterating over
[112:29] I've arranged the grads to be in the
[112:30] same order as the parameters and I'm
[112:32] zipping them up the gradients and the
[112:34] parameters into p and grad and then here
[112:37] I'm going to step with just the grad
[112:38] that we derived manually
[112:40] so the last piece
[112:43] um is that none of this now requires
[112:46] gradients from pytorch and so one thing
[112:49] you can do here
[112:51] um
[112:52] is you can do with no grad and offset
[112:56] this whole code block
[112:58] and really what you're saying is you're
[112:59] telling Pat George that hey I'm not
[113:00] going to call backward on any of this
[113:02] and this allows pytorch to be a bit more
[113:03] efficient with all of it
[113:05] and then we should be able to just uh
[113:07] run this
[113:09] and
[113:11] it's running
[113:13] and you see that losses backward is
[113:16] commented out
[113:18] and we're optimizing
[113:20] so we're going to leave this run and uh
[113:23] hopefully we get a good result
[113:25] okay so I allowed the neural net to
[113:27] finish optimization
[113:28] then here I calibrate the bachelor
[113:31] parameters because I did not keep track
[113:33] of the running mean and very variants in
[113:35] their training Loop
[113:37] then here I ran the loss and you see
[113:39] that we actually obtained a pretty good
[113:40] loss very similar to what we've achieved
[113:42] before
[113:43] and then here I'm sampling from the
[113:45] model and we see some of the name like
[113:47] gibberish that we're sort of used to so
[113:49] basically the model worked and samples
[113:52] uh pretty decent results compared to
[113:54] what we were used to so everything is
[113:56] the same but of course the big deal is
[113:58] that we did not use lots of backward we
[114:00] did not use package Auto grad and we
[114:02] estimated our gradients ourselves by
[114:04] hand
[114:05] and so hopefully you're looking at this
[114:06] the backward pass of this neural net and
[114:08] you're thinking to yourself actually
[114:10] that's not too complicated
[114:12] um
[114:13] each one of these layers is like three
[114:15] lines of code or something like that and
[114:17] most of it is fairly straightforward
[114:18] potentially with the notable exception
[114:20] of the batch normalization backward pass
[114:22] otherwise it's pretty good okay and
[114:25] that's everything I wanted to cover for
[114:26] this lecture so hopefully you found this
[114:29] interesting and what I liked about it
[114:31] honestly is that it gave us a very nice
[114:33] diversity of layers to back propagate
[114:34] through and
[114:36] um I think it gives a pretty nice and
[114:38] comprehensive sense of how these
[114:39] backward passes are implemented and how
[114:41] they work and you'd be able to derive
[114:43] them yourself but of course in practice
[114:45] you probably don't want to and you want
[114:46] to use the pythonograd but hopefully you
[114:49] have some intuition about how gradients
[114:51] flow backwards through the neural net
[114:52] starting at the loss and how they flow
[114:55] through all the variables and all the
[114:56] intermediate results
[114:58] and if you understood a good chunk of it
[115:00] and if you have a sense of that then you
[115:02] can count yourself as one of these buff
[115:03] doji's on the left instead of the uh
[115:06] those on the right here now in the next
[115:09] lecture we're actually going to go to
[115:10] recurrent neural nuts lstms and all the
[115:13] other variants of RNs and we're going to
[115:16] start to complexify the architecture and
[115:17] start to achieve better uh log
[115:19] likelihoods and so I'm really looking
[115:21] forward to that and I'll see you then