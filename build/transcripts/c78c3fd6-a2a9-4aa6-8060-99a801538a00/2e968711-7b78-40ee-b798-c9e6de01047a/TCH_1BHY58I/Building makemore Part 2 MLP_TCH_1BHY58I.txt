Video Title: Building makemore Part 2: MLP
Video ID: TCH_1BHY58I
URL: https://www.youtube.com/watch?v=TCH_1BHY58I
View Count: 454,419

[00:00] hi everyone
[00:01] today we are continuing our
[00:03] implementation of makemore
[00:05] now in the last lecture we implemented
[00:06] the bigram language model and we
[00:08] implemented it both using counts and
[00:10] also using a super simple neural network
[00:12] that had a single linear layer
[00:15] now this is the
[00:17] jupyter notebook that we built out last
[00:19] lecture
[00:20] and we saw that the way we approached
[00:21] this is that we looked at only the
[00:23] single previous character and we
[00:24] predicted the distribution for the
[00:26] character that would go next in the
[00:28] sequence and we did that by taking
[00:30] counts and normalizing them into
[00:32] probabilities
[00:33] so that each row here sums to one
[00:36] now this is all well and good if you
[00:38] only have one character of previous
[00:40] context
[00:41] and this works and it's approachable the
[00:43] problem with this model of course is
[00:45] that
[00:46] the predictions from this model are not
[00:48] very good because you only take one
[00:50] character of context so the model didn't
[00:52] produce very name like sounding things
[00:56] now the problem with this approach
[00:57] though is that if we are to take more
[00:59] context into account when predicting the
[01:01] next character in a sequence things
[01:03] quickly blow up and this table the size
[01:06] of this table grows and in fact it grows
[01:08] exponentially with the length of the
[01:10] context
[01:11] because if we only take a single
[01:12] character at a time that's 27
[01:13] possibilities of context
[01:15] but if we take two characters in the
[01:17] past and try to predict the third one
[01:19] suddenly the number of rows in this
[01:21] matrix you can look at it that way
[01:23] is 27 times 27 so there's 729
[01:26] possibilities for what could have come
[01:28] in the context
[01:30] if we take three characters as the
[01:31] context suddenly we have
[01:34] 20 000 possibilities of context
[01:37] and so there's just way too many rows of
[01:40] this matrix it's way too few counts
[01:43] for each possibility and the whole thing
[01:45] just kind of explodes and doesn't work
[01:47] very well
[01:48] so that's why today we're going to move
[01:50] on to this bullet point here and we're
[01:52] going to implement a multi-layer
[01:53] perceptron model to predict the next uh
[01:56] character in a sequence
[01:58] and this modeling approach that we're
[02:00] going to adopt follows this paper
[02:01] benguetal 2003
[02:04] so i have the paper pulled up here
[02:06] now this isn't the very first paper that
[02:08] proposed the use of multiglio
[02:10] perceptrons or neural networks to
[02:11] predict the next character or token in a
[02:13] sequence but it's definitely one that is
[02:15] uh was very influential around that time
[02:18] it is very often cited to stand in for
[02:20] this idea and i think it's a very nice
[02:22] write-up and so this is the paper that
[02:23] we're going to first look at and then
[02:25] implement now this paper has 19 pages so
[02:28] we don't have time to go into
[02:30] the full detail of this paper but i
[02:31] invite you to read it
[02:33] it's very readable interesting and has a
[02:34] lot of interesting ideas in it as well
[02:37] in the introduction they describe the
[02:38] exact same problem i just described and
[02:40] then to address it they propose the
[02:42] following model
[02:43] now keep in mind that we are building a
[02:46] character level language model so we're
[02:48] working on the level of characters in
[02:50] this paper they have a vocabulary of 17
[02:52] 000 possible words and they instead
[02:54] build a word level language model but
[02:56] we're going to still stick with the
[02:58] characters but we'll take the same
[02:59] modeling approach
[03:01] now what they do is basically they
[03:03] propose to take every one of these words
[03:05] seventeen thousand words and they're
[03:07] going to associate to each word a say
[03:10] thirty dimensional feature vector
[03:12] so every word is now
[03:15] embedded into a thirty dimensional space
[03:17] you can think of it that way so we have
[03:19] 17 000 points or vectors in a 30
[03:22] dimensional space
[03:23] and that's um you might imagine that's
[03:25] very crowded that's a lot of points for
[03:27] a very small space
[03:28] now
[03:30] in the beginning these words are
[03:31] initialized completely randomly so
[03:32] they're spread out at random
[03:34] but then we're going to tune these
[03:36] embeddings of these words using back
[03:38] propagation
[03:39] so during the course of training of this
[03:41] neural network these points or vectors
[03:43] are going to basically move around in
[03:44] this space and you might imagine that
[03:46] for example words that have very similar
[03:48] meanings or that are indeed synonyms of
[03:50] each other might end up in a very
[03:52] similar part of the space and conversely
[03:54] words that mean very different things
[03:56] would go somewhere else in a space
[03:59] now their modeling approach otherwise is
[04:01] identical to ours they are using a
[04:03] multi-layer neural network to predict
[04:04] the next word given the previous words
[04:07] and to train the neural network they are
[04:08] maximizing the log likelihood of the
[04:10] training data just like we did
[04:12] so the modeling approach itself is
[04:14] identical now here they have a concrete
[04:16] example of this intuition
[04:18] why does it work
[04:20] basically suppose that for example you
[04:21] are trying to predict a dog was running
[04:23] in a blank
[04:25] now suppose that the exact phrase a dog
[04:28] was running in a
[04:29] has never occurred in a training data
[04:31] and here you are at sort of test time
[04:33] later when the model is deployed
[04:35] somewhere
[04:36] and it's trying to make a sentence and
[04:38] it's saying a dog was running in a blank
[04:41] and because it's never encountered this
[04:43] exact phrase in the training set you're
[04:45] out of distribution as we say like you
[04:47] don't have fundamentally any
[04:49] reason to suspect
[04:52] what might come next
[04:54] but this approach actually allows you to
[04:55] get around that because maybe you didn't
[04:57] see the exact phrase a dog was running
[04:59] in a something but maybe you've seen
[05:01] similar phrases maybe you've seen the
[05:03] phrase the dog was running in a blank
[05:06] and maybe your network has learned that
[05:07] a and the
[05:08] are like frequently are interchangeable
[05:10] with each other and so maybe it took the
[05:13] embedding for a and the embedding for
[05:15] the and it actually put them like nearby
[05:17] each other in the space and so you can
[05:18] transfer knowledge through that
[05:20] embedding and you can generalize in that
[05:22] way
[05:23] similarly the network could know that
[05:25] cats and dogs are animals and they
[05:26] co-occur in lots of very similar
[05:28] contexts and so even though you haven't
[05:30] seen this exact phrase
[05:32] or if you haven't seen exactly walking
[05:34] or running
[05:35] you can through the embedding space
[05:37] transfer knowledge and you can
[05:39] generalize to novel scenarios
[05:42] so let's now scroll down to the diagram
[05:43] of the neural network
[05:44] they have a nice diagram here
[05:47] and in this example we are taking three
[05:49] previous words
[05:51] and we are trying to predict the fourth
[05:53] word
[05:54] in a sequence
[05:55] now these three previous words as i
[05:57] mentioned uh we have a vocabulary of 17
[06:00] 000 um possible words
[06:02] so every one of these
[06:04] basically basically
[06:05] are the index of the incoming word
[06:09] and because there are 17 000 words this
[06:11] is an integer between 0 and 16999
[06:17] now there's also a lookup table that
[06:19] they call c
[06:20] this lookup table is a matrix that is 17
[06:23] 000 by say 30.
[06:25] and basically what we're doing here is
[06:27] we're treating this as a lookup table
[06:29] and so every index is
[06:31] plucking out a row of this embedding
[06:34] matrix
[06:35] so that each index is converted to the
[06:37] 30 dimensional vector that corresponds
[06:39] to the embedding vector for that word
[06:42] so here we have the input layer of 30
[06:45] neurons for three words making up 90
[06:48] neurons in total
[06:50] and here they're saying that this matrix
[06:52] c is shared across all the words so
[06:54] we're always indexing into the same
[06:56] matrix c over and over um
[06:59] for each one of these
[07:00] words next up is the hidden layer of
[07:03] this neural network the size of this
[07:05] hidden neural layer of this neural net
[07:07] is a hoppy parameter so we use the word
[07:09] hyperparameter when it's kind of like a
[07:11] design choice up to the designer of the
[07:12] neural net and this can be as large as
[07:15] you'd like or as small as you'd like so
[07:16] for example the size could be a hundred
[07:19] and we are going to go over multiple
[07:20] choices of the size of this hidden layer
[07:23] and we're going to evaluate how well
[07:24] they work
[07:26] so say there were 100 neurons here all
[07:28] of them would be fully connected to the
[07:30] 90 words or 90 um
[07:32] numbers that make up these three words
[07:35] so this is a fully connected layer
[07:37] then there's a 10 inch long linearity
[07:40] and then there's this output layer and
[07:42] because there are 17 000 possible words
[07:44] that could come next
[07:45] this layer has 17 000 neurons
[07:48] and all of them are fully connected to
[07:51] all of these neurons in the hidden layer
[07:54] so there's a lot of parameters here
[07:56] because there's a lot of words so most
[07:58] computation is here this is the
[07:59] expensive layer
[08:01] now there are 17 000 logits here so on
[08:04] top of there we have the softmax layer
[08:06] which we've seen in our previous video
[08:08] as well so every one of these logits is
[08:10] exponentiated and then everything is
[08:12] normalized to sum to 1 so that we have a
[08:15] nice probability distribution for the
[08:17] next word in the sequence
[08:19] now of course during training we
[08:21] actually have the label we have the
[08:22] identity of the next word in a sequence
[08:25] that word
[08:26] or its index is used to pluck out the
[08:30] probability of that word
[08:32] and then we are maximizing the
[08:34] probability of that word
[08:37] with respect to the parameters of this
[08:38] neural net
[08:39] so the parameters are the weights and
[08:41] biases of this output layer
[08:44] the weights and biases of the hidden
[08:45] layer and the embedding lookup table c
[08:49] and all of that is optimized using back
[08:50] propagation
[08:52] and these uh dashed arrows ignore those
[08:55] uh that represents a variation of a
[08:57] neural nut that we are not going to
[08:58] explore in this video
[08:59] so that's the setup and now let's
[09:01] implement it
[09:02] okay so i started a brand new notebook
[09:04] for this lecture
[09:05] we are importing pytorch and we are
[09:07] importing matplotlib so we can create
[09:09] figures
[09:10] then i am reading all the names into a
[09:13] list of words like i did before and i'm
[09:15] showing the first eight right here
[09:18] keep in mind that we have a 32 000 in
[09:20] total these are just the first eight
[09:22] and then here i'm building out the
[09:24] vocabulary of characters and all the
[09:25] mappings from the characters as strings
[09:28] to integers and vice versa
[09:31] now the first thing we want to do is we
[09:32] want to compile the data set for the
[09:34] neural network
[09:35] and i had to rewrite this code um i'll
[09:37] show you in a second what it looks like
[09:41] so this is the code that i created for
[09:43] the dataset creation so let me first run
[09:44] it and then i'll briefly explain how
[09:46] this works
[09:48] so first we're going to define something
[09:50] called block size and this is basically
[09:52] the context length of how many
[09:54] characters do we take to predict the
[09:56] next one so here in this example we're
[09:58] taking three characters to predict the
[09:59] fourth one so we have a block size of
[10:02] three that's the size of the block that
[10:04] supports the prediction
[10:06] then here i'm building out the x and y
[10:09] the x are the input to the neural net
[10:12] and the y are the labels for each
[10:15] example inside x
[10:17] then i'm airing over the first five
[10:19] words i'm doing first five just for
[10:21] efficiency while we are developing all
[10:23] the code but then later we're going to
[10:24] come here and erase this so that we use
[10:26] the entire training set
[10:29] so here i'm printing the word
[10:30] emma and here i'm basically showing the
[10:33] examples that we can generate the five
[10:36] examples that we can generate out of the
[10:37] single
[10:38] um sort of word emma
[10:41] so
[10:42] when we are given the context of just uh
[10:44] dot dot the first character in a
[10:45] sequence is e
[10:47] in this context the label is m
[10:50] when the context is this the label is m
[10:53] and so forth and so the way i build this
[10:55] out is first i start with a padded
[10:56] context of just zero tokens
[10:59] then i iterate over all the characters i
[11:01] get the character in the sequence and i
[11:04] basically build out the array y of this
[11:07] current character and the array x which
[11:09] stores the current running context
[11:11] and then here see i print everything and
[11:14] here i um crop the context and enter the
[11:17] new character in a sequence so this is
[11:18] kind of like a rolling window of context
[11:22] now we can change the block size here to
[11:24] for example four
[11:25] and in that case we'll be predicting the
[11:27] fifth character given the previous four
[11:30] or it can be five
[11:31] and then it would look like this
[11:33] or it can be say ten
[11:36] and then it would look something like
[11:37] this we're taking ten characters to
[11:39] predict the eleventh one
[11:40] and we're always padding with dots
[11:43] so let me bring this back to three
[11:45] just so that we have what we have here
[11:47] in the paper
[11:50] and finally the data set right now looks
[11:51] as follows
[11:53] from these five words we have created a
[11:55] data set of 32 examples
[11:57] and each input of the neural net is
[11:59] three integers and we have a label that
[12:02] is also an integer
[12:03] y
[12:04] so x looks like this
[12:06] these are the individual examples
[12:08] and then y are the labels
[12:12] so
[12:13] given this
[12:15] let's now write a neural network that
[12:17] takes these axes and predicts the y's
[12:19] first let's build the embedding lookup
[12:21] table c
[12:23] so we have 27 possible characters and
[12:25] we're going to embed them in a lower
[12:26] dimensional space
[12:28] in the paper they have 17 000 words and
[12:31] they bet them in uh spaces as small
[12:33] dimensional as 30. so they cram 17 000
[12:38] words into 30 dimensional space in our
[12:40] case we have only 27 possible characters
[12:43] so let's grab them in something as small
[12:45] as to start with for example a
[12:46] two-dimensional space
[12:48] so this lookup table will be random
[12:50] numbers
[12:51] and we'll have 27 rows and we'll have
[12:54] two columns
[12:56] right so each 20 each one of 27
[12:58] characters will have a two-dimensional
[13:00] embedding
[13:02] so that's our matrix c of embeddings in
[13:05] the beginning initialized randomly
[13:07] now before we embed all of the integers
[13:10] inside the input x using this lookup
[13:12] table c
[13:14] let me actually just try to embed a
[13:15] single individual integer like say five
[13:19] so we get a sense of how this works
[13:21] now
[13:22] one way this works of course is we can
[13:24] just take the c and we can index into
[13:26] row five
[13:27] and that gives us a vector the fifth row
[13:30] of c
[13:31] and um
[13:33] this is one way to do it
[13:34] the other way that i presented in the
[13:36] previous lecture is actually seemingly
[13:38] different but actually identical so in
[13:40] the previous lecture what we did is we
[13:42] took these integers and we used the one
[13:44] hot encoding to first encode them so f.1
[13:47] hot
[13:48] we want to encode integer 5 and we want
[13:51] to tell it that the number of classes is
[13:52] 27 so that's the 26 dimensional vector
[13:55] of all zeros except the fifth bit is
[13:57] turned on
[13:59] now this actually doesn't work
[14:03] the reason is that
[14:04] this input actually must be a doorstop
[14:06] tensor
[14:07] and i'm making some of these errors
[14:09] intentionally just so you get to see
[14:10] some errors and how to fix them
[14:12] so this must be a tester not an int
[14:14] fairly straightforward to fix we get a
[14:16] one hot vector the fifth dimension is
[14:18] one and the shape of this is 27.
[14:22] and now notice that just as i briefly
[14:24] alluded to in the previous video if we
[14:26] take this one hot vector and we multiply
[14:29] it by c
[14:33] then
[14:35] what would you expect
[14:37] well number one
[14:39] first you'd expect an error
[14:41] because
[14:43] expected scalar type long but found
[14:45] float
[14:46] so a little bit confusing but
[14:48] the problem here is that one hot the
[14:50] data type of it
[14:52] is
[14:53] long it's a 64-bit integer but this is a
[14:56] float tensor and so pytorch doesn't know
[14:59] how to multiply an int with a float and
[15:01] that's why we had to explicitly cast
[15:03] this to a float so that we can multiply
[15:06] now the output actually here
[15:09] is identical
[15:11] and that it's identical because of the
[15:12] way the matrix multiplication here works
[15:15] we have the one hot um vector
[15:17] multiplying columns of c
[15:19] and because of all the zeros they
[15:21] actually end up masking out everything
[15:23] in c except for the fifth row which is
[15:25] plucked out
[15:27] and so we actually arrive at the same
[15:28] result
[15:29] and that tells you that
[15:31] here we can interpret this first
[15:33] piece here this embedding of the integer
[15:35] we can either think of it as the integer
[15:37] indexing into a lookup table c but
[15:40] equivalently we can also think of this
[15:42] little piece here as a first layer of
[15:44] this bigger neural net
[15:46] this layer here has neurons that have no
[15:48] non-linearity there's no 10h they're
[15:50] just linear neurons and their weight
[15:52] matrix is c
[15:55] and then we are encoding integers into
[15:57] one hot and feeding those into a neural
[15:59] net and this first layer basically
[16:01] embeds them
[16:02] so those are two equivalent ways of
[16:04] doing the same thing we're just going to
[16:06] index because it's much much faster and
[16:08] we're going to discard this
[16:09] interpretation of one hot inputs into
[16:12] neural nets and we're just going to
[16:14] index integers and create and use
[16:16] embedding tables now embedding a single
[16:18] integer like 5 is easy enough we can
[16:20] simply ask pytorch to retrieve the fifth
[16:23] row of c
[16:24] or the row index five of c
[16:27] but how do we simultaneously embed all
[16:30] of these 32 by three integers stored in
[16:32] array x
[16:34] luckily pytorch indexing is fairly
[16:36] flexible and quite powerful
[16:38] so it doesn't just work to
[16:41] ask for a single element five like this
[16:44] you can actually index using lists so
[16:46] for example we can get the rows five six
[16:48] and seven
[16:49] and this will just work like this we can
[16:51] index with a list
[16:53] it doesn't just have to be a list it can
[16:55] also be a actually a tensor of integers
[16:58] and we can index with that
[17:00] so this is a integer tensor 567 and this
[17:03] will just work as well
[17:05] in fact we can also for example repeat
[17:08] row 7 and retrieve it multiple times
[17:10] and
[17:12] that same index will just get embedded
[17:14] multiple times here
[17:16] so here we are indexing with a
[17:17] one-dimensional
[17:18] tensor of integers but it turns out that
[17:21] you can also index with
[17:22] multi-dimensional tensors of integers
[17:25] here we have a two-dimensional in tensor
[17:27] of integers so we can simply just do c
[17:30] at x and this just works
[17:34] and the shape of this
[17:36] is
[17:37] 32 by 3 which is the original shape and
[17:40] now for every one of those 32 by 3
[17:41] integers we've retrieved the embedding
[17:43] vector
[17:44] here so basically we have that as an
[17:48] example
[17:49] the 13th or example index 13
[17:54] the second dimension is the integer 1 as
[17:57] an example
[17:58] and so
[17:59] here if we do c of x which gives us that
[18:02] array and then we index into 13 by two
[18:05] of that array
[18:07] then we we get the embedding
[18:09] here
[18:10] and you can verify that
[18:12] c
[18:12] at one which is the integer at that
[18:15] location is indeed equal to this
[18:19] you see they're equal
[18:21] so basically long story short pytorch
[18:23] indexing is awesome and to embed
[18:26] simultaneously all of the integers in x
[18:29] we can simply do c of x
[18:31] and that is our embedding
[18:33] and that just works
[18:35] now let's construct this layer here the
[18:37] hidden layer
[18:38] so we have that w1 as i'll call it are
[18:42] these weights which we will initialize
[18:44] randomly
[18:46] now the number of inputs to this layer
[18:48] is going to be
[18:49] three times two right because we have
[18:51] two dimensional embeddings and we have
[18:52] three of them
[18:53] so the number of inputs is 6
[18:56] and the number of neurons in this layer
[18:58] is a variable up to us
[19:00] let's use 100 neurons as an example
[19:02] and then biases
[19:04] will be also initialized randomly as an
[19:06] example
[19:07] and let's and we just need 100 of them
[19:11] now the problem with this is we can't
[19:13] simply normally we would take the input
[19:15] in this case that's embedding and we'd
[19:17] like to multiply it with these weights
[19:20] and then we would like to add the bias
[19:22] this is roughly what we want to do
[19:24] but the problem here is that these
[19:25] embeddings are stacked up in the
[19:27] dimensions of this input tensor
[19:29] so this will not work this matrix
[19:31] multiplication because this is a shape
[19:32] 32 by 3 by 2 and i can't multiply that
[19:35] by 6 by 100
[19:37] so somehow we need to concatenate these
[19:40] inputs here together so that we can do
[19:41] something along these lines which
[19:43] currently does not work
[19:45] so how do we transform this 32 by 3 by 2
[19:47] into a 32 by 6 so that we can actually
[19:50] perform
[19:51] this
[19:52] multiplication over here i'd like to
[19:54] show you that there are usually many
[19:56] ways of
[19:57] implementing what you'd like to do in
[19:59] torch and some of them will be faster
[20:01] better shorter etc
[20:03] and that's because torch is a very large
[20:06] library and it's got lots and lots of
[20:07] functions so if you just go to the
[20:09] documentation and click on torch you'll
[20:11] see that my slider here is very tiny and
[20:14] that's because there are so many
[20:15] functions that you can call on these
[20:16] tensors
[20:17] to transform them create them multiply
[20:20] them add them perform all kinds of
[20:22] different operations on them
[20:24] and so this is kind of like
[20:28] the space of possibility if you will
[20:31] now one of the things that you can do is
[20:32] if we can control here ctrl f for
[20:34] concatenate and we see that there's a
[20:36] function
[20:37] torque.cat short for concatenate
[20:40] and this concatenates the given sequence
[20:42] of tensors in a given dimension
[20:45] and these sensors must have the same
[20:46] shape etc so we can use the concatenate
[20:49] operation to in a naive way concatenate
[20:52] these three embeddings for each input
[20:56] so in this case we have m of
[20:58] amp of the shape and really what we want
[21:01] to do is we want to retrieve these three
[21:03] parts and concatenate them
[21:04] so we want to grab all the examples
[21:08] we want to grab
[21:10] first the zeroth
[21:13] index
[21:14] and then all of
[21:16] this
[21:17] so this plucks out
[21:20] the 32 by 2 embeddings of just the first
[21:24] word here
[21:26] and so basically we want this guy
[21:28] we want the first dimension and we want
[21:30] the second dimension
[21:32] and these are the three pieces
[21:34] individually
[21:36] and then we want to treat this as a
[21:38] sequence and we want to torch that cat
[21:40] on that sequence so this is the list
[21:43] tor.cat takes a
[21:45] sequence
[21:46] of tensors and then we have to tell it
[21:48] along which dimension to concatenate
[21:51] so in this case all these are 32 by 2
[21:53] and we want to concatenate not across
[21:55] dimension 0 by the cross dimension one
[21:58] so passing in one
[22:00] gives us a result
[22:01] the shape of this is 32 by 6 exactly as
[22:04] we'd like
[22:05] so that basically took 32 and squashed
[22:08] these by concatenating them into 32 by
[22:10] 6.
[22:11] now this is kind of ugly because this
[22:13] code would not generalize if we want to
[22:15] later change the block size right now we
[22:17] have three inputs
[22:19] three words but what if we had five
[22:22] then here we would have to change the
[22:23] code because i'm indexing directly well
[22:26] torch comes to rescue again because that
[22:28] turns out to be a function called unbind
[22:31] and it removes a tensor dimension
[22:35] so it removes the tensor dimension
[22:36] returns a tuple of all slices along a
[22:39] given dimension
[22:40] without it
[22:41] so this is exactly what we need
[22:43] and basically when we call torch dot
[22:45] unbind
[22:48] torch dot unbind
[22:50] of m
[22:51] and pass in dimension
[22:54] 1 index 1
[22:56] this gives us a list of
[22:59] a list of tensors exactly equivalent to
[23:01] this
[23:02] so running this
[23:04] gives us a line
[23:06] 3
[23:07] and it's exactly this list and so we can
[23:09] call torch.cat on it
[23:12] and along the first dimension
[23:14] and this works
[23:16] and this shape is the same
[23:19] but now this is uh it doesn't matter if
[23:21] we have block size 3 or 5 or 10 this
[23:23] will just work
[23:24] so this is one way to do it but it turns
[23:26] out that in this case there's actually a
[23:28] significantly better and more efficient
[23:30] way and this gives me an opportunity to
[23:32] hint at some of the internals of
[23:34] torch.tensor
[23:36] so let's create
[23:37] an array here
[23:40] of elements from 0 to 17
[23:42] and the shape of this
[23:44] is just 18. it's a single picture of 18
[23:46] numbers
[23:47] it turns out that we can very quickly
[23:49] re-represent this as different sized and
[23:52] dimensional tensors
[23:54] we do this by calling a view
[23:57] and we can say that actually this is not
[23:59] a single vector of 18 this is a two by
[24:02] nine tensor or alternatively this is a
[24:05] nine by two tensor
[24:08] or this is actually a three by three by
[24:10] two tensor
[24:11] as long as the total number of elements
[24:13] here multiply to be the same
[24:16] this will just work and
[24:18] in pytorch this operation calling that
[24:21] view is extremely efficient
[24:24] and the reason for that is that
[24:26] in each tensor there's something called
[24:27] the underlying storage
[24:30] and the storage is just the numbers
[24:32] always as a one-dimensional vector and
[24:34] this is how this tensor is represented
[24:37] in the computer memory it's always a
[24:38] one-dimensional vector
[24:41] but when we call that view we are
[24:44] manipulating some of attributes of that
[24:46] tensor that dictate how this
[24:48] one-dimensional sequence is interpreted
[24:50] to be an n-dimensional tensor
[24:53] and so what's happening here is that no
[24:55] memory is being changed copied moved or
[24:57] created when we call that view the
[24:59] storage
[25:00] is identical but when you call that view
[25:03] some of the internal
[25:05] attributes of the view of the sensor are
[25:07] being manipulated and changed in
[25:09] particular that's something there's
[25:10] something called a storage offset
[25:12] strides and shapes and those are
[25:14] manipulated so that this one-dimensional
[25:16] sequence of bytes is seen as different
[25:18] and dimensional arrays
[25:20] there's a blog post here from eric
[25:22] called pi torch internals where he goes
[25:25] into some of this with respect to tensor
[25:27] and how the view of the tensor is
[25:29] represented
[25:30] and this is really just like a logical
[25:32] construct of representing the physical
[25:34] memory
[25:35] and so this is a pretty good um blog
[25:38] post that you can go into i might also
[25:40] create an entire video on the internals
[25:41] of torch tensor and how this works
[25:44] for here we just note that this is an
[25:46] extremely efficient operation
[25:48] and if i delete this and come back to
[25:50] our end
[25:53] we see that the shape of our end is 32
[25:55] by three by two but we can simply
[25:58] ask for pytorch to view this instead as
[26:00] a 32 by six
[26:02] and the way this gets flattened into a
[26:05] 32 by six array
[26:07] just happens that
[26:09] these two
[26:10] get stacked up
[26:12] in a single row and so that's basically
[26:14] the concatenation operation that we're
[26:15] after
[26:17] and you can verify that this actually
[26:18] gives the exact same result as what we
[26:20] had before
[26:22] so this is an element y equals and you
[26:23] can see that all the elements of these
[26:25] two tensors are the same
[26:27] and so we get the exact same result
[26:30] so long story short we can actually just
[26:33] come here
[26:34] and if we just view this as a 32x6
[26:38] instead then this multiplication will
[26:40] work and give us the hidden states that
[26:43] we're after
[26:44] so if this is h
[26:45] then h shape is now
[26:48] the 100 dimensional activations for
[26:51] every one of our 32 examples
[26:53] and this gives the desired result let me
[26:55] do two things here number one let's not
[26:57] use 32 we can for example do something
[27:00] like
[27:03] m.shape at 0
[27:05] so that we don't hard code these numbers
[27:07] and this would work for any size of this
[27:09] amp
[27:10] or alternatively we can also do negative
[27:11] one when we do negative one pi torch
[27:14] will infer what this should be
[27:16] because the number of elements must be
[27:17] the same and we're saying that this is 6
[27:20] by church will derive that this must be
[27:21] 32 or whatever else it is if m is of
[27:24] different size
[27:26] the other thing is here um
[27:29] one more thing i'd like to point out is
[27:33] here when we do the concatenation
[27:35] this actually is much less efficient
[27:37] because um this concatenation would
[27:39] create a whole new tensor with a whole
[27:41] new storage so new memory is being
[27:43] created because there's no way to
[27:44] concatenate tensors just by manipulating
[27:46] the view attributes
[27:48] so this is inefficient and creates all
[27:50] kinds of new memory
[27:52] uh so let me delete this now
[27:55] we don't need this
[27:57] and here to calculate h we want to also
[27:59] dot 10h
[28:01] of this
[28:02] to get our
[28:04] oops to get our h
[28:07] so these are now numbers between
[28:08] negative one and one because of the 10h
[28:10] and we have
[28:11] that the shape is 32 by 100
[28:14] and that is basically this hidden layer
[28:15] of activations here
[28:17] for every one of our 32 examples
[28:20] now there's one more thing i've lost
[28:21] over that we have to be very careful
[28:23] with and that this
[28:24] and that's this plus here
[28:26] in particular we want to make sure that
[28:27] the broadcasting will do what we like
[28:30] the shape of this is 32 by 100 and the
[28:33] ones shape is 100.
[28:35] so we see that the addition here will
[28:37] broadcast these two and in particular we
[28:40] have 32 by 100 broadcasting to 100.
[28:44] so broadcasting will align on the right
[28:47] create a fake dimension here so this
[28:49] will become a 1 by 100 row vector and
[28:51] then it will copy vertically
[28:54] for every one of these rows of 32 and do
[28:57] an element wise addition
[28:58] so in this case the correct thing will
[29:00] be happening because the same bias
[29:02] vector will be added to all the rows
[29:05] of
[29:06] this matrix so that is correct that's
[29:08] what we'd like and it's always good
[29:11] practice you just make sure
[29:12] so that you don't shoot yourself in the
[29:14] foot and finally let's create the final
[29:16] layer here
[29:17] so let's create
[29:19] w2 and v2
[29:22] the input now is 100
[29:24] and the output number of neurons will be
[29:26] for us 27 because we have 27 possible
[29:29] characters that come next
[29:31] so the biases will be 27 as well
[29:34] so therefore the logits which are the
[29:36] outputs of this neural net
[29:38] are going to be um
[29:41] h
[29:42] multiplied by w2 plus b2
[29:47] logistic shape is 32 by 27
[29:50] and the logits look
[29:52] good now exactly as we saw in the
[29:54] previous video we want to take these
[29:55] logits and we want to first exponentiate
[29:58] them to get our fake counts
[30:00] and then we want to normalize them into
[30:01] a probability
[30:02] so prob is counts divide
[30:05] and now
[30:06] counts dot sum along the first dimension
[30:10] and keep them as true exactly as in the
[30:12] previous video
[30:14] and so
[30:16] prob that shape now is 32 by 27
[30:20] and you'll see that every row of prob
[30:23] sums to one so it's normalized
[30:26] so that gives us the probabilities now
[30:28] of course we have the actual letter that
[30:30] comes next and that comes from this
[30:32] array y
[30:34] which we which we created during the
[30:36] dataset creation so why is this last
[30:39] piece here which is the identity of the
[30:40] next character in the sequence that we'd
[30:42] like to now predict
[30:44] so what we'd like to do now is just as
[30:46] in the previous video we'd like to index
[30:48] into the rows of prob and in each row
[30:51] we'd like to pluck out the probability
[30:52] assigned to the correct character
[30:55] as given here
[30:56] so first we have torch.range of 32 which
[31:00] is kind of like a iterator over
[31:03] numbers from 0 to 31
[31:05] and then we can index into prob in the
[31:07] following way
[31:09] prop in
[31:10] torch.range of 32 which iterates the
[31:12] roads and in each row we'd like to grab
[31:15] this column as given by y
[31:19] so this gives the current probabilities
[31:21] as assigned by this neural network with
[31:23] this setting of its weights
[31:24] to the correct character in the sequence
[31:27] and you can see here that this looks
[31:29] okay for some of these characters like
[31:30] this is basically 0.2
[31:32] but it doesn't look very good at all for
[31:34] many other characters like this is
[31:37] 0.0701 probability and so the network
[31:40] thinks that some of these are extremely
[31:42] unlikely but of course we haven't
[31:43] trained the neural network yet so
[31:47] this will improve and ideally all of
[31:48] these numbers here of course are one
[31:50] because then we are correctly predicting
[31:52] the next character
[31:53] now just as in the previous video we
[31:55] want to take these probabilities we want
[31:57] to look at the lock probability
[31:59] and then we want to look at the average
[32:00] probability
[32:02] and the negative of it to create the
[32:04] negative log likelihood loss
[32:07] so the loss here is 17
[32:10] and this is the loss that we'd like to
[32:11] minimize to get the network to predict
[32:14] the correct character in the sequence
[32:16] okay so i rewrote everything here and
[32:18] made it a bit more respectable so here's
[32:20] our data set here's all the parameters
[32:22] that we defined
[32:24] i'm now using a generator to make it
[32:26] reproducible
[32:27] i clustered all the parameters into a
[32:29] single list of parameters so that for
[32:31] example it's easy to count them and see
[32:33] that in total we currently have about
[32:34] 3400 parameters
[32:37] and this is the forward pass as we
[32:38] developed it
[32:39] and we arrive at a single number here
[32:41] the loss that is currently expressing
[32:44] how well
[32:45] this neural network works with the
[32:46] current setting of parameters
[32:48] now i would like to make it even more
[32:50] respectable so in particular see these
[32:52] lines here where we take the logits and
[32:54] we calculate the loss
[32:57] we're not actually reinventing the wheel
[32:59] here this is just um
[33:01] classification and many people use
[33:03] classification and that's why there is a
[33:05] functional.cross entropy function in
[33:07] pytorch to calculate this much more
[33:09] efficiently
[33:10] so we can just simply call f.cross
[33:12] entropy
[33:13] and we can pass in the logits and we can
[33:14] pass in the
[33:16] array of targets y
[33:18] and this calculates the exact same loss
[33:22] so in fact we can simply put this here
[33:25] and erase these three lines and we're
[33:27] going to get the exact same result now
[33:29] there are actually many good reasons to
[33:31] prefer f.cross entropy over rolling your
[33:34] own implementation like this i did this
[33:36] for educational reasons but you'd never
[33:37] use this in practice why is that
[33:40] number one when you use f.cross entropy
[33:42] by torch will not actually create all
[33:44] these intermediate tensors because these
[33:46] are all new tensors in memory and all
[33:49] this is fairly inefficient to run like
[33:51] this instead pytorch will cluster up all
[33:54] these operations and very often create
[33:56] have fused kernels that very efficiently
[33:59] evaluate these expressions that are sort
[34:01] of like clustered mathematical
[34:03] operations
[34:04] number two the backward pass can be made
[34:06] much more efficient and not just because
[34:08] it's a fused kernel but also
[34:10] analytically and mathematically it's
[34:12] much it's often a very much simpler
[34:15] backward pass to implement
[34:17] we actually sell this with micrograd
[34:19] you see here when we implemented 10h the
[34:22] forward pass of this operation to
[34:23] calculate the 10h was actually a fairly
[34:25] complicated mathematical expression
[34:28] but because it's a clustered
[34:29] mathematical expression when we did the
[34:31] backward pass we didn't individually
[34:33] backward through the x and the two times
[34:35] and the minus one in division etc we
[34:38] just said it's one minus t squared and
[34:40] that's a much simpler mathematical
[34:42] expression
[34:43] and we were able to do this because
[34:44] we're able to reuse calculations and
[34:46] because we are able to mathematically
[34:48] and analytically derive the derivative
[34:50] and often that expression simplifies
[34:52] mathematically and so there's much less
[34:54] to implement
[34:56] so not only can can it be made more
[34:57] efficient because it runs in a fused
[34:59] kernel but also because the expressions
[35:01] can take a much simpler form
[35:03] mathematically
[35:05] so that's number one number two
[35:08] under the hood f that cross entropy can
[35:10] also be significantly more um
[35:13] numerically well behaved let me show you
[35:15] an example of how this works
[35:19] suppose we have a logits of negative 2 3
[35:21] negative 3 0 and 5
[35:24] and then we are taking the exponent of
[35:25] it and normalizing it to sum to 1.
[35:27] so
[35:28] when logits take on this values
[35:30] everything is well and good and we get a
[35:31] nice probability distribution
[35:33] now consider what happens when some of
[35:35] these logits take on more extreme values
[35:37] and that can happen during optimization
[35:38] of the neural network
[35:40] suppose that some of these numbers grow
[35:42] very negative like say negative 100
[35:45] then actually everything will come out
[35:47] fine we still get the probabilities that
[35:49] um
[35:50] you know are well behaved and they sum
[35:52] to one and everything is great
[35:54] but because of the way the x works if
[35:56] you have very positive logits let's say
[35:58] positive 100 in here
[36:00] you actually start to run into trouble
[36:02] and we get not a number here
[36:04] and the reason for that is that these
[36:06] counts
[36:08] have an if here
[36:10] so if you pass in a very negative number
[36:12] to x you just get a very negative sorry
[36:15] not negative but very small number very
[36:17] very near zero and that's fine
[36:19] but if you pass in a very positive
[36:21] number suddenly we run out of range in
[36:23] our floating point number that
[36:25] represents these counts
[36:28] so basically we're taking e and we're
[36:29] raising it to the power of 100 and that
[36:31] gives us if because we run out of
[36:34] dynamic range on this floating point
[36:35] number that is count
[36:37] and so we cannot pass very large logits
[36:41] through this expression
[36:43] now let me reset these numbers to
[36:45] something reasonable
[36:47] the way pi torch solved this
[36:49] is that
[36:50] you see how we have a well-behaved
[36:52] result here
[36:53] it turns out that because of the
[36:54] normalization here you can actually
[36:56] offset logits by any arbitrary constant
[36:59] value that you want so if i add 1 here
[37:02] you actually get the exact same result
[37:04] or if i add 2
[37:06] or if i subtract three
[37:08] any offset will produce the exact same
[37:10] probabilities
[37:12] so because negative numbers are okay but
[37:15] positive numbers can actually overflow
[37:16] this x what patrick does is it
[37:19] internally calculates the maximum value
[37:21] that occurs in the logits and it
[37:23] subtracts it so in this case it would
[37:25] subtract five
[37:26] and so therefore the greatest number in
[37:28] logits will become zero and all the
[37:30] other numbers will become some negative
[37:32] numbers
[37:33] and then the result of this is always
[37:35] well behaved so even if we have 100 here
[37:37] previously
[37:39] not good but because pytorch will
[37:41] subtract 100 this will work
[37:44] and so there's many good reasons to call
[37:46] cross-entropy number one the forward
[37:49] pass can be much more efficient the
[37:50] backward pass can be much more efficient
[37:53] and also things can be much more
[37:54] numerically well behaved okay so let's
[37:57] now set up the training of this neural
[37:58] net
[37:59] we have the forward pass
[38:02] uh we don't need these
[38:04] is that we have the losses equal to the
[38:06] f.cross entropy that's the forward pass
[38:09] then we need the backward pass first we
[38:12] want to set the gradients to be zero so
[38:14] for p in parameters
[38:16] we want to make sure that p dot grad is
[38:18] none which is the same as setting it to
[38:19] zero in pi torch
[38:20] and then lost that backward to populate
[38:23] those gradients
[38:24] once we have the gradients we can do the
[38:25] parameter update so for p in parameters
[38:28] we want to take all the
[38:30] data and we want to nudge it
[38:32] learning rate times p dot grad
[38:36] and then we want to repeat this
[38:39] a few times
[38:43] and let's print the loss here as well
[38:48] now this won't suffice and it will
[38:50] create an error because we also have to
[38:51] go for pn parameters
[38:53] and we have to make sure that p dot
[38:55] requires grad is set to true in pi torch
[38:59] and this should just work
[39:03] okay so we started off with loss of 17
[39:05] and we're decreasing it
[39:08] let's run longer
[39:10] and you see how the loss decreases
[39:12] a lot here so
[39:17] if we just run for a thousand times
[39:19] we get a very very low loss and that
[39:21] means that we're making very good
[39:22] predictions now the reason that this is
[39:25] so straightforward right now
[39:27] is because we're only um
[39:29] overfitting 32 examples
[39:32] so we only have 32 examples uh of the
[39:34] first five words
[39:36] and therefore it's very easy to make
[39:37] this neural net fit only these two 32
[39:40] examples because we have 3 400
[39:43] parameters and only 32 examples so we're
[39:46] doing what's called overfitting a single
[39:48] batch of the data
[39:50] and getting a very low loss and good
[39:52] predictions
[39:53] um but that's just because we have so
[39:55] many parameters for so few examples so
[39:56] it's easy to
[39:58] uh make this be very low
[40:00] now we're not able to achieve exactly
[40:01] zero
[40:02] and the reason for that is we can for
[40:04] example look at logits which are being
[40:06] predicted
[40:08] and we can look at the max along the
[40:11] first dimension
[40:13] and in pi torch
[40:15] max reports both the actual values that
[40:17] take on the maximum number but also the
[40:20] indices of piece
[40:22] and you'll see that the indices are very
[40:23] close to the labels
[40:26] but in some cases they differ
[40:28] for example in this very first example
[40:31] the predicted index is 19 but the label
[40:33] is five
[40:35] and we're not able to make loss be zero
[40:36] and fundamentally that's because here
[40:40] the very first or the zeroth index is
[40:42] the example where dot dot dot is
[40:44] supposed to predict e but you see how
[40:46] dot dot dot is also supposed to predict
[40:48] an o and dot dot is also supposed to
[40:50] predict an i and then s as well and so
[40:53] basically e o a or s are all possible
[40:57] outcomes in a training set for the exact
[40:59] same input so we're not able to
[41:01] completely over fit and um
[41:03] and make the loss be exactly zero so but
[41:06] we're getting very close in the cases
[41:08] where
[41:09] there's a unique input for a unique
[41:11] output in those cases we do what's
[41:13] called overfit and we basically get the
[41:15] exact same and the exact correct result
[41:19] so now all we have to do
[41:21] is we just need to make sure that we
[41:22] read in the full data set and optimize
[41:24] the neural net
[41:25] okay so let's swing back up
[41:27] where we created the dataset
[41:29] and we see that here we only use the
[41:30] first five words so let me now erase
[41:32] this
[41:33] and let me erase the print statements
[41:35] otherwise we'd be printing way too much
[41:38] and so when we processed the full data
[41:39] set of all the words we now had 228 000
[41:42] examples instead of just 32.
[41:45] so let's now scroll back down
[41:47] to this is much larger reinitialize the
[41:50] weights the same number of parameters
[41:52] they all require gradients
[41:54] and then let's push this print out
[41:56] lost.item to be here
[41:58] and let's just see how the optimization
[41:59] goes if we run this
[42:04] okay so we started with a fairly high
[42:05] loss and then as we're optimizing the
[42:07] loss is coming down
[42:12] but you'll notice that it takes quite a
[42:13] bit of time for every single iteration
[42:15] so let's actually address that because
[42:17] we're doing way too much work forwarding
[42:19] and backwarding 220 000 examples
[42:22] in practice what people usually do is
[42:24] they perform forward and backward pass
[42:26] and update on many batches of the data
[42:29] so what we will want to do is we want to
[42:31] randomly select some portion of the data
[42:33] set and that's a mini batch and then
[42:35] only forward backward and update on that
[42:37] little mini batch and then
[42:40] we iterate on those many batches
[42:42] so in pytorch we can for example use
[42:43] storage.randint
[42:45] we can generate numbers between 0 and 5
[42:47] and make 32 of them
[42:52] i believe the size has to be a
[42:54] tuple
[42:56] in my torch
[42:57] so we can have a tuple 32 of numbers
[43:00] between zero and five but actually we
[43:02] want x dot shape of zero here
[43:05] and so this creates uh integers that
[43:08] index into our data set and there's 32
[43:10] of them so if our mini batch size is 32
[43:13] then we can come here and we can first
[43:14] do a mini batch
[43:18] construct
[43:20] so in the integers that we want to
[43:22] optimize in this
[43:24] single iteration
[43:25] are in the ix
[43:27] and then we want to index into
[43:29] x
[43:30] with ix to only grab those rows
[43:34] so we're only getting 32 rows of x
[43:36] and therefore embeddings will again be
[43:38] 32 by three by two not two hundred
[43:40] thousand by three by two
[43:43] and then this ix has to be used not just
[43:45] to index into x
[43:46] but also to index into y
[43:50] and now this should be many batches and
[43:52] this should be much much faster so
[43:55] okay so it's instant almost
[43:57] so this way we can run many many
[44:00] examples
[44:01] nearly instantly and decrease the loss
[44:03] much much faster
[44:05] now because we're only dealing with mini
[44:07] batches the quality of our gradient is
[44:09] lower so the direction is not as
[44:11] reliable it's not the actual gradient
[44:13] direction
[44:14] but the gradient direction is good
[44:16] enough even when it's estimating on only
[44:18] 32 examples that it is useful and so
[44:22] it's much better to have an approximate
[44:24] gradient and just make more steps than
[44:26] it is to evaluate the exact gradient and
[44:29] take fewer steps so that's why in
[44:31] practice uh this works quite well
[44:34] so let's now continue the optimization
[44:38] let me take out this lost item from here
[44:41] and uh
[44:42] place it over here at the end
[44:45] okay so we're hovering around 2.5 or so
[44:50] however this is only the loss for that
[44:51] mini batch so let's actually evaluate
[44:53] the loss
[44:54] here
[44:56] for all of x
[44:58] and for all of y just so we have a
[45:00] full sense of exactly how all the model
[45:03] is doing right now
[45:05] so right now we're at about 2.7 on the
[45:07] entire training set
[45:09] so let's
[45:10] run the optimization for a while
[45:12] okay right 2.6
[45:15] 2.57
[45:17] 2.53
[45:21] okay
[45:22] so one issue of course is we don't know
[45:25] if we're stepping too slow or too fast
[45:28] so this point one i just guessed it
[45:30] so one question is how do you determine
[45:33] this learning rate
[45:34] and how do we gain confidence that we're
[45:37] stepping in the right
[45:39] sort of speed so i'll show you one way
[45:41] to determine a reasonable learning rate
[45:43] it works as follows let's reset our
[45:46] parameters
[45:47] to the initial
[45:49] settings
[45:51] and now let's
[45:52] print in every step
[45:54] but let's only do 10 steps or so
[45:58] or maybe maybe 100 steps
[46:00] we want to find like a very reasonable
[46:02] set
[46:03] search range if you will so for example
[46:05] if this is like very low
[46:07] then
[46:09] we see that the loss is barely
[46:11] decreasing so that's not
[46:13] that's like too low basically so let's
[46:15] try
[46:16] this one
[46:18] okay so we're decreasing the loss but
[46:20] like not very quickly so that's a pretty
[46:21] good low range
[46:23] now let's reset it again
[46:25] and now let's try to find the place at
[46:27] which the loss kind of explodes
[46:29] uh so maybe at negative one
[46:33] okay we see that we're minimizing the
[46:34] loss but you see how uh it's kind of
[46:36] unstable it goes up and down quite a bit
[46:39] um so negative one is probably like a
[46:41] fast learning rate let's try negative
[46:44] 10.
[46:45] okay so this
[46:46] isn't optimizing this is not working
[46:48] very well so negative 10 is way too big
[46:50] negative one was already kind of big
[46:53] um
[46:54] so therefore
[46:56] negative one was like somewhat
[46:57] reasonable if i reset
[47:00] so i'm thinking that the right learning
[47:01] rate is somewhere between
[47:03] uh negative zero point zero zero one and
[47:05] um
[47:06] negative one
[47:08] so the way we can do this here is we can
[47:10] use uh torch shot lens space
[47:13] and we want to basically do something
[47:14] like this between zero and one but
[47:17] um
[47:19] those number of steps is one more
[47:20] parameter that's required let's do a
[47:22] thousand steps this creates 1000
[47:26] numbers between 0.01 and 1
[47:29] but it doesn't really make sense to step
[47:31] between these linearly so instead let me
[47:33] create learning rate exponent
[47:36] and instead of 0.001 this will be a
[47:39] negative 3 and this will be a zero
[47:41] and then the actual lrs that we want to
[47:43] search over are going to be 10 to the
[47:45] power of lre
[47:48] so now what we're doing is we're
[47:49] stepping linearly between the exponents
[47:51] of these learning rates this is 0.001
[47:54] and this is 1 because 10 to the power of
[47:57] 0 is 1.
[47:58] and therefore we are spaced
[48:00] exponentially in this interval
[48:02] so these are the candidate learning
[48:03] rates
[48:04] that we want to sort of like search over
[48:06] roughly
[48:07] so now what we're going to do is
[48:10] here we are going to run the
[48:12] optimization for 1000 steps
[48:14] and instead of using a fixed number
[48:16] we are going to use learning rate
[48:19] indexing into here lrs of i
[48:22] and make this i
[48:25] so basically let me reset this to be
[48:28] again starting from random
[48:30] creating these learning rates between
[48:32] negative
[48:33] zero points between 0.001 and um
[48:36] one but exponentially stopped
[48:39] and here what we're doing is we're
[48:41] iterating a thousand times
[48:43] we're going to use the learning rate
[48:45] um that's in the beginning very very low
[48:48] in the beginning is going to be 0.001
[48:50] but by the end it's going to be
[48:52] 1.
[48:53] and then we're going to step with that
[48:55] learning rate
[48:57] and now what we want to do is we want to
[48:58] keep track of the uh
[49:04] learning rates that we used and we want
[49:05] to look at the losses
[49:07] that resulted
[49:09] and so here let me
[49:12] track stats
[49:14] so lri.append lr
[49:16] and um lost side that append
[49:20] loss that item
[49:22] okay
[49:23] so again reset everything
[49:27] and then run
[49:30] and so basically we started with a very
[49:31] low learning rate and we went all the
[49:33] way up to a learning rate of negative
[49:35] one
[49:35] and now what we can do is we can plt
[49:37] that plot
[49:38] and we can plot the two so we can plot
[49:41] the learning rates on the x-axis and the
[49:43] losses we saw on the y-axis
[49:46] and often you're going to find that your
[49:48] plot looks something like this
[49:50] where in the beginning
[49:52] you had very low learning rates so
[49:53] basically anything
[49:54] barely anything happened
[49:57] then we got to like a nice spot here
[50:00] and then as we increase the learning
[50:01] rate enough
[50:02] we basically started to be kind of
[50:04] unstable here
[50:05] so a good learning rate turns out to be
[50:07] somewhere around here
[50:10] um and because we have lri here
[50:13] um
[50:14] we actually may want to
[50:16] um
[50:19] do not lr
[50:20] not the learning rate but the exponent
[50:22] so that would be the lre at i is maybe
[50:25] what we want to log so let me reset this
[50:27] and redo that calculation
[50:30] but now on the x axis we have the
[50:32] [Music]
[50:34] exponent of the learning rate and so we
[50:36] can see the exponent of the learning
[50:37] rate that is good to use it would be
[50:38] sort of like roughly in the valley here
[50:41] because here the learning rates are just
[50:42] way too low and then here where we
[50:44] expect relatively good learning rates
[50:46] somewhere here and then here things are
[50:47] starting to explode so somewhere around
[50:50] negative one x the exponent of the
[50:51] learning rate is a pretty good setting
[50:54] and 10 to the negative one is 0.1 so 0.1
[50:58] is actually 0.1 was actually a fairly
[50:59] good learning rate around here
[51:02] and that's what we had in the initial
[51:03] setting
[51:05] but that's roughly how you would
[51:06] determine it and so here now we can take
[51:09] out the tracking of these
[51:12] and we can just simply set lr to be 10
[51:15] to the negative one or
[51:18] basically otherwise 0.1 as it was before
[51:20] and now we have some confidence that
[51:21] this is actually a fairly good learning
[51:23] rate
[51:24] and so now we can do is we can crank up
[51:26] the iterations
[51:27] we can reset our optimization
[51:30] and
[51:32] we can run for a pretty long time using
[51:34] this learning rate
[51:36] oops and we don't want to print that's
[51:38] way too much printing
[51:40] so let me again reset
[51:42] and run ten thousand stops
[51:48] okay so we're 0.2 2.48 roughly let's run
[51:52] another 10 000 steps
[51:58] 2.46
[52:00] and now let's do one learning rate decay
[52:02] what this means is we're going to take
[52:03] our learning rate and we're going to 10x
[52:05] lower it and so we're at the late stages
[52:08] of training potentially and we may want
[52:10] to go a bit slower let's do one more
[52:12] actually at 0.1 just to see if
[52:16] we're making a dent here
[52:18] okay we're still making dent and by the
[52:20] way the
[52:21] bi-gram loss that we achieved last video
[52:24] was 2.45 so we've already surpassed the
[52:27] bi-gram model
[52:29] and once i get a sense that this is
[52:30] actually kind of starting to plateau off
[52:32] people like to do as i mentioned this
[52:34] learning rate decay so let's try to
[52:36] decay the loss
[52:37] the learning rate i mean
[52:42] and we achieve it about 2.3 now
[52:46] obviously this is janky and not exactly
[52:48] how you would train it in production but
[52:50] this is roughly what you're going
[52:51] through you first find a decent learning
[52:53] rate using the approach that i showed
[52:54] you
[52:55] then you start with that learning rate
[52:57] and you train for a while
[52:58] and then at the end people like to do a
[53:00] learning rate decay where you decay the
[53:02] learning rate by say a factor of 10 and
[53:03] you do a few more steps and then you get
[53:06] a trained network roughly speaking
[53:08] so we've achieved 2.3 and dramatically
[53:11] improved on the bi-gram language model
[53:13] using this simple neural net as
[53:15] described here
[53:17] using these 3 400 parameters now there's
[53:20] something we have to be careful with
[53:22] i said that we have a better model
[53:24] because we are achieving a lower loss
[53:26] 2.3 much lower than 2.45 with the
[53:28] bi-gram model previously
[53:30] now that's not exactly true and the
[53:32] reason that's not true is that
[53:37] this is actually fairly small model but
[53:39] these models can get larger and larger
[53:41] if you keep adding neurons and
[53:42] parameters so you can imagine that we
[53:44] don't potentially have a thousand
[53:45] parameters we could have 10 000 or 100
[53:47] 000 or millions of parameters
[53:49] and as the capacity of the neural
[53:51] network grows
[53:52] it becomes more and more capable of
[53:54] overfitting your training set
[53:56] what that means is that the loss on the
[53:59] training set on the data that you're
[54:00] training on will become very very low as
[54:03] low as zero
[54:04] but all that the model is doing is
[54:06] memorizing your training set verbatim so
[54:09] if you take that model and it looks like
[54:10] it's working really well but you try to
[54:12] sample from it you will basically only
[54:14] get examples exactly as they are in the
[54:16] training set you won't get any new data
[54:19] in addition to that if you try to
[54:20] evaluate the loss on some withheld names
[54:23] or other words
[54:24] you will actually see that the loss on
[54:26] those can be very high and so basically
[54:29] it's not a good model
[54:30] so the standard in the field is to split
[54:32] up your data set into three splits as we
[54:35] call them we have the training split the
[54:37] dev split or the validation split
[54:40] and the test split
[54:42] so
[54:43] training split
[54:45] test or um sorry dev or validation split
[54:49] and test split and typically this would
[54:53] be say eighty percent of your data set
[54:54] this could be ten percent and this ten
[54:56] percent roughly
[54:58] so you have these three splits of the
[55:00] data
[55:01] now these eighty percent of your
[55:02] trainings of the data set the training
[55:04] set is used to optimize the parameters
[55:06] of the model just like we're doing here
[55:08] using gradient descent
[55:10] these 10 percent of the
[55:12] examples the dev or validation split
[55:14] they're used for development over all
[55:16] the hyper parameters of your model so
[55:19] hyper parameters are for example the
[55:21] size of this hidden layer
[55:22] the size of the embedding so this is a
[55:24] hundred or a two for us but we could try
[55:26] different things
[55:28] the strength of the regularization which
[55:29] we aren't using yet so far
[55:31] so there's lots of different hybrid
[55:33] parameters and settings that go into
[55:34] defining your neural net and you can try
[55:36] many different variations of them and
[55:38] see whichever one works best on your
[55:41] validation split
[55:43] so this is used to train the parameters
[55:45] this is used to train the hyperprimers
[55:48] and test split is used to evaluate
[55:51] basically the performance of the model
[55:53] at the end
[55:54] so we're only evaluating the loss on the
[55:55] test plate very very sparingly and very
[55:57] few times because every single time you
[56:00] evaluate your test loss and you learn
[56:02] something from it
[56:03] you are basically starting to also train
[56:06] on the test split
[56:08] so you are only allowed to test the loss
[56:10] on a test
[56:11] set
[56:13] very very few times otherwise you risk
[56:15] overfitting to it as well as you
[56:17] experiment on your model
[56:19] so let's also split up our training data
[56:22] into train dev and test and then we are
[56:25] going to train on train
[56:26] and only evaluate on tests very very
[56:28] sparingly okay so here we go
[56:31] here is where we took all the words and
[56:33] put them into x and y tensors
[56:36] so instead let me create a new cell here
[56:38] and let me just copy paste some code
[56:39] here
[56:40] because i don't think it's that
[56:43] complex but
[56:45] we're going to try to save a little bit
[56:46] of time
[56:47] i'm converting this to be a function now
[56:49] and this function takes some list of
[56:51] words and builds the arrays x and y for
[56:54] those words only
[56:56] and then here i am shuffling up all the
[56:59] words so these are the input words that
[57:01] we get
[57:02] we are randomly shuffling them all up
[57:04] and then um
[57:06] we're going to
[57:08] set n1 to be
[57:09] the number of examples that there's 80
[57:11] of the words and n2 to be
[57:13] 90
[57:14] of the way of the words so basically if
[57:16] len of words is 32 000 n1 is
[57:21] well sorry i should probably run this
[57:24] n1 is 25 000 and n2 is 28 000.
[57:28] and so here we see that
[57:29] i'm calling build data set to build the
[57:32] training set x and y
[57:33] by indexing into up to and one so we're
[57:36] going to have only 25 000 training words
[57:39] and then we're going to have
[57:42] roughly
[57:44] n2 minus n1
[57:46] 3 3 000 validation examples or dev
[57:49] examples and we're going to have
[57:53] when of words basically minus and two
[57:57] or
[57:58] 3 204 examples
[58:00] here for the test set
[58:03] so
[58:04] now we have x's and y's
[58:07] for all those three splits
[58:13] oh yeah i'm printing their size here
[58:14] inside the function as well
[58:18] but here we don't have words but these
[58:20] are already the individual examples made
[58:22] from those words
[58:25] so let's now scroll down here
[58:27] and the data set now for training is
[58:31] more like this
[58:33] and then when we reset the network
[58:38] when we're training we're only going to
[58:40] be training using x train
[58:43] x train and y train
[58:47] so that's the only thing we're training
[58:49] on
[58:57] let's see where we are on the
[59:00] single batch
[59:02] let's now train maybe a few more steps
[59:08] training neural networks can take a
[59:09] while usually you don't do it inline you
[59:11] launch a bunch of jobs and you wait for
[59:12] them to finish um can take in multiple
[59:15] days and so on
[59:16] luckily this is a very small network
[59:20] okay so the loss is pretty good
[59:23] oh we accidentally used a learning rate
[59:25] that is way too low
[59:27] so let me actually come back
[59:29] we use the decay learning rate of 0.01
[59:35] so this will train much faster
[59:37] and then here when we evaluate
[59:39] let's use the dep set here
[59:42] xdev
[59:43] and ydev to evaluate the loss
[59:47] okay
[59:48] and let's now decay the learning rate
[59:50] and only do say 10 000 examples
[59:55] and let's evaluate the dev loss
[59:57] ones here
[59:59] okay so we're getting about 2.3 on dev
[60:01] and so the neural network when it was
[60:02] training did not see these dev examples
[60:05] it hasn't optimized on them and yet
[60:08] when we evaluate the loss on these dev
[60:10] we actually get a pretty decent loss
[60:12] and so we can also look at what the
[60:16] loss is on all of training set
[60:18] oops
[60:20] and so we see that the training and the
[60:22] dev loss are about equal so we're not
[60:24] over fitting
[60:25] um this model is not powerful enough to
[60:28] just be purely memorizing the data and
[60:31] so far we are what's called underfitting
[60:33] because the training loss and the dev or
[60:35] test losses are roughly equal so what
[60:38] that typically means is that our network
[60:40] is very tiny very small and we expect to
[60:43] make performance improvements by scaling
[60:46] up the size of this neural net so let's
[60:47] do that now so let's come over here
[60:50] and let's increase the size of the
[60:51] neural net the easiest way to do this is
[60:53] we can come here to the hidden layer
[60:55] which currently has 100 neurons and
[60:56] let's just bump this up so let's do 300
[60:58] neurons
[61:00] and then this is also 300 biases and
[61:03] here we have 300 inputs into the final
[61:05] layer
[61:07] so
[61:08] let's initialize our neural net we now
[61:10] have ten thousand ex ten thousand
[61:12] parameters instead of three thousand
[61:13] parameters
[61:15] and then we're not using this
[61:18] and then here what i'd like to do is i'd
[61:19] like to actually uh keep track of uh
[61:23] tap
[61:24] um
[61:27] okay let's just do this let's keep stats
[61:29] again
[61:30] and here when we're keeping track of the
[61:34] loss let's just also keep track of the
[61:37] steps and let's just have i here
[61:40] and let's train on thirty thousand
[61:44] or rather say
[61:46] okay let's try thirty thousand
[61:48] and we are at point one
[61:51] and
[61:52] we should be able to run this
[61:54] and optimize the neural net
[61:57] and then here basically i want to
[61:59] plt.plot
[62:00] the steps
[62:02] against the loss
[62:09] so these are the x's and y's
[62:11] and this is
[62:13] the loss function and how it's being
[62:15] optimized
[62:16] now you see that there's quite a bit of
[62:18] thickness to this and that's because we
[62:19] are optimizing over these mini batches
[62:21] and the mini batches create a little bit
[62:23] of noise
[62:24] in this
[62:25] uh where are we in the def set we are at
[62:28] 2.5 so we still haven't optimized this
[62:30] neural net very well
[62:32] and that's probably because we made it
[62:33] bigger it might take longer for this
[62:34] neural net to converge
[62:36] um
[62:37] and so let's continue training
[62:40] um
[62:42] yeah let's just continue training
[62:46] one possibility is that the batch size
[62:48] is so low
[62:49] that uh we just have way too much noise
[62:51] in the training and we may want to
[62:53] increase the batch size so that we have
[62:54] a bit more um correct gradient and we're
[62:57] not thrashing too much and we can
[62:59] actually like optimize more properly
[63:07] okay
[63:08] this will now become meaningless because
[63:10] we've reinitialized these so
[63:13] yeah this looks not
[63:14] pleasing right now but there probably is
[63:16] like a tiny improvement but it's so hard
[63:18] to tell
[63:20] let's go again
[63:22] 2.52
[63:25] let's try to decrease the learning rate
[63:27] by factor two
[63:50] okay we're at 2.32
[63:52] let's continue training
[64:05] we basically expect to see a lower loss
[64:07] than what we had before because now we
[64:08] have a much much bigger model and we
[64:10] were under fitting so we'd expect that
[64:12] increasing the size of the model should
[64:14] help the neural net
[64:16] 2.32 okay so that's not happening too
[64:18] well
[64:19] now one other concern is that even
[64:21] though we've made the 10h layer here or
[64:23] the hidden layer much much bigger it
[64:25] could be that the bottleneck of the
[64:26] network right now are these embeddings
[64:28] that are two dimensional it can be that
[64:30] we're just cramming way too many
[64:32] characters into just two dimensions and
[64:34] the neural net is not able to really use
[64:36] that space effectively and that that is
[64:38] sort of like the bottleneck to our
[64:39] network's performance
[64:42] okay 2.23 so just by decreasing the
[64:45] learning rate i was able to make quite a
[64:46] bit of progress let's run this one more
[64:48] time
[64:51] and then evaluate the training and the
[64:53] dev loss
[64:56] now one more thing after training that
[64:58] i'd like to do is i'd like to visualize
[65:00] the um
[65:02] embedding vectors for these
[65:05] characters before we scale up the
[65:07] embedding size from two
[65:09] because we'd like to make uh this
[65:11] bottleneck potentially go away
[65:13] but once i make this greater than two we
[65:15] won't be able to visualize them
[65:17] so here
[65:18] okay we're at 2.23 and 2.24
[65:21] so um we're not improving much more and
[65:24] maybe the bottleneck now is the
[65:25] character embedding size which is two
[65:28] so here i have a bunch of code that will
[65:29] create a figure
[65:31] and then we're going to visualize
[65:34] the embeddings that were trained by the
[65:35] neural net
[65:36] on these characters because right now
[65:38] the embedding has just two so we can
[65:40] visualize all the characters with the x
[65:42] and the y coordinates as the two
[65:43] embedding locations for each of these
[65:46] characters
[65:47] and so here are the x coordinates and
[65:50] the y coordinates which are the columns
[65:51] of c
[65:52] and then for each one i also include the
[65:55] text of the little character
[65:58] so here what we see is actually kind of
[65:59] interesting
[66:02] the network has basically learned to
[66:04] separate out the characters and cluster
[66:06] them a little bit uh so for example you
[66:08] see how the vowels
[66:09] a e i o u are clustered up here
[66:12] so that's telling us that is that the
[66:14] neural net treats these is very similar
[66:16] right because when they feed into the
[66:17] neural net
[66:18] the embedding uh for all these
[66:20] characters is very similar and so the
[66:22] neural net thinks that they're very
[66:23] similar and kind of like interchangeable
[66:25] if that makes sense
[66:27] um
[66:29] then the the points that are like really
[66:31] far away are for example q q is kind of
[66:33] treated as an exception and q has a very
[66:35] special
[66:36] embedding vector so to speak
[66:38] similarly dot which is a special
[66:40] character is all the way out here
[66:42] and a lot of the other letters are sort
[66:44] of like clustered up here and so it's
[66:46] kind of interesting that there's a
[66:48] little bit of structure here
[66:50] after the training
[66:51] and it's not definitely not random and
[66:53] these embeddings make sense
[66:55] so we're now going to scale up the
[66:57] embedding size and won't be able to
[66:59] visualize it directly but we expect that
[67:01] because we're under fitting
[67:03] and we made this layer much bigger and
[67:06] did not sufficiently improve the loss
[67:08] we're thinking that the um
[67:10] constraint to better performance right
[67:12] now could be these embedding pictures so
[67:15] let's make them bigger okay so let's
[67:16] scroll up here
[67:17] and now we don't have two dimensional
[67:19] embeddings we are going to have
[67:21] say 10 dimensional embeddings for each
[67:23] word
[67:25] then
[67:26] this layer will receive 3 times 10 so 30
[67:30] inputs
[67:31] will go into
[67:33] the hidden layer
[67:35] let's also make the hidden layer a bit
[67:37] smaller so instead of 300 let's just do
[67:38] 200 neurons in that hidden layer
[67:41] so now the total number of elements will
[67:43] be slightly bigger at 11 000
[67:47] and then here we have to be a bit
[67:48] careful because um
[67:50] okay the learning rate we set to 0.1
[67:53] here we are hardcoded in six and
[67:56] obviously if you're working in
[67:56] production you don't wanna be
[67:57] hard-coding magic numbers but instead of
[68:00] six this should now be thirty
[68:02] um
[68:04] and let's run for fifty thousand
[68:05] iterations and let me split out the
[68:07] initialization here outside
[68:10] so that when we run this cell multiple
[68:12] times it's not going to wipe out
[68:14] our loss
[68:17] in addition to that
[68:19] here
[68:20] let's instead of logging lost.item let's
[68:22] actually
[68:23] log the
[68:25] let's
[68:26] do log 10
[68:28] i believe that's a function of the loss
[68:32] and i'll show you why in a second let's
[68:34] optimize this
[68:37] basically i'd like to plot the log loss
[68:39] instead of the loss because when you
[68:40] plot the loss many times it can have
[68:42] this hockey stick appearance and log
[68:44] squashes it in
[68:46] uh so it just kind of like looks nicer
[68:48] so the x-axis is step i
[68:51] and the y-axis will be the loss i
[69:00] and then here this is 30.
[69:03] ideally we wouldn't be hard-coding these
[69:08] okay so let's look at the loss
[69:11] okay it's again very thick because the
[69:13] mini batch size is very small but the
[69:15] total loss over the training set is 2.3
[69:18] and the the tests or the def set is 2.38
[69:20] as well
[69:21] so so far so good uh let's try to now
[69:24] decrease the learning rate
[69:25] by a factor of 10
[69:29] and train for another 50 000 iterations
[69:35] we'd hope that we would be able to beat
[69:37] uh 2.32
[69:43] but again we're just kind of like doing
[69:44] this very haphazardly so i don't
[69:46] actually have confidence that our
[69:48] learning rate is set very well that our
[69:50] learning rate decay which we just do
[69:52] at random is set very well
[69:55] and um so the optimization here is kind
[69:57] of suspect to be honest and this is not
[69:59] how you would do it typically in
[70:00] production in production you would
[70:02] create parameters or hyper parameters
[70:04] out of all these settings and then you
[70:05] would run lots of experiments and see
[70:07] whichever ones are working well for you
[70:11] okay
[70:12] so we have 2.17 now and 2.2 okay so you
[70:16] see how the training and the validation
[70:19] performance are starting to slightly
[70:21] slowly depart
[70:23] so maybe we're getting the sense that
[70:24] the neural net
[70:26] is getting good enough or
[70:28] that number of parameters is large
[70:30] enough that we are slowly starting to
[70:32] overfit
[70:34] let's maybe run one more iteration of
[70:36] this
[70:37] and see where we get
[70:41] but yeah basically you would be running
[70:43] lots of experiments and then you are
[70:44] slowly scrutinizing whichever ones give
[70:46] you the best depth performance and then
[70:48] once you find all the
[70:50] hyper parameters that make your dev
[70:51] performance good you take that model and
[70:54] you evaluate the test set performance a
[70:55] single time and that's the number that
[70:57] you report in your paper or wherever
[70:59] else you want to talk about and brag
[71:01] about your model
[71:05] so let's then rerun the plot and rerun
[71:08] the train and death
[71:11] and because we're getting lower loss now
[71:12] it is the case that the embedding size
[71:15] of these was holding us back very likely
[71:20] okay so 2.162.19 is what we're roughly
[71:22] getting
[71:24] so there's many ways to go from many
[71:26] ways to go from here we can continue
[71:28] tuning the optimization
[71:30] we can continue for example playing with
[71:32] the sizes of the neural net or we can
[71:34] increase the number of uh
[71:36] words or characters in our case that we
[71:38] are taking as an input so instead of
[71:39] just three characters we could be taking
[71:41] more characters as an input and that
[71:43] could further improve the loss
[71:46] okay so i changed the code slightly so
[71:48] we have here 200 000 steps of the
[71:50] optimization and in the first 100 000
[71:52] we're using a learning rate of 0.1 and
[71:54] then in the next 100 000 we're using a
[71:56] learning rate of 0.01
[71:58] this is the loss that i achieve
[72:00] and these are the performance on the
[72:01] training and validation loss
[72:03] and in particular the best validation
[72:05] loss i've been able to obtain in the
[72:07] last 30 minutes or so is 2.17
[72:10] so now i invite you to beat this number
[72:12] and you have quite a few knobs available
[72:14] to you to i think surpass this number
[72:17] so number one you can of course change
[72:18] the number of neurons in the hidden
[72:20] layer of this model you can change the
[72:22] dimensionality of the embedding
[72:24] lookup table
[72:25] you can change the number of characters
[72:26] that are feeding in as an input
[72:29] as the context into this model
[72:32] and then of course you can change the
[72:33] details of the optimization how long are
[72:35] we running what is the learning rate how
[72:37] does it change over time
[72:39] how does it decay
[72:41] you can change the batch size and you
[72:42] may be able to actually achieve a much
[72:44] better convergence speed
[72:46] in terms of
[72:47] how many seconds or minutes it takes to
[72:49] train the model and get
[72:51] your result in terms of really good
[72:54] loss
[72:55] and then of course i actually invite you
[72:57] to read this paper it is 19 pages but at
[72:59] this point you should actually be able
[73:00] to read a good chunk of this paper and
[73:03] understand
[73:04] pretty good chunks of it
[73:06] and this paper also has quite a few
[73:08] ideas for improvements that you can play
[73:09] with
[73:11] so all of those are not available to you
[73:13] and you should be able to beat this
[73:14] number i'm leaving that as an exercise
[73:16] to the reader and that's it for now and
[73:18] i'll see you next time
[73:24] before we wrap up i also wanted to show
[73:25] how you would sample from the model
[73:28] so we're going to generate 20 samples
[73:31] at first we begin with all dots so
[73:33] that's the context
[73:35] and then until we generate
[73:37] the zeroth character again
[73:40] we're going to embed the current context
[73:43] using the embedding table c now usually
[73:47] uh here the first dimension was the size
[73:49] of the training set but here we're only
[73:51] working with a single example that we're
[73:52] generating so this is just the mission
[73:55] one just for simplicity
[73:58] and so this embedding then gets
[74:00] projected into the end state you get the
[74:02] logits
[74:03] now we calculate the probabilities for
[74:05] that you can use f.softmax
[74:09] of logits and that just basically
[74:10] exponentiates the logits and makes them
[74:12] sum to one and similar to cross entropy
[74:15] it is careful that there's no overflows
[74:18] once we have the probabilities we sample
[74:20] from them using torture multinomial to
[74:22] get our next index and then we shift the
[74:24] context window to append the index and
[74:26] record it and then we can just
[74:29] decode all the integers to strings
[74:31] and print them out
[74:33] and so these are some example samples
[74:34] and you can see that the model now works
[74:36] much better so the words here are much
[74:38] more word like or name like so we have
[74:41] things like ham
[74:44] joes
[74:48] you know it's starting to sound a little
[74:49] bit more name-like so we're definitely
[74:51] making progress but we can still improve
[74:53] on this model quite a lot
[74:55] okay sorry there's some bonus content i
[74:57] wanted to mention that i want to make
[74:59] these notebooks more accessible and so i
[75:01] don't want you to have to like install
[75:03] jupyter notebooks and torch and
[75:04] everything else so i will be sharing a
[75:06] link to a google colab
[75:08] and google collab will look like a
[75:10] notebook in your browser and you can
[75:13] just go to the url and you'll be able to
[75:15] execute all of the code that you saw in
[75:17] the google collab and so this is me
[75:20] executing the code in this lecture and i
[75:22] shortened it a little bit but basically
[75:24] you're able to train the exact same
[75:25] network and then plot and sample from
[75:28] the model and everything is ready for
[75:30] you to like tinker with the numbers
[75:31] right there in your browser no
[75:33] installation necessary
[75:35] so i just wanted to point that out and
[75:36] the link to this will be in the video
[75:38] description