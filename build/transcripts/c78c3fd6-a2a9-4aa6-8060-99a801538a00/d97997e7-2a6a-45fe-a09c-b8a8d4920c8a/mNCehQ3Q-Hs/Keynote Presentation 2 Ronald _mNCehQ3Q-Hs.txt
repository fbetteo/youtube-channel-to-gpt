Video Title: Keynote Presentation 2: Ronald Yurko [2020 UConn Sports Analytics Symposium]
Video ID: mNCehQ3Q-Hs
URL: https://www.youtube.com/watch?v=mNCehQ3Q-Hs
View Count: 124

[00:00] um well i'm bryan mcdonald i'm gonna be
[00:02] chairing this session and it's my
[00:04] pleasure
[00:05] to introduce ron yerko
[00:08] um so ron is a phd student in the
[00:11] department of statistics and data
[00:13] science at carnegie mellon um in
[00:16] addition to the work that he does
[00:18] um
[00:20] uh as far as his statistical
[00:22] methodology research
[00:24] with genetics and and genomics um
[00:28] he's a big part of the sports analytics
[00:30] community is is very actively involved
[00:33] in
[00:33] statistics in sports research
[00:37] uh music co-organizer of the
[00:39] carnegie mellon sports analytics
[00:41] conference and research reproducible
[00:44] research competition
[00:46] which is
[00:47] coming up uh pretty soon i believe in a
[00:50] few weeks
[00:51] and
[00:52] he's developed multiple r packages
[00:56] including the nfl scraper package which
[00:59] has been featured in the athletic wall
[01:01] street journal on 538
[01:04] and um ron uh previously worked
[01:08] as a quantitative analyst
[01:10] in finance and also
[01:13] uh as an intern for the pittsburgh
[01:15] pirates and uh he's going to be talking
[01:18] with us today
[01:19] about continuous time
[01:22] um
[01:23] assessments of game outcomes in
[01:27] american football
[01:31] all right thanks brian can you hear me
[01:33] just fine
[01:34] yeah
[01:35] all right
[01:37] so let me share my screen
[01:44] okay can you see my screen just fine
[01:47] just thumbs up
[01:50] yep
[01:51] all right
[01:52] so yeah i just thank you brian for that
[01:54] introduction and also i just want to
[01:56] thank the organizers of the conference
[01:58] um
[01:59] i think this really is a testament to
[02:00] the community to the fact that in the
[02:03] middle of a global pandemic we are still
[02:07] able to connect in this way and you know
[02:09] people from literally all over the world
[02:11] so i mean it's a testament to sports
[02:13] analytics the statistics community so
[02:15] thank you again for just setting this up
[02:17] and uh including me in this pretty
[02:20] awesome lineup of speakers
[02:22] so while the title is this fancy title
[02:25] um
[02:26] i'd say that this is going to be a very
[02:27] broad talk in the sense of
[02:29] thinking about valuing
[02:32] plays in football and how we can
[02:34] ultimately
[02:35] sort of steer towards the direction of
[02:38] better metrics for player evaluation
[02:42] so to begin
[02:44] i just want to start with a big mantra
[02:46] of football analytics that's the lesson
[02:49] that not all yards are created equal
[02:52] so you
[02:54] everybody knows the case of
[02:56] if it's a third down situation and let's
[02:59] say on the left hand side we're looking
[03:01] at a third and 15
[03:03] and you've seen the team they do the
[03:06] typical half back draw and that running
[03:08] back will gain you know like 10 yards
[03:11] and that's great from fantasy points and
[03:13] whatnot but in the end the team ends up
[03:15] punting afterwards
[03:16] and those 10 yards were essentially
[03:19] meaningless
[03:20] maybe some little gain and field
[03:22] position on the punt
[03:24] but on the right hand side we could have
[03:26] another third down situation
[03:28] where
[03:29] the running back or the quarterback
[03:31] converts say two or three yards to then
[03:33] get the first down
[03:34] so those two or three yards are
[03:37] fundamentally a lot more valuable than
[03:39] the 10-yard half-back draw that just
[03:42] concedes for the other team
[03:45] so this is the whole essence of the fact
[03:47] that in football context matters it's
[03:50] not just about
[03:51] the literal event and counting up
[03:53] yardage it's about taking into
[03:55] consideration what's going on in terms
[03:57] of actually fundamentally moving the
[04:00] ball down the field and maintaining
[04:01] possession
[04:03] so this has really led to then two
[04:06] paradigms that are connected to thinking
[04:09] about how do we actually value
[04:11] plays in football
[04:14] so that's expected points and win
[04:16] probability so expected points just
[04:19] saying given all the information i have
[04:22] about the team with possession of the
[04:23] current moment
[04:24] where they're at the field the yards to
[04:26] go
[04:27] down situation
[04:29] possibly time remaining
[04:31] how many how many points
[04:33] is this team expected to score
[04:36] and then win probability is just
[04:38] thinking of okay given all the same
[04:40] information now accounting for something
[04:42] like score differential
[04:44] how likely is the team to win the game
[04:47] and then you can actually
[04:50] value
[04:51] those
[04:52] 10 yards on that third and 15 half back
[04:54] draw where there's two yards on the qb
[04:57] sneak what not by looking at the change
[05:00] in the
[05:01] the the start of the play and the end of
[05:03] the play that leads to these expected
[05:05] points added and win probability added
[05:09] so we actually do to arrive at something
[05:11] like expected points and win probability
[05:14] is we predict different events
[05:17] so
[05:18] for expected points
[05:20] the work i've done previously we we
[05:22] focus on predicting the probability of
[05:26] different possible scoring events that
[05:28] could take place for a team
[05:30] and then we put those together to arrive
[05:33] at expected points so to sort of
[05:36] visually demonstrate this
[05:38] the figures on the top show you
[05:41] it's broken up by down
[05:43] and then you see these lines for each of
[05:46] these possible scoring events by with
[05:48] respect to the team with possession and
[05:51] the color indicates blue being good for
[05:53] the team and then this darker red
[05:55] indicating bad for the team with the
[05:58] ball it means that the opposing team is
[06:00] scoring hence the negative value
[06:03] so then on the y-axis here are the
[06:05] actual predicted probabilities
[06:07] for each of these events and then on the
[06:09] x-axis is the yards away from the
[06:12] opponent's end zone so the closer you
[06:14] are on the left-hand side the more
[06:17] likely the team is to score a touchdown
[06:19] you're
[06:20] the zero indicates the team is right up
[06:23] on the opponent's goal line and they are
[06:25] about to run it in or throw it in so
[06:28] what you could see is on a first down
[06:31] right on your opponent's end zone the
[06:34] highest probability event is
[06:36] the team with the football scoring a
[06:38] touchdown
[06:39] and then as you get further away from
[06:41] the target end zone
[06:43] then the opposing team actually becomes
[06:46] more likely to score and you see then
[06:48] differences by the down situation as
[06:51] well where in the fourth down actually
[06:54] the most likely event
[06:56] is for the team with the football to
[06:58] score a field goal right which makes
[07:01] sense given how teams have played over
[07:03] time and i'll just add this is based on
[07:06] our models from
[07:08] 2009 2017 so this itself could be
[07:11] changing and especially in recent years
[07:15] so what we end up doing then is we take
[07:17] all these probabilities for these
[07:19] different events
[07:20] and then we aggregate them together by
[07:23] just taking the predicted probability
[07:25] multiplying by its associated point
[07:28] value so for touchdowns plug in the
[07:30] seven a field goal three
[07:34] add up those quantities then to arrive
[07:36] at the expected points it's your
[07:38] expected value of a situation so the
[07:41] figure on the bottom then just puts
[07:43] these together
[07:44] again looking at this x-axis yards from
[07:47] opponent's end zone so left-hand side
[07:49] again i'm right up on my opponent's
[07:51] school line
[07:52] and then the y-axis now are the expected
[07:55] points values
[07:56] and then each of these lines
[07:58] are
[07:59] for the different down situations where
[08:02] this bright blue is for first down and
[08:04] then you keep getting a decrease for
[08:07] second third and then finally fourth
[08:09] down
[08:11] and so just for reference we actually
[08:12] include here in this figure
[08:14] classical expected points models from
[08:17] virgil carter and then also the hidden
[08:19] game of football which
[08:21] didn't do any of the fancy modeling that
[08:23] we did but it's still something that's
[08:25] pretty comparable and close to what we
[08:27] have just we get a little bit more uh
[08:30] variability from accounting for more
[08:31] information
[08:34] so this this is expected points and then
[08:37] you could literally view as you move
[08:40] between situations
[08:42] right from say going on a fourth down
[08:45] on the 35 yard line and then converting
[08:48] a one yard to then get a first down you
[08:51] could see then a massive bump in
[08:53] expected points value all right so
[08:55] that's the that's the purpose then of
[08:58] putting this context is we can actually
[09:01] give an appropriate value added for
[09:03] players involved rather than just
[09:06] looking at the yardage that may be
[09:09] really naive
[09:13] now in relation to expected points
[09:15] something people are probably more
[09:16] familiar with are
[09:17] wind probability models so i'll just use
[09:20] an example of a great twitter account by
[09:23] elise sharp who he will tweet out these
[09:27] win probability charts and so this is
[09:31] a summary of a recent
[09:34] meltdown by the atlanta falcons a few
[09:36] weeks ago where they lost the chicago
[09:38] bears and so you could see the wind
[09:41] probability in a sense really gives you
[09:42] a story of games over time
[09:46] all right and they might themselves
[09:48] knight
[09:49] may be failing in some ways and so one
[09:50] of the reasons why i actually chose this
[09:52] example is
[09:53] when we think about wind probability and
[09:56] the tail events the very extreme events
[09:59] and the high wind probability atlanta
[10:02] had and yet they still lost so this is
[10:05] something
[10:06] difficult for people to grasp but we
[10:07] also don't have that much data on these
[10:10] types of games that take place so maybe
[10:13] this could itself be fundamentally
[10:15] flawed and doesn't represent any of the
[10:17] uncertainty in these types of games or
[10:19] outcomes
[10:20] and just for fun another example on the
[10:22] show because i always take joy
[10:25] the best possible outcome that could
[10:26] happen for a pittsburgh steelers fan
[10:28] like myself for teams like cincinnati
[10:31] cleveland philadelphia i mean i'm from
[10:33] pittsburgh so i naturally do everything
[10:36] philadelphia does
[10:37] is when they play each other and neither
[10:39] team wins and so we could see the mess
[10:41] that took place
[10:42] not that long ago between the bengals
[10:44] and the eagles where
[10:46] once again it once it goes to overtime
[10:50] wind probability models it's again a
[10:52] very challenging thing
[10:54] but you could think though in this
[10:56] process right it's a story of these
[10:58] games and so lee has here annotated
[11:01] different events taking place right so
[11:04] we could see on this left-hand side this
[11:06] zackert's catch resulting in a plus 25
[11:10] win probability right we're seeing
[11:12] events that have value
[11:14] in some case then we want to
[11:16] arrive at ways of
[11:18] better quantifying player impact by
[11:20] considering expected points added by
[11:22] considering wind probability added
[11:27] so what the work i've done previously on
[11:30] these expected points and wind
[11:32] probability were the
[11:34] ultimately the result of working on this
[11:36] r package called nfl scraper
[11:39] which
[11:40] became very popular in the football
[11:42] analytics community i think um
[11:45] we've had
[11:46] the the thing that really jumped out
[11:49] just to start with is something like i
[11:51] tweet too much so i get more credit than
[11:53] i deserve but it stems from an original
[11:55] project by max horowitz what he started
[11:58] as our package and with the idea of
[12:01] really
[12:02] hopefully leading to the growth of the
[12:04] community for people to access a common
[12:07] data repository and to make it easier
[12:10] for people to get their hands on data
[12:12] and i really think we've accomplished
[12:14] that and i'm very proud of that work
[12:16] and so
[12:18] but the only reason why it actually
[12:19] grows the way it does is because of the
[12:20] people that use it so i just want to
[12:23] shout out here in terms of it's really
[12:26] grown people like ben baldwin josh
[12:28] hermsmeyer
[12:30] uh
[12:31] kevin cole and these specific accounts
[12:33] for different teams and we've seen
[12:36] people that have used nfl scraper go on
[12:38] to then work for teams work for pro
[12:40] football focus
[12:42] teams in these companies featuring
[12:44] questions about nfl scraper projects and
[12:47] interviews and whatnot
[12:49] and
[12:50] that's why the culmination of our work
[12:52] on expected points and win probability
[12:55] was then to include these in these r
[12:57] packages to allow people to then do
[13:00] more advanced football analytics work
[13:04] because before expected points wind
[13:06] probability were things that were kind
[13:07] of behind closed doors so we could
[13:09] provide that out front and i'm very
[13:11] happy to see that um
[13:13] ben baldwin and sebastian carl have
[13:15] continued this
[13:16] and
[13:17] because of my short attention span as an
[13:19] academic i have to move on from nfl
[13:21] scraper and they have continued working
[13:24] on and releasing an updated version of
[13:26] the package essentially nfl faster
[13:29] with their own models that i may
[13:31] disagree with in some ways but they're
[13:34] providing a great way a lot of tutorials
[13:36] a lot of example code for people to then
[13:39] get their hands on data so if you're
[13:42] interested in getting your hands on
[13:44] nfl play-by-play data and working with
[13:46] it check out the nfl faster package just
[13:50] google nfl faster and you're going to
[13:53] find a lot of tutorials it does a great
[13:55] website
[13:57] so given all this given expected points
[14:00] given when probability
[14:02] and i keep talking about player
[14:03] evaluation
[14:05] so how do we actually do this player
[14:07] evaluation steps how do we go from
[14:09] expected points added and when
[14:11] probability added to really attribute
[14:14] to what what a player did on the field
[14:17] so
[14:18] a fundamental thing about football right
[14:20] talk about context but also talk about
[14:21] the fact that there are 11 players on
[14:24] both sides
[14:26] during during the game right so
[14:29] the funny thing was once we started
[14:31] initially get uh some popularity in nfl
[14:34] scraper
[14:35] the first media hit was
[14:37] pittsburgh post gazette or did this
[14:39] interview and i was talking about it and
[14:41] naturally of course i have to go to the
[14:43] comments section
[14:44] and as it turns out my fellow yinzers
[14:47] provided some great insight in terms of
[14:49] actually how do we evaluate players
[14:52] in terms for football all right and they
[14:55] talk about in this comment the fact that
[14:56] you know you can't statistically
[14:58] evaluate a running back without
[14:59] evaluating his offensive line that's
[15:02] correct
[15:03] same thing with a qb you can't evaluate
[15:05] a qb without evaluating his receivers
[15:08] that's also correct
[15:09] stats can only be helpful when an
[15:11] athlete is doing something completely on
[15:13] his own that's not correct that is why
[15:15] the nfl doesn't go crazy over stats it's
[15:17] a team sport on every single play the
[15:19] only stat that counts the w
[15:21] that's a little mixed but
[15:23] it hits some key points in here right
[15:25] football is a very complex game it's a
[15:28] lot more complicated than baseball it's
[15:30] one of the reasons why you know maybe
[15:32] it's a little bit more difficult for it
[15:34] to sort of adapt to than
[15:36] embracing data and analytics
[15:40] so how do we then take you know some of
[15:42] the concepts in here and how do we
[15:43] actually evaluate players using things
[15:46] like expected points and when
[15:48] probability
[15:49] so the challenge of working with
[15:52] the publicly available uh play-by-play
[15:55] data is you only have information about
[15:59] who is directly involved in a play so
[16:01] i'm passing plays i know who threw the
[16:03] football i know who's the target
[16:05] receiver if it's a catch i know who then
[16:07] made the tackle who intercepted
[16:09] potentially
[16:11] and then you have some context like how
[16:13] far was the pass attempted just a
[16:15] generic bucket of left middle or right
[16:19] if the qb was hit on the play we also
[16:22] have information then for for running
[16:24] plays you know you know who had the
[16:26] football who was it actually doing the
[16:28] run attempt
[16:29] and then we know
[16:31] to which direction they were trying to
[16:33] run with the football all right some
[16:35] information about this run gap between
[16:38] uh the the different alignment but this
[16:41] is again very generic information and
[16:44] it's you know it's mainly documented and
[16:46] charted so it's extremely limited though
[16:49] in the sense of we don't have who is on
[16:51] the field for every play at every moment
[16:55] we don't know what takes place within a
[16:57] play all we're looking at is the start
[16:59] and the end we're ignoring all decision
[17:02] making in between
[17:04] so we
[17:05] when i say we i mean max horowitz sam
[17:08] ventura and myself we did what we could
[17:11] we thought with play-by-play data to try
[17:14] to arrive at a wins above replacement
[17:16] measure by looking at essentially like
[17:19] the average effect
[17:21] that ben roethlisberger has on his
[17:23] passing plays and we we came up with a
[17:26] way of arriving at winds above
[17:28] replacement and this figure on the
[17:30] bottom right here this shows you
[17:33] the change in war
[17:35] over time based on our efforts in this
[17:38] paper
[17:39] that's on the archive
[17:41] you can check out
[17:42] that
[17:44] we could see that
[17:47] uh big ben antonio brown and le'veon
[17:49] bell one of the key takeaways with um
[17:53] and one of the
[17:54] another twitter mantra is the fact that
[17:56] they say running backs don't matter
[17:58] when you know it's just the essence of
[18:01] rushing is less efficient than passing
[18:04] so what we see in terms of our winds
[18:06] above replacement measure in terms of
[18:08] the value added by these different
[18:10] players le'veon bell here
[18:13] in the yellow line
[18:14] he essentially was over this time period
[18:17] 2013 to 2017
[18:20] was like an average wide receiver versus
[18:23] running backs which were on average
[18:25] lower value all right because of his
[18:28] value is really coming from his threat
[18:30] as a receiving running back
[18:33] but again this is something that is
[18:35] extremely limited
[18:37] we don't have any sense of the decision
[18:40] making for instance that le'veon bell
[18:42] would make how much value does he add
[18:45] relative to other running backs when he
[18:47] receives a football
[18:49] on a handoff
[18:50] given context about where linemen are at
[18:53] where where defenders are at we don't
[18:56] know how he does in terms of like the
[18:58] cut he makes the decision he does
[19:01] relative to
[19:03] other running backs right this this is
[19:05] just the start and end between plays
[19:09] what we really want to get at is what
[19:11] happens within a play how can we
[19:13] actually attribute
[19:16] what a player
[19:17] who gets credit for what within a single
[19:20] play itself that's how you're actually
[19:22] going to fundamentally arrive at proper
[19:24] measurements of football player
[19:27] evaluation and
[19:29] it's you see
[19:30] manual efforts to do this with groups
[19:32] like pro football focus where they go
[19:34] through and they try to manually grade
[19:37] players on every player like looking at
[19:40] each individual player what did they do
[19:42] throughout the play and they get a grade
[19:44] how can we actually do that objectively
[19:46] fully objectively just based on data and
[19:48] we can't do that with the play-by-play
[19:50] data
[19:51] but fortunately
[19:54] we were able to attempt this
[19:56] once the big data bull was started and
[19:58] this is all credit due to mike lopez and
[20:01] his efforts and i just want to say that
[20:03] i think this is the type of example this
[20:06] competition that i really hope
[20:09] other leagues will follow in the sense
[20:11] of well i do think hackathons are great
[20:14] there are a lot of very difficult
[20:16] problems of working with tracking data
[20:19] that doing a public release that gives
[20:22] people such as myself and others
[20:24] a nice long period of time to really
[20:27] think about these problems and what we
[20:28] could do with the data
[20:30] really awesome opportunities then to
[20:32] work on challenging difficult problems
[20:35] and so this big dateable release
[20:38] the the first one back in 2018 it
[20:40] released the first just the first six
[20:42] weeks of the 2017 season which in itself
[20:44] is a lot of data as i'll show
[20:46] and
[20:48] nfl tracking data is due from the fact
[20:50] that now
[20:51] all nfl players and the ball have chips
[20:54] and players of chips and their shoulder
[20:55] pads and chips on the ball where now you
[20:58] we get the xy position of the players in
[21:01] the football at every tenth of a second
[21:05] and so there's been awesome work then
[21:07] coming from this just
[21:09] first six weeks of the 2017 season of
[21:12] the first big data bowl of
[21:14] uh danny chu and others at sam fraser
[21:17] university working on characterizing
[21:19] routes and work they've continued with
[21:21] the nfl uh samirda spawn and katherine
[21:24] evans on their expected hypothetical
[21:26] completion probability model and a lot
[21:28] of other great work
[21:30] based on using this sample of data and
[21:33] so i encourage you to really check out
[21:35] if you haven't uh and i think
[21:38] i don't know if they made the
[21:39] announcement earlier i can't remember
[21:40] but the you know another big data bull
[21:43] coming as well it's an awesome
[21:44] opportunity to get your hands on just
[21:46] try to think about how to work with
[21:49] very complicated data that fundamentally
[21:52] a lot of teams are interested in see if
[21:54] you can highlight your skills with it
[21:57] so what does this data actually look
[21:59] like
[22:00] as i said you know it's a lot of
[22:01] information here
[22:03] and so what we have then for
[22:05] literally every player and the ball
[22:08] are the coordinates these x y locations
[22:11] for them you know there's speed and we
[22:13] get a
[22:14] a a notation of the direction they're
[22:17] facing
[22:18] and then we get these annotations of
[22:21] different events that are taking place
[22:22] throughout the play such as when a
[22:24] handoff occurred when there was first
[22:26] contact when they reached the end zone
[22:28] when the ball arrived on a pass etc
[22:32] so for for the sake of this talk i'm
[22:34] going to use this example play of a
[22:36] touchdown run it was a 47 yard jet sweep
[22:40] by wide receiver cordell patterson
[22:42] in the game in 2017 i think he was on
[22:44] the raiders at the time
[22:46] but
[22:47] so this will be the example play i used
[22:49] throughout so what we could see is we
[22:51] could take then
[22:52] there's this giant table basically every
[22:55] single row is a player at the tenth of a
[22:58] second
[22:59] and so we have all of this information
[23:01] for literally every player at every
[23:04] tenth of a second throughout the play
[23:06] and the football and we could take this
[23:08] table and then we can convert it to
[23:10] actually show us what takes place
[23:12] throughout the play all right the
[23:14] tracking data
[23:15] is the embodiment it's the it's the data
[23:17] representation of
[23:19] the movements throughout a single play
[23:21] so in this animation
[23:23] taking that previous table and the blue
[23:26] dots show the team on offense
[23:28] the i think the raiders in this case and
[23:30] then the
[23:32] orange dots are the defense and cordell
[23:34] patterson he is this black dot once he
[23:37] becomes the ball carrier so we can see
[23:39] his movements throughout the play as he
[23:41] ultimately ends up reaching this
[23:43] touchdown
[23:54] so throughout this play things we might
[23:57] be interested in is thinking about
[24:00] you know what is the expectation
[24:03] of the outcome of this play or how
[24:06] things are going to go at various points
[24:08] so
[24:09] the second big data bull focus on this
[24:11] notion of can you predict
[24:14] where the the run is going to end up at
[24:17] given information at handoff so right
[24:19] given this information of where lineman
[24:21] have moved where
[24:23] defenders are coming from and where how
[24:25] the running back is initially starting
[24:27] where do we think
[24:29] he's going to reach by the end of the
[24:31] play and then there could be you know
[24:33] points of time of oh at first contact
[24:35] you know given the first contact taking
[24:37] place where do we think that that
[24:39] running back or the pearson with the
[24:41] ball is going to end up reaching
[24:44] and then finally it leads to the
[24:46] touchdown so okay like is there going to
[24:47] be at some point during this play where
[24:50] we can have a sense of
[24:53] where the person is you know going to
[24:55] reach the end zone so throughout these
[24:57] moments we really want to get a sense of
[25:00] you know what is the expected points
[25:03] at handoff what's the what's the win
[25:05] probability now
[25:06] at handoff you know how it changed from
[25:09] snap to handoff because then i work on
[25:12] sort of dividing the credit between
[25:14] players given just a change in the
[25:17] single moment of a play based on the
[25:20] running back of the ball carries the
[25:21] decision to cut a certain way
[25:25] or just because of how fast they're
[25:28] moving relative to what the average ball
[25:31] carrier moves at that certain moment or
[25:33] just due to
[25:34] this lineman successfully blocking this
[25:37] person at this particular spot
[25:41] so we want to think then a whole process
[25:44] that goes from snap to then everything
[25:48] in between to
[25:50] the end point of how can then we have
[25:52] like a stock ticker value of expected
[25:56] points or win probability
[25:58] betw within a single play itself
[26:02] and so after
[26:04] months of thinking about this
[26:07] a group of phd students and myself along
[26:10] with sam ventura and costas polychronous
[26:13] arrived at this whole framework we put
[26:15] together of
[26:17] how we would go about trying to model
[26:20] every basic play in football to arrive
[26:24] at a continuous
[26:26] value
[26:27] to somehow say we can
[26:30] estimate what is the expected points at
[26:33] a single particular moment within a play
[26:35] at a single tenth of a second
[26:37] what is the win probability for a play
[26:39] and so we could see these decisions that
[26:41] could take place right in terms of
[26:44] at the start of a play a qb could drop
[26:46] back or there could be a handoff and
[26:48] then if it's a qb drop back you know
[26:50] what's the decision
[26:51] is qb throw the ball away that ends the
[26:54] play it's incomplete do they pass who do
[26:57] they then pass to this we have to
[26:59] predict who's the receiving target and
[27:01] then you know do we know when they throw
[27:04] the football if it is caught
[27:06] if so if it's caught
[27:08] we then
[27:10] model then okay who is actually going to
[27:12] catch it and then once we know who
[27:14] catches it they have the football and
[27:16] then they run some somewhere you know if
[27:18] the qb doesn't decide to throw the
[27:19] football do they scramble or sack if
[27:21] they do we again go to the fact of we
[27:23] need to predict where are they going to
[27:26] run to so everything in the end
[27:29] what we realize is it all fundamentally
[27:31] comes back to
[27:33] this ball carrier model
[27:35] so there's going to be all these sort of
[27:37] complicated steps that will each of them
[27:39] themselves take a lot of work and a lot
[27:42] of time
[27:43] but the final piece will be once a
[27:46] person
[27:47] has the football and their goal
[27:50] they're not going to attempt to throw or
[27:51] anything their goal will be just somehow
[27:55] reach the end zone or maybe um
[27:58] win the game by sliding or whatnot or
[28:01] and clock you know their their goal
[28:02] though is to still move with the
[28:04] football all right so what we're going
[28:06] to do is focus then on once they have
[28:09] the football predict what is the yard
[28:12] line that they will reach at the end of
[28:14] the play
[28:15] because knowing the yard line that they
[28:17] reach at the end of the play that dick
[28:19] takes the context
[28:21] that decides then what is the update for
[28:24] the expected points or wooden
[28:25] probability if i say at hand off i now
[28:28] expect this running back to
[28:31] somehow get 12 yards and that converts
[28:33] the first down
[28:34] that means at handoff i can somehow
[28:37] predict this amazing opportunity this
[28:39] amazing change in value
[28:42] all right so what we want to see is
[28:44] within the play itself
[28:46] let's try to predict where that ball
[28:48] carrier is going to end up because that
[28:49] ultimately decides
[28:51] the
[28:52] within play value in terms of expected
[28:55] points and in terms of wind probability
[28:57] where they end up on that field at that
[28:59] at those charts those plots at the
[29:01] beginning of the yards from opponent
[29:03] opponent's end zone this fundamentally
[29:06] decides that
[29:08] so in terms of then actually doing this
[29:11] of working with this data and tracking
[29:13] data you know
[29:15] tracking data are very difficult to work
[29:18] with all right it's not just plug and
[29:20] play
[29:21] you have to somehow decon take these all
[29:24] of these little observations of every
[29:26] tenth of a second
[29:28] and put them together in a clever way of
[29:31] representing useful information
[29:34] about
[29:35] the the outcome of interest that you're
[29:38] trying to model so in the case of
[29:40] thinking about modeling where the ball
[29:42] carrier is going to go we want to look
[29:44] at
[29:45] the ball carrier here in black and think
[29:47] of you know how far away are they from
[29:50] the nearest defender and other defenders
[29:52] what is the angle between these players
[29:55] is this defender even running in the
[29:57] correct direction or are they being
[29:59] blocked by somebody
[30:01] or how far away are they actually from
[30:04] you know the target end zone of interest
[30:05] what direction is the ball carrier
[30:08] facing themselves
[30:10] you have to take that xy data and you
[30:13] have to
[30:14] work on you know actually constructing
[30:16] it and doing proper engineering
[30:19] to then have something useful all right
[30:21] so this is the this is the most
[30:22] important step arguably of any work with
[30:25] things like tracking data is how do you
[30:27] actually make use of it in
[30:30] an efficient way as well
[30:32] things that are related to
[30:33] what you're interested in
[30:35] it's something that is very popular
[30:38] approach for working with tracking data
[30:40] or known as voronoi tessellations and so
[30:42] there's been a lot of work by the group
[30:44] of the luke born lab
[30:47] of taking
[30:49] where you have positions of where
[30:51] players are out on the field and using
[30:52] things like what direction they're
[30:54] facing the speed etc
[30:57] and breaking up the field into these
[30:59] partitions these tiles to give you a
[31:01] sense of ownership
[31:03] and so what we can try to what we do is
[31:06] we take information like on the previous
[31:08] slide of like the direction and distance
[31:10] between defenders and then we also
[31:12] summarize for instance here like the
[31:14] size of the area that the ball carrier
[31:17] owns in terms of this of this
[31:19] tessellation and things like uh does it
[31:22] contain the end zone how much does their
[31:25] team own
[31:27] you know does it is it connected to what
[31:29] they own
[31:30] we try to summarize variables that have
[31:32] all of that information as well
[31:34] so
[31:35] the
[31:36] most of the work of working with
[31:38] tracking data is a lot of engineering
[31:41] and data wrangling and mongoling to just
[31:44] understand try to extract something
[31:46] useful out of it to then arrive at the
[31:48] process that you're going to use it for
[31:50] your model of interest
[31:54] so all of this information then
[31:57] it leads to a very complex situation you
[32:00] you fundamentally have a lot of
[32:02] variables so a high dimensional problem
[32:04] we're going to have things that we don't
[32:06] think are just a simple linear model all
[32:08] right so
[32:09] what i mean here is i'm dealing with
[32:10] something very complex it has a lot of
[32:12] these interactions between variables
[32:14] potentially the distance between this
[32:16] defender
[32:17] and the angle they're facing could have
[32:19] some interaction
[32:21] the speed they're running at
[32:23] with the defenders some also
[32:25] in this context there could be a time
[32:28] component of maybe knowing information
[32:31] about
[32:32] previous
[32:34] parts of the of the play can provide
[32:37] some predictive power at knowing what
[32:40] the outcome could be not even just at
[32:42] the current moment
[32:44] so how can we use that structure as well
[32:48] in terms of modeling and you know what
[32:49] we end up doing is we view some very
[32:51] complex models and the one thing i want
[32:54] to because you know this is the thing
[32:56] also for undergrads high school students
[32:58] is any time you're trying to think of
[33:01] building a model or trying to evaluate
[33:04] something you're creating you need a
[33:05] baseline
[33:07] i can't just say i'm going to take all
[33:09] these things i summarize i'm going to
[33:11] throw it into a fancy deep learning
[33:13] model and wow look at that it's amazing
[33:15] no i need to evaluate it against
[33:17] something so what we do
[33:20] is we take different steps different
[33:22] sorts of models at different levels of
[33:25] complexity
[33:26] and this intercept only this is a model
[33:29] that it ignores all variables it's
[33:31] looking at the variance in the data it's
[33:33] just saying if i just use the average
[33:37] if i just use
[33:38] the average yards gained and as i
[33:42] predict that for every single moment
[33:46] what will be my
[33:48] uh
[33:48] you know
[33:50] i use no information at all so if i just
[33:52] use the average without any information
[33:55] about how i summarize the player
[33:56] tracking data how well do i do
[33:59] and then these different models vary in
[34:01] terms of their complexity i'm not going
[34:03] to go into the details of this but the
[34:06] last two here on the right these are
[34:08] these deep learning models where the one
[34:10] on the right-hand side this lstm model
[34:13] long short-term memory this is the only
[34:15] one of the ones we considered that
[34:18] accounts for this time component as well
[34:21] to include
[34:23] previous steps of information
[34:25] within the play itself
[34:28] so in terms of then
[34:30] doing these different considerations and
[34:32] models what you want to actually do then
[34:33] is have some way of validating and
[34:36] comparing these approaches
[34:38] that with the emphasis of how well does
[34:40] a model i build generalize out of sample
[34:44] so for if you're thinking of what a
[34:46] football team would want in this case
[34:48] they're going to have a model that then
[34:51] they're going to want to apply to new
[34:53] football games
[34:54] all right so if i have a model that
[34:56] cannot generalize to data it has not
[34:59] seen before then it's not useful so what
[35:02] we do is we evaluate our models based on
[35:05] a
[35:06] leave one week out cross validation
[35:08] approach so all this means
[35:10] is i'm going to build a model for
[35:12] instance on the first five weeks of the
[35:14] season and then testile does on the hold
[35:17] out sixth week and then i just rotate
[35:20] between the weeks hold out the first
[35:22] test on two through six hold out the
[35:25] second test on one then three through
[35:27] six and so on the um and what we then
[35:29] measure is like this overall error rate
[35:33] and then we also look at to see how does
[35:36] how do these models compare over the
[35:38] course of these ball carrier sequences
[35:41] as well
[35:43] and so what we can see
[35:48] this
[35:49] what we end up arriving at was the model
[35:52] this lstm model this model that has all
[35:54] of these sort
[35:56] of levels of complexity of dealing with
[36:02] high dimensional interactions
[36:04] we're looking at then
[36:06] in this in this time component as well
[36:09] it has the best overall performance
[36:11] compared to the other models
[36:13] and so the way they interpret these
[36:14] plots the y-axis is this air this rmse
[36:18] value or how we're measuring the air
[36:21] between the different models and the
[36:23] x-axis is the frames so the the tenth of
[36:27] a second from the start of a run
[36:30] essentially so what shouldn't be
[36:32] surprising is at the very beginning of a
[36:35] run on the far left hand side
[36:38] you're going to see a lot of a lot of
[36:41] error or that the error is going to be
[36:43] higher for all of these models right
[36:45] that's in a way that's where you view
[36:46] the least amount of information at the
[36:48] moment at
[36:50] at the moment of handoff
[36:51] and then once we just get you know
[36:55] 10 20 frames in that's where then the
[36:58] model starts to learn
[37:00] and that's where it starts to really
[37:01] improve
[37:03] relative to say in the top left here
[37:05] this intercept only approach and so you
[37:07] can see the intercept only approach
[37:09] really does bad throughout time all
[37:11] right but the thing that's interesting
[37:13] is while
[37:15] these different other models lasso xg
[37:18] boost other
[37:19] feed forward neural network
[37:21] start to do start to do better than the
[37:23] intercept only approach later on they
[37:25] actually do worse than it all right
[37:28] the lstm model by having this time
[37:30] component was the only one we saw that
[37:32] then actually achieved a lower error
[37:35] than this intercept only and did it
[37:37] throughout the course of the run just to
[37:39] make this clear the the how dark these
[37:43] bars are throughout the course of the
[37:44] run just indicate how many of the
[37:47] observations you have so every single
[37:49] bar ball carrier sequence
[37:51] is these very very dark on the left-hand
[37:53] side because they have all all of them
[37:55] have the start of a run but
[37:57] there's fewer fewer runs that have you
[38:00] know over 10 seconds in length or
[38:02] whatnot
[38:04] so this is another justification then
[38:07] this is a justification that we see as a
[38:09] usefulness of using this lstm model and
[38:12] as another way of looking at it instead
[38:14] of just this
[38:15] rmse error rate
[38:17] we also evaluate based on
[38:21] if we just look at the actual value of
[38:23] the air to see if there's any sort of
[38:25] bias
[38:27] in under or over predicting over the
[38:30] course of a run so what shouldn't be
[38:32] surprising
[38:33] is the intercept only approach this
[38:36] baseline approach is a biased approach
[38:40] we're then predicting over the course of
[38:43] the run at various points a long time
[38:46] it's biased because we can see it's
[38:47] systematically different from zero what
[38:49] we want is a model that's essentially
[38:51] hovering the zero line the whole way
[38:53] through
[38:54] and what we can see is this lstm model
[38:57] does the best at that versus these
[38:59] others we considered so
[39:02] in this case this most complex model
[39:04] that we use
[39:06] we was actually the better approach and
[39:09] this was the reason why we wrote this
[39:11] paper called going deep because we use a
[39:13] deep learning model
[39:16] because that is a fad
[39:20] all right so once we have this now
[39:23] using the same animation from before but
[39:26] now i'm plugging in this red line to
[39:28] give you a prediction this is this lstms
[39:31] model
[39:32] predicting once cordell patterson has a
[39:35] ball where are they expected to land or
[39:38] where are they expected to reach at the
[39:40] end of the play so we can see how it
[39:42] changes over time for these different
[39:44] moments
[39:45] to then see where they're expected to
[39:47] reach a touchdown
[39:49] and
[39:50] expect it at different points and so one
[39:52] of the things is we can have a clear
[39:54] sense of like the impact different
[39:56] variables have on these models
[39:58] and
[39:59] so for instance on this left-hand side
[40:01] here this time into play along the
[40:03] x-axis the y-axis is this distance to
[40:06] the closest defender and which shouldn't
[40:08] be surprising to you is
[40:10] the first contact the distance is
[40:13] greater than zero tracking data
[40:16] none of this is perfect right so don't
[40:18] expect if you ever get your hands on to
[40:20] work on this the big data bull that is
[40:21] going to be very clean but it's still
[40:24] something that gets close to zero
[40:26] so you can see how that changes over
[40:28] time with respect to this red line kind
[40:30] of going back and forth but one thing
[40:32] that's interesting here in this example
[40:34] run is the fact that
[40:36] he maintains a high level of speed
[40:38] throughout the course of the entire run
[40:41] itself
[40:44] as i said though you know what we want
[40:46] to do
[40:47] is take
[40:48] the
[40:50] expected yard line and we're not just
[40:52] interested in that we want to get value
[40:54] added so we want to convert that to
[40:56] expected points of wind probability and
[40:58] as i said before
[41:00] this is actually simple we already have
[41:02] these expected points of wind
[41:03] probability models
[41:05] and we could just take this prediction
[41:07] and update
[41:08] you know
[41:09] if i expect them to reach this yard line
[41:12] what's my new predicted expected points
[41:14] of wind probability
[41:16] and so what we could do then is plug in
[41:18] those estimates to then ch view this
[41:21] stock ticker value to see what is the
[41:24] expect points how does it change over
[41:26] time over the course of this run and
[41:30] also as well the wind probability once
[41:32] he starts moving further and further
[41:34] down the field and he's expected to
[41:35] reach something like the end zone
[41:37] right he reaches these points where we
[41:38] think he's going to get a touchdown so
[41:41] then his expected points is seven points
[41:52] however
[41:54] note of caution on doing this
[41:58] this is not
[42:00] the expectation
[42:01] of
[42:03] expected points or win probability all
[42:05] right all i've done here is i've taken a
[42:08] single
[42:09] predicted value
[42:11] my expected end of yard line
[42:14] and then i was plugging that into
[42:17] predicting the expected points and wind
[42:20] probability so i'm using the word
[42:21] expected a lot i know this is going to
[42:23] cause confusion to give you a headache
[42:25] but
[42:27] this is a single point estimate value
[42:30] what i really actually want to arrive at
[42:32] to compute the true expectation
[42:36] of expected points or win probability
[42:38] that you learn about in your intro to
[42:42] statistics probability class when you're
[42:44] learning about continuous distributions
[42:47] is i actually need a full distribution
[42:50] for the end of yard line
[42:53] all right i need to actually get a sense
[42:55] of what are all of the possible outcomes
[42:59] that could take place over the course of
[43:01] the run that i think have some
[43:03] likelihood of happening
[43:06] and
[43:07] then alwa should be able to arrive at
[43:08] which would be a much smoother
[43:10] representation of this expected points
[43:12] win probability these are very jagged
[43:14] all right these are don't i don't
[43:16] actually even think this is proper in
[43:18] some cases this avoids any uncertainty i
[43:21] have
[43:22] about where the ball carrier could end
[43:24] up on a play
[43:26] so what we then do is we don't just use
[43:28] a single point estimate value we then
[43:31] actually model the full distribution of
[43:34] potential outcomes essentially
[43:37] that
[43:37] the ball carrier will arrive at on the
[43:39] field in terms of their end of yard line
[43:41] so once the ball carrier gets the ball
[43:43] now i have all of these red lines
[43:45] indicating various points of likelihood
[43:49] that they will arrive at and we do this
[43:51] with the technique known as conditional
[43:53] density estimation where this was
[43:55] essentially building off a thesis work
[43:56] by taylor plus pistol
[43:58] of using random forest so another high
[44:01] dimensional complex model to then model
[44:04] in a non-parametric way the full
[44:06] distribution of the end-of-yard line so
[44:10] what you see is some like bimodal
[44:13] structure
[44:14] where the darker the red is the more
[44:16] likely we think
[44:18] the higher the density that the player
[44:20] will have of
[44:21] going at that location
[44:23] so what we could see over the course of
[44:25] this run for instance is once they get
[44:27] the ball we start to have some more
[44:30] likelihood here and then it really
[44:31] darkens at that one particular point but
[44:34] then the target end zone as you can see
[44:36] gets this thicker thicker red line over
[44:39] the time because this ends up becoming
[44:41] the more likely destination of the run
[44:44] there's still some non-zero
[44:46] chance that he gets tackled to close to
[44:49] where he's at
[44:51] but
[44:52] we start to see this opportunity over
[44:54] time
[44:56] and so this is just the still shot then
[44:58] of showing
[45:00] this bimodal
[45:03] structure of once do they have the ball
[45:05] how likely do we think just where
[45:07] they're at
[45:09] at the particular moment versus there's
[45:11] some non-zero
[45:13] chance that we think that the ball
[45:15] carrier could arrive actually at the end
[45:17] zone so once we actually use then this
[45:19] full distribution now on the right hand
[45:21] side now we actually have the proper
[45:25] expected expected points expected when
[45:28] probability and you can see this is
[45:30] something that's much smoother it
[45:31] doesn't have the
[45:33] jagged spikes as before
[45:36] so
[45:37] how do we then actually use this whole
[45:40] process i just went through i talked
[45:42] about the beginning of
[45:43] i want to arrive at better player
[45:45] evaluation
[45:46] and
[45:47] you know this is actually fundamentally
[45:49] how we're going to do that the first
[45:52] step was to how do we properly value
[45:54] within a play and you know there's still
[45:56] a lot of work to be done in this then
[45:58] fill out all these rest of these steps i
[46:00] will say though that uh matthew ryers
[46:02] simon fraser he's done recent work and
[46:05] you can check out his twitter account of
[46:07] basically implementing the quarterback
[46:09] decision models that we we described and
[46:12] he goes into a full better quarterback
[46:14] decision rating that's fantastic work um
[46:18] and one of the things i hope for
[46:20] undergrads and high school students
[46:21] watching this
[46:22] is
[46:23] i'm only here because i tweet a lot and
[46:27] share my work online and we tried to
[46:29] promote this nfl scraper
[46:31] and it successfully worked i think so i
[46:34] want you to be wary of do not be afraid
[46:36] to share what you're working on get
[46:39] feedback and don't be afraid to promote
[46:41] it it could feel awkward at times but
[46:44] people aren't going to see what you're
[46:46] doing unless you get it out there
[46:48] and once you get it out there then
[46:50] they'll reach out to you be interested
[46:52] you'll get feedback good criticism
[46:55] and improvements so
[46:57] examples of this on the left here this
[46:59] was a project by sarah molipali where
[47:01] she scraped nfl next gen
[47:04] stat images for
[47:06] past locations she converted images to
[47:09] data sets
[47:10] and she has the quickest way of ever
[47:12] getting hired by an nfl team she now
[47:14] works for the baltimore ravens and then
[47:16] on the right uh rashav duda former
[47:18] undergrad at cmu a software engineer he
[47:21] um he's recently done some awesome work
[47:23] of imputing the third dimension to the
[47:25] tracking data using simple physics that
[47:28] i don't understand but he figured it out
[47:30] so you know
[47:32] what given that you know i want you to
[47:34] know that this next you know that
[47:36] working with tracking data is very
[47:38] complicated it's a lot of hard work but
[47:40] these are going to be the steps and this
[47:42] is what people are interested in if
[47:43] you're thinking about careers in sports
[47:45] as well to
[47:46] ultimately better ride that especially
[47:48] within football better true estimates of
[47:51] player value added
[47:53] so just want to conclude by thanking the
[47:55] others that worked on this and my uh our
[47:57] team for going deep and then make sure
[48:00] as brian mentioned the beginning you
[48:02] check out we have our own conference
[48:05] coming up in a couple weeks the carnegie
[48:06] mellon sports analytics conference so
[48:08] that is completely open for registration
[48:10] now and uh
[48:12] with that thank you and any questions
[48:18] all right uh
[48:19] thank you ron um please join me in
[48:21] thanking the speaker
[48:24] you can probably only hear one clap
[48:25] right now but i think uh probably uh
[48:28] there's several dozen people clapping
[48:30] and by themselves at home
[48:32] um so uh let's start off with the
[48:35] questions here so this question is from
[48:38] will riley
[48:40] how is expected points adjusted
[48:42] depending on what player what the player
[48:45] is being asked to do so if a player is
[48:47] put in a position where he set up for
[48:48] failure uh for example a yards after
[48:52] completion reliant receiver asks to get
[48:56] open on his own
[48:57] does the data still suggest it was his
[49:00] fault how can teams take into
[49:03] hold on this is a couple part question
[49:06] here into account the reason for changes
[49:08] of epa in a play
[49:10] yeah so that's a great question
[49:13] and that gets at the point of why you
[49:15] want to do this continuous time value
[49:18] right in terms of
[49:20] if we consistently hear the statement of
[49:23] you know running backs don't matter all
[49:24] right because it's just not as rushing
[49:26] plays are not as efficient as passing
[49:29] plays so
[49:30] given that we can know that and given
[49:32] that a running back gets the football
[49:35] out on a handoff and the expected points
[49:38] at the moment of handoff
[49:40] may not be that great because we know
[49:42] it's a running play
[49:44] but in the end that running back
[49:46] themselves still ends up adding more
[49:49] over the course of the run
[49:52] relative to what others do and by using
[49:55] the information the tracking data
[49:56] assessing you know
[49:58] the lineman locations and whatnot to get
[50:00] a better sense of the expected
[50:03] points at those particular moments
[50:06] then we're actually able to say
[50:09] yes okay the running play itself wasn't
[50:12] that valuable but maybe christian
[50:14] mccaffery is better still at those
[50:16] situations than other particular running
[50:20] backs right so it's still a way of
[50:22] trying to figure out how better players
[50:25] are relative to others in the same
[50:28] scenarios right so you know it's
[50:30] absolutely a thing where if you just
[50:32] look at this raw expected points added
[50:35] when probability added you're not going
[50:37] to get a sense of uh
[50:39] you know
[50:41] you're just gonna if you're just gonna
[50:42] look at it and see well they're still
[50:43] net negative well okay the expectation
[50:47] is that negative now with respect to
[50:49] that do they improve versus how others
[50:52] do as well
[50:57] all right uh question from marcos hi ron
[51:00] do you think hybrid models uh machine
[51:03] learning plus stochastic are competitive
[51:06] to deep learning approaches for example
[51:10] x boost uh might mean xg boost plus
[51:13] temporal random effects
[51:16] yeah no that's a great question if um
[51:18] honestly it's something we didn't look
[51:20] into is enough and i'll say i mean i
[51:22] wouldn't be surprised if with
[51:25] a lot more clever uh engineering and
[51:28] variables if the lasso could have been
[51:31] almost as just as good as the as the
[51:34] deep learning model i mean it's
[51:35] complicated in the sense of tracking
[51:37] data there's a lot of information
[51:40] and maybe
[51:41] you know we ended up while we did a lot
[51:43] of hard work on constructing variables
[51:46] you know it was still in a way a little
[51:48] lazy versus what we've seen um say in
[51:51] the big data bull entries i know the the
[51:54] winner of the second big data ball i'm
[51:55] pretty sure they use a convolutional
[51:58] neural net which is just thinking of
[52:00] let's view the tracking data as like an
[52:02] image
[52:03] trying to like break up locations that
[52:05] way uh so
[52:08] i mean potentially you know thinking of
[52:10] actually boosting random forest
[52:13] would
[52:14] go maybe with a little bit more
[52:15] engineering could probably do just as
[52:17] well if not better it wouldn't be
[52:19] surprising to me i personally i'll say
[52:21] this about something like deep learning
[52:23] is
[52:25] when you
[52:26] work on it when you view the results of
[52:28] it it's like wow this is amazing it's a
[52:31] lot of
[52:32] hard work and construction
[52:34] to get it to be something that is
[52:36] working in tune properly
[52:39] something like xgboost and random forest
[52:41] are very easy off the shelf
[52:43] deep learning is not neural nets are not
[52:46] easy off the shelf
[52:47] um so that's just something else to keep
[52:49] in mind
[52:51] all right a question from devin hey ron
[52:54] do you think
[52:55] there's value in predicting not just
[52:57] expected tackle position in the x
[53:00] dimension
[53:01] but also in the y dimension
[53:04] yeah that's a great question so in our
[53:07] case
[53:08] we
[53:08] [Music]
[53:10] because we were using
[53:12] our expected points of wind probability
[53:14] models that ignore the y dimension we
[53:17] just focus on the x dimension but there
[53:20] could be something to be said i mean i i
[53:22] don't know if
[53:24] there is any fundamental difference
[53:27] in
[53:28] an expected points or win probability
[53:30] model even just between play given x y
[53:34] right given side information on the
[53:37] field
[53:38] maybe there could be something there
[53:40] and in that case then it would be more
[53:42] valuable to
[53:44] do the x y prediction but for our case
[53:46] just because all we needed was
[53:49] the single dimension to update expected
[53:51] points of wind probability we viewed it
[53:53] as something that
[53:54] makes the problem simpler right because
[53:56] if you just try to predict the joint
[53:59] distribution the joint outcome of the x
[54:01] y that's gonna just add more complexity
[54:05] to which it's already a really
[54:07] complicated problem and uh you'll just
[54:10] deal with more struggling models in some
[54:12] case um but it is a good question there
[54:14] could be
[54:15] some differences useful information
[54:18] there i don't know
[54:21] okay i've got about four minutes left so
[54:25] we'll go with a few more questions here
[54:27] um so this is from john if we had uh
[54:30] consistent data for for who was on the
[54:32] field for each play could that lead to
[54:34] optimal line formation and when i
[54:36] replace a player does it make a cohort
[54:38] better or uh bring a cohort down so for
[54:42] example when x and y are playing and y
[54:44] gets hurt does z make x better or not
[54:47] are we able to get this player uh this
[54:50] player time on field data from
[54:53] from the ngs data
[54:56] so
[54:57] working with the nextgen stats tracking
[55:00] data right the player tracking data
[55:02] you absolutely know
[55:05] every player that was on the field at
[55:06] every particular moment so
[55:09] i mean the the
[55:11] logical next steps in some way
[55:13] is trying to break up the stock ticker
[55:16] value
[55:17] in like a plus minus approach
[55:20] of you see with like uh hockey work
[55:24] brian has done nba regression plus minus
[55:27] approaches where you're trying to like
[55:28] credit uh players at these particular
[55:31] frames
[55:32] maybe based on even like the direction
[55:34] they take or whatnot because like the
[55:36] play-by-play data that's publicly
[55:38] available we don't have who's on the
[55:40] field at every particular moment so like
[55:43] the goal would be to try to answer your
[55:46] questions right of like if we plugged in
[55:49] uh
[55:50] you know juju smith schuster on this uh
[55:54] passing play
[55:55] does he do better than someone else like
[55:59] i mean fundamentally we're all
[56:01] interested in like that causal question
[56:03] i don't think we're anywhere i don't
[56:05] think we're quite there yet
[56:06] but
[56:08] it's still like that's the goal of what
[56:11] we're hoping to do by doing these better
[56:13] valuations um so i don't know if i
[56:16] answered that question at all i feel
[56:17] like it kind of rambled but that would
[56:19] be that's like the goal of what we're
[56:21] hoping to do
[56:24] okay the question from steven how can
[56:26] expected points and win probability
[56:27] models
[56:29] be used in real time during the course
[56:31] of the game
[56:35] yeah that's that's an interesting
[56:36] question i remember i was actually asked
[56:38] this question
[56:40] by the crew of warden moneyball
[56:43] and i think i botched the answer then i
[56:46] will say now i don't think the goal
[56:49] is to really think of like
[56:51] during the game
[56:53] um
[56:54] can it lead to a better decision like i
[56:56] don't think that's a practical thing i
[56:58] think this is really about we can now
[57:00] try to objectively measure the
[57:03] contributions of everybody throughout
[57:05] the play and do it after the game all
[57:08] right you know of actually looking back
[57:10] and what were the values added
[57:12] within these plays by these players
[57:15] because you know right now we don't have
[57:17] all of these metrics about linemen about
[57:19] cornerbacks about all these things that
[57:21] we can't
[57:22] measure there's no counting stats for
[57:24] these different players that then the
[57:26] tracking data can lead to i don't know
[57:28] about within the game itself
[57:31] i i'd have to think about it more like
[57:32] what's practical i view this as
[57:35] how we actually can now go back and
[57:38] properly create new statistics
[57:41] to
[57:41] uh summarize player performance
[57:45] great thanks and uh with that it is
[57:48] 10.50 so we're going to have to
[57:52] move on to the next part of the
[57:53] conference and uh thanks again to ron uh
[57:58] for presenting uh his work on this uh
[58:01] continuous time
[58:03] uh
[58:04] evaluation of game outcomes and if you
[58:06] had a question there's a bunch of
[58:08] questions here that we didn't get to if
[58:09] you had a question please feel free to
[58:11] reach out to ron
[58:13] um after the conference or during a
[58:15] break or something like that and i'm
[58:16] sure
[58:17] happy to answer your questions if you uh
[58:20] if you ask on twitter also he'll uh
[58:22] he'll respond to you on twitter
[58:24] so with that i think we'll turn it over
[58:28] to uh to june now