Video Title: TensorFlow with Applications in Sports Analytics [2021 UConn Sports Analytics Symposium]
Video ID: jMJQwAKTgRI
URL: https://www.youtube.com/watch?v=jMJQwAKTgRI
View Count: 85

[00:00] nice to meet you all here today for the
[00:02] workshop of the introduction to the
[00:04] tensorflow with the application in the
[00:07] sport analytics
[00:09] and hope you can learn something useful
[00:11] this afternoon
[00:13] the content for today will be divided
[00:15] into
[00:16] mainly two parts and we will firstly
[00:19] introduce the basic concepts and tools
[00:22] and then use a simulated example and one
[00:26] real data example to
[00:28] facilitate your again of the knowledge
[00:31] and during the presentation
[00:34] and you you are welcome to unmute
[00:36] yourself and if you have any questions
[00:40] let me make sure that uh
[00:43] all our
[00:44] all of the attendees are
[00:46] in the panel list
[00:54] yeah
[00:55] let's begin our content today
[00:57] for the first several pages
[01:00] i think i i could skip them because it's
[01:03] just
[01:04] describe how popular the
[01:06] tensorflow is and how dominant it is
[01:09] comparing with another
[01:11] deep learning framework
[01:13] and
[01:14] the the next two pages will show what is
[01:17] the tensor and why the tensor learning
[01:20] framework is important in a nowaday
[01:22] analytics
[01:24] the tensor is just the data with
[01:26] dimension
[01:27] uh zero dimension tensor adjusts the
[01:30] scalar as we know the one dimensions
[01:33] tensor is the vectors and two dimension
[01:36] tensor is a matrix and three dimensions
[01:38] you can imagine that that will be a cube
[01:42] and why tensor is so especially in the
[01:45] nowaday analytics because the the vector
[01:48] matrix operations and the gradients
[01:52] just the derivatives are are dominant in
[01:54] machine learning and deep learning
[01:56] framework uh for example uh the
[01:58] following algorithm you may very
[02:01] very very familiar with is is the
[02:02] greatest gradient descent we it just an
[02:05] update raw and the the beta had the the
[02:09] is related with the uh older version all
[02:12] the beta hat and new beta hat and the
[02:15] beta had just the vector and also the
[02:17] gradient is also a vector so uh it just
[02:21] the
[02:22] operation among the tensor
[02:24] and also the the gpu structure
[02:27] uh
[02:28] algorithms can lead to a powerful
[02:30] ability to solve the linear tensor
[02:32] operation so that is the reason why
[02:34] tensor is so important here
[02:38] and the next page shows how to install
[02:40] the tensorflow in python and i think i
[02:43] have already posted two
[02:45] uh
[02:46] videos that
[02:48] that can guide you about the the
[02:49] installation of the tensorflow
[02:51] there are several methods for anaconda
[02:54] or just a through the pipe of python
[02:57] and you can verify whether your
[02:59] installation is
[03:00] successful
[03:02] based on this three line of code
[03:05] so now i i assume that all of you has
[03:07] already
[03:09] installed this uh prerequisite
[03:11] and so let's uh
[03:13] begin the
[03:15] most important part of today yeah i
[03:18] think this is this part is the most
[03:20] important the the concept
[03:22] and
[03:24] there are totally three concepts the
[03:26] operations
[03:27] the graph
[03:29] and the the session
[03:31] let's
[03:32] the operation is just a very simple just
[03:35] like a
[03:37] matrix multiplications matrix add and
[03:40] others and the graph is uh we build
[03:43] graphs where which represents represent
[03:46] the data flow of the computation and
[03:49] with operation and graph already then we
[03:52] use session to run and to
[03:55] to obtain the result
[03:57] so
[03:58] i i know i know just that talking about
[04:00] this is not clear enough so
[04:03] uh i don't know whether you have uh
[04:05] ever played this game uh in your
[04:08] childhood but it's my favorite games
[04:10] and uh
[04:13] in this game the mission is to deliver
[04:15] the waters from the faucet to the target
[04:19] and you can switch
[04:20] every pieces of the
[04:23] of the tube
[04:24] and
[04:26] we use the appropriate uh operation to
[04:29] connect them and and and if you if the
[04:32] tube is finally completed and then you
[04:34] can open the faucet and let the waters
[04:38] to
[04:38] to get to the target
[04:40] and just actually to to write a
[04:42] tensorflow
[04:44] uh
[04:45] script it's just a very similar with
[04:47] this uh small games
[04:49] and each actually each pieces of of the
[04:52] tube is just the the nodes in in the
[04:55] tensorflow we can make it constant make
[04:58] it a a constant a variable
[05:02] a placeholder and another kind of thing
[05:05] and we use the appropriate operations
[05:07] like add matrix mattress multiplication
[05:10] to connect these
[05:12] nodes
[05:13] and then to form a graph this blue
[05:16] circle a graph and with all of this
[05:18] settled down then you can use the
[05:22] tensorflow.session to open the faucet
[05:24] and then the water actually the data
[05:26] will flow in and then
[05:29] through
[05:30] through your established line to to the
[05:33] target and
[05:35] obtain the result so this is basically a
[05:38] taste of uh
[05:40] writing the tensorflow programming
[05:42] yeah
[05:44] and in fact
[05:46] the order we described above is the
[05:48] programming strategy for tensorflow
[05:50] version one tensorflow v1 for short
[05:54] you may have
[05:55] feel that it is very strict to write you
[05:57] should
[05:59] well develop your
[06:01] pipeline and
[06:01] [Music]
[06:03] data flow and then can
[06:05] input all the data in so this is very
[06:08] strict so it does not allow you to open
[06:11] the faucet when the passageway is not
[06:13] completed
[06:15] when it is broken so by contrast the
[06:18] tensorflow v2 version two the faucet is
[06:22] open moment by moment it means that you
[06:24] can put there
[06:26] one node and make a valid connection
[06:28] between a byte operation then the water
[06:30] will flow to the end of the tube and
[06:32] stop there it it will knock it will not
[06:35] leak so we use the word uh eager
[06:39] executions to describe the forces it's
[06:42] open opening moment by moment then you
[06:44] can
[06:45] immediately obtain the data the result
[06:47] step by step
[06:49] so v1 v2 have their own pros and cons v1
[06:54] is good at integrity but but loose at
[06:57] the flexibility well the v2 version 2 is
[07:01] is the opposite
[07:02] so
[07:03] version 1 is frequently used in the
[07:06] final delivery but the version 2 is good
[07:09] at debugging
[07:11] when you mix some error
[07:13] so in today's workshop we are focusing
[07:16] on
[07:17] the tensorflow tensorflow v1 but i have
[07:20] to mention that v1 and v2 are
[07:23] fortunately compatible now in the
[07:25] package tensorflow
[07:26] so the default is version two but if you
[07:30] want to use the uh if you want to use
[07:33] the version once you we can just say
[07:36] input import
[07:38] tensorflow.compad.v1 as tfv1 then you
[07:42] can use all the functions and method
[07:45] under the tensorflow b1 so it is it it
[07:49] it's no matter that we use b1 we do
[07:51] account we can also use we can all use
[07:53] this only package tensorflow
[07:56] but it's the there's a different way to
[07:58] call them
[08:00] and we will involve this in our
[08:02] uh further example
[08:07] so let's next get into the details and
[08:11] firstly we will introduce several
[08:13] possible notes
[08:15] remember that recap that knows is just a
[08:18] play a role like each pieces of the tube
[08:21] in the whole graph in the whole graph
[08:24] and there are several possible
[08:29] notes can be selected let's let me
[08:32] show it uh
[08:34] comparing with our
[08:36] code
[08:37] by the way the all of the resource
[08:40] today's are in the in in my github repo
[08:43] and i think we have the link in our
[08:46] meetings
[08:47] conference website
[08:49] and
[08:50] the the slides are here in the slice.pdf
[08:54] and all the code involved is in the code
[08:57] folders and there is a
[09:01] code there's a
[09:03] ipy notebook
[09:05] file here named jupiter slash code and
[09:09] you can obtain all the code here
[09:12] yeah
[09:14] so after your installation after the
[09:17] installation of tensorflow and you can
[09:19] run this
[09:20] to check the
[09:22] version of the tensorflow is right so by
[09:25] default just as what we
[09:28] said uh previously the default version
[09:30] is version two and you can also import
[09:33] the version one like this
[09:35] import tensorflow.comback v1
[09:38] and then the first
[09:41] possible node for the graph is the
[09:43] constant the constant
[09:45] can be either scalar vector
[09:47] matrix it's just a concept which is
[09:50] offset with the variable variable can be
[09:53] changed in filters but the the constant
[09:56] cannot
[09:57] so uh
[09:59] to create a constant it's easy you just
[10:01] said s equal to the tf dot constant
[10:05] and scalars can be just inputted inside
[10:09] of the
[10:10] parentheses and
[10:13] if you want to create a matrix
[10:15] and you can just
[10:17] use the double brackets
[10:19] and each rows in the single bracket
[10:24] and then it will generate the matrix and
[10:27] also you may generate a matrix constant
[10:29] like this and just in one
[10:32] single
[10:33] bracket and with the shape control
[10:36] and assign is to have two columns and
[10:38] two rows and then the the data will
[10:41] file in
[10:42] in every each rows by by row by row yeah
[10:46] and actually if you have already uh
[10:50] assigned the the constant and then you
[10:52] can
[10:54] try and now try to build a graph
[10:57] just as what we described about you have
[11:00] a graph and you put the nodes into the
[11:02] graph
[11:03] so how to do it we just said g equal to
[11:06] tf dot graph the dot the graph tf.graph
[11:09] will just create a new graph and you
[11:12] said with g dot as default you mean that
[11:16] now we are focusing on this graph
[11:19] and in this graph we add the note on
[11:22] that we add node s
[11:25] add the node m
[11:27] and then connect them with the
[11:30] uh
[11:31] just the multiple multiplications
[11:34] operations just uh then obtain the mmul
[11:39] well this is our name for it and after
[11:42] you assign them then you can call this
[11:45] graph and you will see that that this is
[11:48] a special structure and it will tell you
[11:51] this is a tensorflow graph
[11:53] and then you can run that
[11:56] now we have already completed a graph so
[11:59] uh
[12:00] we have the the the all the big picture
[12:03] here and then we have two nodes on it
[12:07] let me let me say it more clearly so we
[12:10] build a graph
[12:12] and only two nodes on it
[12:15] one called m another code s and we
[12:18] connect them with the multiplications
[12:21] and we have
[12:23] now we we now we need to uh define what
[12:26] kind of thing we want to
[12:28] have for the output so just just define
[12:31] this output
[12:33] and so we use the uh
[12:36] we use the
[12:37] uh
[12:38] a tf we want those sessions to form a
[12:41] sessions and then we we we we we
[12:45] told the computer that we want mmul to
[12:49] be the output
[12:51] even though in this case there is no
[12:53] input there's no such a thing like a
[12:55] faucet but uh we also need an output
[12:59] yeah so we says dot brand can assign
[13:02] tell the computer that we need mmul as
[13:05] the output
[13:07] so
[13:08] let's
[13:09] see the code here
[13:11] this is what we told
[13:14] about
[13:15] and we
[13:16] settled down this nodes
[13:18] and settle down the the graph and we
[13:21] call g it may tell you that this is a
[13:23] tensorflow framework graph
[13:26] and then so we create sessions
[13:29] and we tell that we we we we
[13:32] assign the graph g to be the graph we
[13:35] use
[13:36] and tell the computer that we want mmul
[13:40] to be the output
[13:42] and then run session.run
[13:44] and you can see that this is the output
[13:47] right because we define the matrix m one
[13:50] the first row is one two second row is
[13:52] three four and we times two on that so
[13:55] this is the output
[14:00] yeah uh either anyone have questions uh
[14:04] until now
[14:07] uh if no will continue
[14:11] the second possible node is the variable
[14:14] so variable are are used to hold and
[14:17] update the parameters
[14:19] uh just that just like what we said
[14:21] about it it can be changed further after
[14:24] the creating
[14:26] so to create a variable is also very
[14:29] easy we use the tf dot variable uh
[14:32] please pay attention to the capitalize
[14:34] here
[14:35] as a the capitalizes is necessary for
[14:38] computer to detect
[14:40] and
[14:42] we use the tf.1 to create a matrix uh
[14:46] two by two matrix and with all the
[14:48] elements as one of course you can use
[14:51] the same method we
[14:53] showed above just directly create a
[14:55] matrix
[14:57] the only difference is here is constant
[14:59] here is a variable
[15:01] and also
[15:03] the the the next difference is that the
[15:06] variable cannot be directly called or
[15:09] directly used in the graph you must
[15:13] firstly initialize
[15:15] this variable because
[15:18] it is a variable so it needs an initial
[15:21] value of it so here the initial value is
[15:24] just a two by two matrix with all the
[15:26] elements as one so you need the initial
[15:29] value here you you should just run this
[15:31] line says dot run
[15:33] and run what
[15:35] tfv1.global variable initializer to make
[15:39] it come true
[15:40] so uh
[15:41] let's see the
[15:43] the code for it
[15:46] and here
[15:47] we define the graph and in this graph we
[15:50] place the node on it it is a variable
[15:54] and it is a
[15:55] two by two matrix all the element has
[15:58] one and if we run this code and we
[16:02] create a sessions and we use a
[16:05] session.run to just
[16:07] tell the computer that we want the
[16:08] output to be directly w
[16:11] and it will
[16:12] give you an error
[16:14] why because
[16:17] you can see the error told
[16:19] here the error told you that uh
[16:23] the variables uh
[16:25] was uninitialized
[16:27] so you need this line set dot one to
[16:30] initialize this and then you can use
[16:33] this in this graph
[16:35] and we can
[16:36] once again run this and it will return
[16:38] you the w you define so it is a two by
[16:42] two matrix with all the elements as one
[16:45] yeah
[16:46] and further just as what we describe
[16:49] above we can
[16:52] assign the variable for with a new value
[16:55] so this is
[16:56] what the the constant cannot do
[17:00] so
[17:02] how to
[17:03] update the new value so
[17:06] there is a
[17:07] method dot assign under
[17:10] the the variable you define so
[17:13] w dot assign
[17:16] and
[17:17] inside of the parentheses you can give a
[17:20] new value to this variable w
[17:23] and so let's have a look on that
[17:27] it will
[17:29] uh give you i i first print the initial
[17:32] value and then
[17:34] give the w a new value w dot assign give
[17:37] it a new w the new w is a
[17:41] two by two matrix with all the elements
[17:43] as zero
[17:44] so i once again print this new value so
[17:47] you can see this is the first one this
[17:49] is the new the updated one
[17:56] yes and
[17:58] is there anyone have questions for this
[18:00] widget
[18:03] uh if not we are first uh continuous to
[18:06] the to our next
[18:08] notes possible notes
[18:10] you may notice that even though the
[18:12] variable is more flexible than the
[18:14] constant because it can change
[18:16] it still needs an initial value right
[18:19] you
[18:20] but in practice sometimes the value of a
[18:23] parameter is determined by the actual
[18:26] data for example you do the regression
[18:29] you have the response y
[18:31] and you have the
[18:33] covariates x
[18:35] but
[18:37] but
[18:38] before the data is coming
[18:41] you you can you cannot tell the the the
[18:43] the the indeed the value of the y and x
[18:46] but you know it will be and y and x so
[18:50] how can we handle this uh situation so
[18:53] we use the placeholder
[18:55] we use we use placeholder to tell the
[18:57] computer there is a variable but i don't
[19:00] tell you right now
[19:02] i will give you the value when i open
[19:04] the data faucet so that means in the
[19:08] tensorflow.session
[19:09] in the session i will i i will give you
[19:11] the the
[19:12] the true value but now i i don't know
[19:14] error right
[19:16] so
[19:16] uh
[19:18] we use the
[19:19] because the placeholder is the special
[19:22] uh node under the tensorflow version one
[19:25] so we use the tfv1 dot
[19:27] placeholder to create a placeholder
[19:31] uh tf we we we assigned the node one to
[19:35] be a
[19:36] placeholder
[19:38] tfv1.placeholder and in inside the
[19:40] parentheses the first position tells the
[19:44] tells the tensorflow uh what kind of the
[19:46] data type of this placeholder would we
[19:49] tell that oh this is a float and then i
[19:53] tell the computer that the shape of this
[19:55] node is
[19:57] it has one column
[19:59] and two laws
[20:01] so here i i create a node one a
[20:04] placeholder one and then i i create
[20:07] another placeholder node two
[20:10] and then i will
[20:11] this is very uh interesting just like
[20:14] just like i
[20:15] i i
[20:17] draw a picture
[20:18] so uh in the in the first step i i told
[20:21] it that it i have a note 1 is
[20:25] for example a and b
[20:27] and i taught the computer that there's a
[20:30] node 2 c and d
[20:32] and then the
[20:35] the the next thing we will do is to
[20:36] connect this to a placeholder and we
[20:40] tell the computer that oh uh
[20:43] we just want it to be uh
[20:46] firstly uh
[20:48] the first
[20:49] placeholder times matrix w the w is uh
[20:53] what we defined previously it is a
[20:57] tensorflow variable with all the
[21:00] elements
[21:01] as one is a two by two matrix so it's
[21:03] times one one one one it is a matrix
[21:07] and then
[21:08] uh i told that this will
[21:11] plus the node two c and d
[21:16] and then
[21:17] we we name this to be uh
[21:20] w
[21:21] linear so we are in the in the data flow
[21:25] we will connect them
[21:27] and output w slash
[21:31] and then we uh in the uh this graph is
[21:35] uh created already
[21:36] and for this
[21:38] for for this graph it's it because it's
[21:41] a placeholder that right so is need us
[21:44] to feed the data in it
[21:46] so how how could we do this
[21:49] we also use this is the similar with
[21:51] previous code we
[21:53] named the
[21:54] tfv1.session create a new session and
[21:56] name it as sess
[21:58] and sesta runs a global initializer this
[22:02] makes us
[22:03] this give us ability to use the w we
[22:06] defined before
[22:08] and then
[22:09] firstly i just
[22:11] print out w
[22:12] and then i print out the outcome here in
[22:15] this in this line i just tell the
[22:18] computer that we want w
[22:21] slash to be the outcome right and then
[22:24] because it's involved two placeholder so
[22:26] i
[22:27] used a dictionary here so this
[22:30] dictionary is created for feeding the
[22:32] data so we tell the computer that how we
[22:34] feed the data we use this feed dick to
[22:37] to fit it and the first key is the node
[22:41] one the value is the
[22:43] uh the the the the data i just fit in
[22:47] it's a uh
[22:49] numpy uh vector uh this i use an
[22:52] mp.matrix here so it's also
[22:55] okay for you to use the mp dot array
[22:58] which is an empire area
[23:00] and it's a vector for the
[23:03] first
[23:04] number is 1 and the second number is two
[23:08] and also i tell the computer that our
[23:11] second node node two to be the value of
[23:14] uh
[23:15] first
[23:16] position is one second position is two
[23:19] so the the final finally it will output
[23:22] the w slash uh according to this
[23:24] operation so you can calculate this
[23:27] uh this is uh first is uh 1 times 1 1
[23:30] times 2 which is 3
[23:33] and 1 times 1 1 times 1 and 1 times 2 it
[23:36] is also the 3 3 3 minus 1 2 so it is uh
[23:40] should be the 4 5 right so you can have
[23:42] a try here
[23:45] let us
[23:46] open this
[23:48] and
[23:49] run the code
[23:50] so this is actually conduct this
[23:52] operation
[23:54] and also
[23:56] just as what we said the tensorflow v2
[23:59] is
[24:00] eager
[24:01] execution and now we just disable this
[24:04] eager execution to follow the uh
[24:07] programming style as the tensorflow v1
[24:10] and
[24:11] actually this can be written as the very
[24:14] beginning that means
[24:15] uh in the above in the when you just
[24:18] import the tensorflow and then you can
[24:19] just uh do this
[24:21] but no i just uh conduct this uh in this
[24:24] code block
[24:25] and with the uh
[24:28] g as the graph as default and
[24:31] we put the node 1 node 2 and
[24:35] connect them
[24:36] node one node together and in this graph
[24:39] and with this graph built completely and
[24:43] we firstly
[24:44] initialize it it has w and to see the w
[24:48] and then to fill in the data to see the
[24:51] w linear
[24:53] so you can see that this is a match over
[24:56] expectation right
[24:58] but actually here the there it is the
[25:01] point that it was worth to mention
[25:04] remember that
[25:05] we create the w as a variable uh before
[25:09] right uh here we create a w as a
[25:11] variable
[25:12] but you remember that we change the
[25:15] value of w by assign a new
[25:19] matrix for it right
[25:21] but
[25:23] this time we run the sets of run w it
[25:26] still give us the initial value this is
[25:29] because
[25:30] we run this line we we
[25:33] run sets.1 global initializer it's give
[25:36] the initial value for w once again
[25:39] overlapped the zero all the zero matrix
[25:42] we align it
[25:43] so
[25:44] so you when you adjust the coding you
[25:46] may pay attention to this kind of
[25:48] details
[25:51] and
[25:52] now we have already introduced all three
[25:55] possible
[25:56] nodes for the tensor flow
[25:59] that that is most commonly used
[26:01] uh notes
[26:03] and
[26:04] next we will get into the operations so
[26:07] is there anyone have any questions
[26:09] if not we will continue
[26:14] nope yep so let's continue
[26:17] and just as we can say we can we can see
[26:20] uh
[26:22] recap that this game figure we have
[26:25] already placed the node on the graph
[26:28] the next step is to make sure that you
[26:30] are use the operation to connect them
[26:33] together to forming a
[26:36] complete data flow
[26:38] so uh there are
[26:40] many possible operations you can use
[26:44] tf.ad just add these two elements
[26:46] together it is element wise
[26:49] and if the if the size of two elements
[26:52] is not matched then it will be a
[26:53] broadcast
[26:55] you may know the broadcast in the python
[26:57] right
[26:58] and this is a
[27:00] fundamental thing and you can use the
[27:04] tensorflow.subtract to to to to do the
[27:07] minus
[27:08] and also the the element-wise product
[27:11] can be
[27:12] implemented by the multiply
[27:14] and element-wise divided can be directly
[27:18] implemented by the derivative direct
[27:20] divide
[27:21] and matrix del product can be used can
[27:24] be implemented by the
[27:26] tf.matrix multiplication
[27:29] and
[27:30] there are three judgments and which can
[27:33] return the boolean type
[27:36] values and they are
[27:38] less greater or less less equal
[27:41] so there are commonly used uh
[27:45] operations
[27:47] and and and certainly you can you can
[27:49] just write x plus y
[27:53] but actually you can also write the t f
[27:55] dot add x and y it will it will uh
[27:58] return back the the same the same result
[28:01] but just uh if if you are if you you
[28:04] need a very very strict uh coding style
[28:06] you may also you may
[28:09] always use this a function
[28:11] type operation
[28:15] uh another kind of uh thing is which is
[28:18] very important is the gradient just as
[28:20] what we mentioned about the gradient
[28:22] descent and another kind of optimization
[28:24] algorithms need the gradient
[28:26] so
[28:27] how to calculate the gradient
[28:31] under the tensorflow package there is a
[28:34] function gradient tape it just creates a
[28:37] tape and in this table you can write the
[28:39] formula of the
[28:41] relation between y and x and then use
[28:44] the tape dot gradients
[28:47] the first position you should put into
[28:49] is the the response and others
[28:53] places there's a a bracket and in and in
[28:57] this bracket you put in the covariate
[28:59] and then it will return you the
[29:01] the gradient of the response with the
[29:03] respect of the
[29:05] different covariate
[29:07] separate separately yeah
[29:09] so let's have a have a have a taste of
[29:11] the
[29:13] the of the how to calculate ingredients
[29:15] in tensorflow this is very important if
[29:18] you want to write an
[29:20] algorithm by yourself
[29:23] now
[29:24] we want to calculate the gradient of y
[29:27] equal to the square
[29:29] square power of x
[29:32] so how to do it firstly we just
[29:35] define the variable x equal to t of the
[29:38] variable the value is 3
[29:40] and then
[29:42] use the
[29:44] just as what we mentioned use
[29:45] tf.gradient tape to create a tape name
[29:48] it as tape and then in under this tape
[29:51] you write the formula y equal to the x
[29:54] uh
[29:56] square uh square power
[29:57] power with two
[29:59] and then
[30:00] we use uh tia tape dot gradient first is
[30:05] response y second because we in this
[30:07] case is just a one covariate we just
[30:09] input x x here and it will return the
[30:12] gradient
[30:14] so
[30:15] as we all know the the gradient will be
[30:17] what will be two times x right
[30:20] and then the 2 times x and then the
[30:24] in this step it will return 2 times the
[30:26] value of x it is 3. so it will return 6.
[30:31] so let's see
[30:32] whether it returns 6. so
[30:34] also we
[30:35] after the the the graph is complete we
[30:39] use tf.session to create a session we
[30:42] initialize the variable here and then we
[30:46] return we tell the computer that our
[30:47] target is this
[30:49] gradient
[30:51] and then print it out to seal let's see
[30:53] the result okay it tells us six
[30:56] so this is the the way to uh calculate
[30:59] the gradient
[31:00] uh well uh when
[31:02] we just this is uh this example is for
[31:05] the one covariate right and when there
[31:07] are four covariates and
[31:10] uh
[31:11] let's let's see uh how to handle handle
[31:14] it
[31:16] and uh
[31:18] the first one
[31:20] we were at the firstly we defined uh
[31:22] four four variables three cons and one
[31:25] constant
[31:26] and we
[31:27] see that uh
[31:29] we will define x0
[31:31] x1 x2 and x3 and give it the initial
[31:34] value of it
[31:36] and we give them a name the name means
[31:39] that in in the following graph
[31:42] how we call this variable we call this
[31:45] as little
[31:46] x0 x1 x2 x3
[31:49] and then we we we tell tell the computer
[31:52] that x1 is uh the trainable is is it is
[31:56] false
[31:57] the trainable force is very uh maybe a
[32:00] very useful trick for you if you assign
[32:03] is trainable to be false and then it
[32:05] will uh in the in the following calls
[32:08] and it it will not allow you to
[32:11] calculate the the gradient of it and
[32:14] even
[32:14] update of it so it's just to fix it so
[32:17] it's you can just simply regard it as
[32:19] this it can
[32:20] uh
[32:22] roll back to a constant yeah
[32:24] and with this defined variables we as a
[32:27] constant we
[32:28] create a tape and in this table we write
[32:31] the formula y equal to x square
[32:34] and plus x1 squared plus x2 squared
[32:38] and then do the do the tape.gradient to
[32:40] calculate the gradient and we taught
[32:42] that we we need uh grade to be the
[32:46] gradient with respect of the x0
[32:49] and so we use the the bracket to
[32:52] represent it yeah
[32:54] and so uh
[32:56] also we with this well developed the
[32:58] graph and we
[33:00] set the sessions and we initialize the
[33:03] variables and we obtain output
[33:06] so let's have a look it's six yeah
[33:10] because uh
[33:12] you take the derivative with respect of
[33:14] x zero and then it will only
[33:16] the only term involved will be the first
[33:18] term and then it will be the same result
[33:20] with the
[33:22] the previous code
[33:24] yeah
[33:25] so
[33:26] this is
[33:27] the contents about the gradient
[33:30] so is there anyone has question
[33:33] if no we will
[33:34] move further
[33:39] can you explain the uh the why the
[33:41] trainable equals falls is
[33:44] important again
[33:45] uh yeah yeah uh
[33:47] here i i set the trainable to be the
[33:49] force it means that
[33:52] we can't calculate this uh
[33:54] gradient uh with respect of x1 and do
[33:57] the update so it's you can simply
[34:02] regard this to be go back to the to a
[34:04] constant because previously it is a
[34:06] variable and then use the trainable dot
[34:08] equal to force and then we cannot uh
[34:11] calculate the
[34:12] uh i mean it's the derivative of this
[34:15] term and then just
[34:16] let it be a fixed value
[34:18] so uh if you want to uh
[34:21] see uh more
[34:23] uh information about the
[34:25] the usage of this and you can
[34:29] check the tensorflow.org and to see the
[34:34] details explanation of it so you can
[34:37] just search this and see
[34:40] train
[34:40] [Music]
[34:42] naval
[34:43] and
[34:48] here you may just search the
[34:50] variable
[34:53] yeah and use this tf.variable and then
[34:56] search the
[34:58] trainable
[34:59] and then you may have find the
[35:03] explanation of it
[35:05] along this web page
[35:07] yeah
[35:08] but but uh you
[35:09] to to understand it's easily you can
[35:11] just regard it as it can play a player a
[35:15] player role of the constant yeah
[35:23] uh
[35:24] is there any question else if no will no
[35:26] further
[35:29] yeah and the for for for programming
[35:32] language the the another most important
[35:34] thing is is a logic so in python r we
[35:37] have the if else
[35:40] condition condition so here we also have
[35:43] the same thing
[35:44] we the the core line is just the tf.com
[35:48] co and d conditions and in the first
[35:51] position you will give a statement
[35:53] to to to let the computer make judgment
[35:55] of it whether it is true or false and
[35:57] then if the tr slash statement is true
[36:00] the first position is true and then it
[36:03] will run a and otherwise it will run b
[36:06] so this is a very simple logic
[36:09] uh i think
[36:10] this is a two elements uh uh judgment so
[36:13] you if you if you want to write a more
[36:16] complicated situation have more than two
[36:18] we have three you have four and you can
[36:20] use the tf dot uh switch slash case you
[36:23] can use this function and it will gives
[36:26] you a
[36:27] a way to do that
[36:29] but certainly you can
[36:31] try the
[36:32] tf.condition in the tfl condition
[36:35] so i mean in the position a or position
[36:38] b and you can give another t f dot
[36:40] condition so it may also make the
[36:42] judgment more complicated and to realize
[36:45] more function
[36:47] so an example is that we
[36:50] just
[36:50] create a constant t1 constant t2 and
[36:55] defined uh
[36:57] a function f1 to be
[37:01] x at t1 plus t2 f2 to be t1 minus t2
[37:05] and you set results equal to
[37:07] tf.condition
[37:08] make judgment
[37:10] t1 smaller than t2 so with this uh
[37:13] value so we can always see that it is
[37:16] true so it will run the f1 so it will
[37:19] give you return your
[37:21] t1 plus t2
[37:24] so uh let's see the
[37:26] the code here
[37:28] is not this call this block and then if
[37:30] i'll give you three the t1 plus t2
[37:34] and also the next uh
[37:37] logic is well
[37:39] well loop so uh in the well loop is also
[37:43] needs a
[37:44] judgment in the first position
[37:46] and
[37:47] the the body in the second position the
[37:49] variables in the third position so uh if
[37:52] the the first position judgment is true
[37:55] then it will run the body
[37:57] until the statement is false
[37:59] and the variable positions needs a two
[38:02] pose or at least
[38:03] include all the tensorflow variable
[38:06] needed in the body parts
[38:08] so what does this mean it means that
[38:11] here for example in the body part you
[38:13] have a variable t1 and t2
[38:17] and then here you just you should need a
[38:19] parenthesis and
[38:21] just include t1 t2 here to tell the
[38:24] computer that in the body we need these
[38:26] two variables
[38:28] so it is very strict but
[38:31] it is a the method how to use it
[38:35] the t1 is a constant t2 is a constant as
[38:38] well
[38:38] and define the body
[38:41] to be a
[38:42] uh
[38:43] tf dot at t1 t1 minus 1 and and and then
[38:48] uh
[38:50] t
[38:51] and then t2 and you define the body like
[38:55] to be a a function as this function is
[38:58] that it's just
[38:59] t1 equal to the t1 plus one and then
[39:02] return the t1 t2 so
[39:06] the first place is also the
[39:08] just the input is a condition
[39:10] so it is t1 smaller than t2 so uh in
[39:14] this case
[39:15] obviously it is correct so it will run
[39:18] this body
[39:20] so in this body as we can see i see that
[39:22] it just
[39:24] updates t1
[39:25] by t1 plus one
[39:28] and then until this uh
[39:31] condition does not hold true
[39:34] so
[39:34] the first is one five the second is uh
[39:38] the second place is uh two five and then
[39:41] three five four five five five
[39:44] uh until the five five the uh
[39:47] this condition is false and then it will
[39:49] stop and then return the
[39:52] result
[39:54] so uh
[39:55] this is a more uh complicated than the
[39:58] eval statement but is uh sometimes
[40:00] useful in the programming so let's try
[40:03] this
[40:04] so it gives you five five
[40:06] so this is a how to use a while loop
[40:10] so it is not very
[40:12] different with just the directly used r
[40:14] or python but
[40:16] you should follow the the style of it
[40:19] the the structure of it yeah
[40:22] uh there anyone has questions as well
[40:26] nope
[40:27] nope then we will continue for our
[40:30] framework yeah so after uh until now we
[40:33] have already involved all the
[40:35] uh the widget or the operations uh
[40:40] that that is designed for today's and
[40:43] now we just turn into a
[40:46] some
[40:47] example to just
[40:51] just make sure that you just fully get
[40:53] the
[40:54] the idea of it
[40:56] uh the the example is uh the simulated
[40:59] example is a linear regression so we
[41:01] assign the true model to be the y equal
[41:04] to
[41:05] x times beta the beta is the
[41:08] vector the first element is one second
[41:10] element is five plus epsilon epsilon is
[41:13] the residual uh which obeys a normal
[41:15] distribution with zero as mean and one
[41:18] as the
[41:18] standard
[41:20] as the variance
[41:22] so uh firstly we we we just use the the
[41:26] numpy the python numpy to create the
[41:28] data set right
[41:30] so uh we set the the data size to be the
[41:33] 5000 sample
[41:35] and then use the
[41:37] numpy.random.normal to generate the
[41:40] multi-normal distributions very
[41:42] sample so the x is a two-dimensional
[41:49] uh uh
[41:51] multi-normal uh random variables and
[41:54] with uh y to be the
[41:57] uh
[41:57] np dot dot to be the matrix uh
[42:00] multiplications
[42:02] of x and another matrix
[42:05] this matrix is the beta matrix here
[42:08] it is a vector 1 and 5
[42:10] and then
[42:12] plus the residual term residual term
[42:14] also created by the numpy random.normal
[42:19] yeah so
[42:20] this first block is served as the data
[42:23] generator
[42:25] and with all the data ready
[42:27] now we just
[42:29] want to
[42:30] make sure make a just as what i
[42:33] mentioned about we need a
[42:35] graph we need a
[42:36] design design a designer graph to
[42:39] uh represents the the data flow the
[42:42] computation flow of it so here is a
[42:45] empty graph
[42:47] and uh for in this empty graph
[42:50] uh we
[42:52] so firstly you should just have an idea
[42:54] that how to calculate the the calculator
[42:57] operation you you like to use
[43:00] so here we use the the newton method
[43:02] which is updated raw just based on the
[43:04] gradient you can you can also call this
[43:07] gradient descent so it means that we
[43:09] give it a initial value it may be better
[43:13] we call it beta0 we can and then update
[43:16] it so how to update it and it it is
[43:19] about uh
[43:21] mathematical knowledge we
[43:23] use the
[43:24] beta
[43:26] you call beta order equal to the beta nu
[43:28] equal to the beta old
[43:31] minus and given learning rate times the
[43:35] gradient
[43:36] the gradient of loss functions with
[43:39] respect of the beta
[43:41] so uh this is all about the the the mass
[43:45] knowledge so so how we
[43:48] uh write it in the code
[43:50] so here we give uh
[43:52] our set a variable uh
[43:54] for the beta so the beta this data just
[43:58] represents the estimation of beta so we
[44:00] we we just assign it to be a variable
[44:03] then with initial value of all all the
[44:05] elements to be zero
[44:07] and then
[44:08] we want to calculate the the gradient
[44:10] right
[44:11] so firstly we we should remember that
[44:14] for to calculate the gradient we should
[44:16] firstly write down the expression of it
[44:19] so
[44:20] the expression of it is y
[44:23] minus
[44:24] x transpose beta
[44:27] and do the
[44:29] the the this reduced mean is the i mean
[44:32] i mean that i i just tell the computer
[44:35] that first they will calculate the
[44:36] square and for the squared arrow we
[44:39] calculate the mean to be the loss
[44:41] so the loss is played it's just
[44:44] so it's just player player row as we
[44:46] tell the computer our loss is y equal to
[44:49] uh
[44:50] the law the loss is equal to the
[44:53] sum of
[44:54] y minus x beta
[44:57] square
[44:59] divided by n
[45:01] yeah so we tell computer losses looks
[45:03] like this
[45:05] and with the expression of the loss we
[45:08] use the
[45:09] tape tap dot
[45:11] gradients and first position is a
[45:13] response right this y is lost
[45:16] and the second position is the
[45:18] covariance you need to take derivative
[45:21] with with respect of we take derivative
[45:24] with respect of the beta so beta is here
[45:26] and then the
[45:28] uh
[45:30] the gradient will be equal to the beta
[45:32] minus given a given learning rate is 0.5
[45:36] and times this gradient so this is the
[45:41] the so the grid here is just player row
[45:43] of new beta
[45:44] the beta nu is equal to beta old minus
[45:47] given learning rate times the gradient
[45:50] so this is the updated uh
[45:52] function the formula and so with
[45:55] all of the things on the graph and we
[45:58] run the assess we run a session we
[46:01] firstly do the initializer of the
[46:03] variable and then to tell the computer
[46:06] that
[46:06] this grid this new beta will be the
[46:08] output so let's see the result
[46:12] so if we last the initial value of beta
[46:15] to be 0 and 0 the first iteration will
[46:19] return you the result of
[46:21] approximately 0.8 and 5.1 so actually
[46:25] this is a very close to the true value
[46:27] one and five
[46:30] and
[46:31] the actually this is not enough right we
[46:33] need to update the beta one time one
[46:35] time on another this is the iteration
[46:38] so actually for
[46:39] uh writing a a beautiful
[46:42] code well we need a framework so you may
[46:45] have already used have a use of the
[46:47] secret learn right in the section learn
[46:49] the the
[46:51] the machine learning package for the
[46:53] for python just
[46:55] have this kind of style you just call
[46:57] this function to build a model
[46:59] and then for model it it frequently have
[47:02] a
[47:03] method called feed
[47:05] feed and then you can fit in the data x
[47:07] and y
[47:08] and this model will be trained
[47:10] completely and then you use the model
[47:12] dot predict okay to give a value to make
[47:15] prediction so this is actually what a
[47:18] kind of tool the the product we i mean
[47:21] the final products we will we would like
[47:23] to see
[47:24] so how to realize that in python
[47:28] frequently we use class function
[47:30] for example we define a class named the
[47:32] linear regression
[47:34] and
[47:35] next in the class it should have some
[47:38] initial right the slash slash initial
[47:41] slash frequently in the class to let you
[47:44] define the initial value
[47:46] so uh
[47:47] the self is just
[47:49] you can treat it that all the thing
[47:51] related with itself will be shared will
[47:53] be shareable in this class function
[47:57] and here you set a
[47:59] initial value of beta to be a tensorflow
[48:02] variable i told that this type is flawed
[48:06] and we name this
[48:08] variable to be the beta
[48:11] and then
[48:12] we define
[48:13] in this function we have already find
[48:15] the variable and i will create another
[48:17] function define the loss
[48:20] loss is just
[48:22] because here it's also need itself
[48:24] because we need the beta value here so
[48:27] we need self and we need an x and y and
[48:30] it will return you that
[48:32] the x minus
[48:36] the y minus x
[48:38] the matrix multiplication with beta and
[48:42] then do the sum of square the mean value
[48:44] of mean value of sum of square
[48:46] and then return the loss so this this
[48:48] function define the loss
[48:51] another function defined
[48:53] uh
[48:54] uh this call this coefficient function
[48:56] is not very important it just led you to
[48:59] finally you have already trained the
[49:00] model and then you want to obtain the
[49:02] coefficient then you can call this
[49:04] function
[49:05] so it is not very uh
[49:08] it's not it's not very useful but the
[49:10] the core thing is in this fit function
[49:13] in this fit functions you need an input
[49:15] x and y and given a learning rate gamma
[49:18] equal to 0.5 you settle the maximum
[49:21] iteration to be 100
[49:23] and you define the
[49:25] gradient tape looks like this this is
[49:27] what we have previously
[49:29] and we will just
[49:31] call this gradient as dw
[49:34] finally you just use the you will use
[49:37] what we
[49:39] previously talked about we for the
[49:41] variable we can assign a new value for
[49:44] it you can just say variable this
[49:46] variable beta dot assign
[49:49] but this function a line slash sub means
[49:52] that beta nu equal to beta old
[49:57] subtract this kind of thing so this is
[50:00] similar you can just write it in another
[50:01] way just use the the function assign but
[50:04] this is also okay to write in this way
[50:07] and we tell that the beta update uh just
[50:10] the subtract this gamma
[50:13] times the
[50:15] gradient we calculate about
[50:17] and then with uh tensorflow create a
[50:20] tensorflow session we use session.1
[50:23] firstly
[50:25] give the give the variable initialize
[50:28] and write a for loop and in the for loop
[50:33] we just
[50:34] print the self.beta so you can see oh
[50:37] this is a
[50:39] how to reflect that it has already
[50:41] suffering from the iteration because we
[50:43] talk computer this is uh we want
[50:45] now we want the result of beta the
[50:48] computer will automatically trace back
[50:50] okay you want the beta so on the on the
[50:53] big graph where the beta come from okay
[50:56] the beta comes from this line
[50:59] so it will automatically run this line
[51:02] the oh so it will so so in this line it
[51:05] will involve dw so where did dw come
[51:07] from
[51:08] okay dw will come from the calculation
[51:10] of the gradient
[51:11] so for the
[51:13] through this uh
[51:15] graph it will here it will
[51:18] work through for
[51:20] 100 times because there is a for loop
[51:24] and then uh
[51:26] we have already run this loop for 100
[51:29] times then we print the final
[51:32] output self.beta
[51:34] and then we save this uh model because
[51:37] we have already tried this right we want
[51:39] to save this uh beta coefficient so we
[51:43] save this model to be a saver and
[51:46] saver.save but we named this model to be
[51:49] why
[51:51] my model uh
[51:52] my model yes
[51:54] and
[51:56] actually this is not very necessary if
[51:58] you don't want to save this
[52:00] but if you save this and then it will
[52:03] give you a convenience to
[52:05] obtain the coefficient
[52:07] so here we we just uh from just we don't
[52:10] need to train again we just from the the
[52:13] model and then to import this uh
[52:16] well-trained beta and then return the
[52:18] beta so this is just the trick of it
[52:22] and also the predict
[52:24] function and with all this function
[52:26] together
[52:28] and we can just call this o model equal
[52:30] to the linear regression
[52:32] then it will uh gives you an initial of
[52:35] beta and then you you tell the hotel.oh
[52:38] we will use x and y to trend this model
[52:42] and then it will automatically uh
[52:44] iteration 100 times and give you a final
[52:47] estimation
[52:48] 0.99 and 5.01 so it is very close to the
[52:52] true value
[52:53] and also you because we have already
[52:55] designed so many functions in the class
[52:58] so we can uh directly call the
[53:00] prediction and then it will also we can
[53:03] all obtain the coefficient directly from
[53:06] the storing
[53:08] uh models in the
[53:10] uh
[53:11] in the
[53:12] meta file yeah so this is uh basically
[53:15] the
[53:17] how to create a such a
[53:19] learning framework for
[53:21] tensorflow
[53:22] just combining the knowledge of python
[53:25] yeah and the last part is i think we
[53:27] have already made the time limit but
[53:30] actually i want to mention the
[53:32] the real data analysis the real data is
[53:35] the data is from the
[53:36] batchsentiment.com it is a data with
[53:39] respect of
[53:41] uh the
[53:42] uh the the football match and the the
[53:46] there's a lot of calvaria's home home
[53:49] team awaiting and the sentiment score
[53:52] for the home team away team and for the
[53:54] players and the response is the final
[53:56] result result is uh
[53:59] there are three possible values one two
[54:00] three it means different have different
[54:02] meanings
[54:03] and then with this kind of thing you can
[54:05] train a
[54:07] trainer neural network models and in the
[54:09] neural neural network model the the
[54:11] codes are also
[54:13] below
[54:14] yeah and also actually we
[54:17] the first idea is that you need to just
[54:20] have a you need to train it
[54:22] singly and then pack it up by the class
[54:25] function
[54:26] so the the first uh
[54:28] several blocks is
[54:30] data preprocessing of the
[54:33] of the of the of our data and just this
[54:36] in this line creates tree model actually
[54:38] is the
[54:40] standard all iso called standalone it
[54:42] means that it just run this one times
[54:45] and
[54:46] this block is for the visualization yeah
[54:49] visualization of the performance and
[54:52] this code this block is just a
[54:56] same step as what we previously have we
[54:59] package all the
[55:00] functions feeds predicts
[55:03] get coefficient
[55:04] into one class and
[55:06] and we call this we can directly call a
[55:09] build a model by this and feed the value
[55:12] in here
[55:13] and obtain the coefficient and make
[55:14] prediction we also use this kind of
[55:17] class
[55:18] model a class type model
[55:22] and so uh in here in this uh
[55:26] i mean real data analysis we
[55:28] we use the feed we use the placeholder
[55:31] but because we just we assigned the beta
[55:34] to be a placeholder and then uh
[55:37] affiliate uh
[55:38] just as what i mentioned the x and y
[55:41] there is x and y but actually when i
[55:43] designed the code we i can't tell you
[55:46] the exact value of it even the initial
[55:48] value i don't know so i will use
[55:50] placeholder and then we will use the fit
[55:53] dictionary to feed in the
[55:55] the true value of of the x and y and
[55:57] then the the other thing is just similar
[56:00] with what we have in the linear
[56:02] regression
[56:03] yeah so that is uh
[56:06] basically the the contents for today
[56:08] so if you have a question you may
[56:11] either ask here or by buy the by the
[56:14] email from me so you can just send me
[56:16] the email and i will reply if you have
[56:19] any relative questions with it my email
[56:21] is june.jin
[56:23] uconn.edu so you can email me if you
[56:26] have
[56:27] any further questions yeah
[56:30] and all the materials is uh
[56:32] in okay you know github reports and
[56:35] there's a department website there's our
[56:38] data science lab website yeah
[56:41] thank you thank you for listening yeah
[56:48] yeah i i i'll be in this room for