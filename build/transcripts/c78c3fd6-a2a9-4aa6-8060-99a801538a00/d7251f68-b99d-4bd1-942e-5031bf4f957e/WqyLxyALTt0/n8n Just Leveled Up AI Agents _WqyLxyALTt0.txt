Video Title: n8n Just Leveled Up AI Agents (Anthropic's Think Method)
Video ID: WqyLxyALTt0
URL: https://www.youtube.com/watch?v=WqyLxyALTt0
View Count: 81,197

[00:00] All right, so here is our beautiful
[00:01] ultimate assistant with the new GPT 4.1
[00:03] chat model as well as the new think
[00:05] tool. All right, can you please create
[00:06] an event for tonight 7 p.m. It will be
[00:09] an hour long with Dexter Morgan. And can
[00:11] you send him an email to make sure that
[00:12] time works? Can you delete my dinner at
[00:14] 6 p.m. And then can you email Michael
[00:16] Scott to let him know that I'm not going
[00:17] to make it? Okay, cool. So, we just sent
[00:19] that off. Our ultimate assistant is
[00:21] going to get that message. Use GBT41 in
[00:23] order to take action. And as you can
[00:25] see, it's going to hit its different
[00:26] child agents down here. So, it's using
[00:28] the contact agent to get that
[00:29] information, the email addresses. Now,
[00:31] it's going to hit calendar and email
[00:32] agent in order to delete that event,
[00:35] create the event, send off those two
[00:36] emails, and then, as you can see, it
[00:38] just used its think tool down here to
[00:40] make sure that it did everything right.
[00:41] So, the response that we got was all
[00:43] set. Your 7 p.m. meeting with Dexter
[00:44] Morgan has been scheduled, and he's also
[00:46] been emailed. The 6 p.m. dinner has been
[00:48] deleted, and Michael Scott has been
[00:49] notified that you can't make it. So
[00:51] before we go verify that, let's click
[00:52] into the Ultimate Assistant and we'll
[00:54] look at its logs because this lets us
[00:56] see exactly how it thought about what it
[00:58] needed to do and what actions it took.
[01:00] So when we're looking at the logs, we
[01:01] read it from top to bottom. So the first
[01:03] thing it did was it used its memory to
[01:04] see if there was any past conversations.
[01:06] And then it used its chat model to read
[01:08] through its system prompt and understand
[01:09] what I need to do. From there, it's just
[01:11] a mix of tool calling and going back to
[01:12] its model. So the first thing it did was
[01:15] contact agent to get Dexter Morgan's
[01:17] email. And then it did the same thing
[01:18] for Michael Scott. Then it went to the
[01:20] calendar agent to create an event
[01:21] tonight for 7 p.m. called meeting with
[01:23] Dexter Morgan. Then it went to the email
[01:25] agent to send Dexter an email to confirm
[01:27] if that works. It went to the calendar
[01:29] agent to delete the dinner. It went to
[01:30] the email agent to let Michael Scott
[01:32] know that we're not going to make it to
[01:34] dinner. And then finally, what it did is
[01:35] it used its think tool, which is
[01:37] basically like a scratch pad for your
[01:38] agent to write down its thoughts and its
[01:40] actions and just to make sure that it's
[01:42] aligned with what you asked it to do.
[01:44] So, what it wrote down here was
[01:45] confirmed that all requested actions
[01:47] were completed, created the meeting
[01:48] event for 7 p.m. with Dexter Morgan,
[01:50] emailed him to check if that time works,
[01:52] deleted the 6 p.m. dinner, and notified
[01:54] Michael Scott by email. No further steps
[01:56] needed unless a user requests more. So,
[01:58] if at this point it realized, okay, I
[02:00] wrote down what I did and what the user
[02:01] wanted. Oh, I actually do have more
[02:03] things I need to do. I'm going to go
[02:04] back and call those tools until I'm
[02:06] ready to go. So, this was perfect. It
[02:08] did its actions. It thought about it,
[02:09] and it decided that we were finished.
[02:10] And then it let us know all the things
[02:12] you just requested have been done. So
[02:13] looking at my calendar, you can see the
[02:15] 6 p.m. dinner was deleted and we now
[02:16] have a meeting with Dexter Morgan. And
[02:18] we can see that he was in fact invited.
[02:20] You can see here in our sent emails, we
[02:22] have a meeting time confirmation for
[02:23] Dexter and 7:00 p.m. And then we also
[02:25] have a meeting to Michael Scott about
[02:27] dinner tonight. We're not going to be
[02:28] able to make it. So I'm super excited to
[02:30] continue to experiment with this kind of
[02:31] stuff. when you pair a really nice model
[02:33] with the ability to basically write down
[02:35] its thoughts and reason. I think that
[02:37] this is going to be huge for creating
[02:39] agentic systems. But let's hop into one
[02:41] more demo and then we'll break down what
[02:43] this node does, how it works, and when
[02:45] to use it. Okay, so here is our think
[02:47] agent. As you can see, it has two tools
[02:49] as well as a think tool. So, what I'm
[02:50] going to do in this chat is ask it if me
[02:52] and three friends can stay from April
[02:54] 15th to April 20th. So, we'll send that
[02:56] off and we'll see what it does. You're
[02:58] going to see here that it's going to
[02:59] call both the quotot and the
[03:00] availability tool. And then it's going
[03:02] to hit the think tool. And then you'll
[03:03] notice it hit the quoter tool again. So
[03:05] as you can see, it responds to us with
[03:07] you and your three friends, four guests
[03:08] total, can stay from April 15th to the
[03:10] 20th. Those dates are available.
[03:12] However, I was unable to retrieve a
[03:14] quote for your requested stay at this
[03:15] time. If we click into the logs of the
[03:17] actual agent, we will see exactly what
[03:19] it did. You can see that it first called
[03:20] the availability tool, which the
[03:22] response is that the dates are
[03:23] available. It then tried to call the
[03:24] quot tool, and the response was no quote
[03:27] provided. So the agent at this point
[03:28] realized, okay, I ran into an issue. I'm
[03:30] going to use this think tool to think
[03:31] about what I need to do. Basically, what
[03:33] this is is a scratch pad for the agent
[03:35] just to write something down and think
[03:36] about it. So, we're essentially turning
[03:37] GPT41, which is not a reasoning model,
[03:40] into a reasoning model. What we wrote
[03:42] down was availability has been
[03:43] confirmed, but the quote was not
[03:45] provided. I need to retry the quote
[03:46] request with the correct number of
[03:48] guests and dates. So, it's thinking,
[03:49] okay, maybe I just sent over the wrong
[03:51] information. Let me go ahead and try
[03:52] again. What it did is it called the
[03:54] quoter tool once again and same thing.
[03:56] No quote provided. And so what I just
[03:58] did is I fixed the quoter tool. So let's
[04:00] go ahead and resend this off and see if
[04:01] we get anything different. It should
[04:03] call both these tools and then it should
[04:04] call the think tool just to make sure
[04:05] that it did everything correctly. As you
[04:07] can see, it just called the think tool.
[04:08] And now we have our actual availability
[04:11] with the quote. So once again, we'll
[04:13] click into the agent. We'll click into
[04:14] logs and we'll see that it got the
[04:16] availability. It got the quote, which at
[04:18] this point was something ridiculous like
[04:20] 12,000 bucks. And then it hit the think
[04:22] tool just to make sure. So it says
[04:24] confirming the requested dates are
[04:25] available. There are four guests. The
[04:27] quote tool has provided the per person
[04:29] cost of 3,000 for 6 days at 5,000 per
[04:32] day. The user wants to know if four
[04:34] people can stay and what the total cost
[04:35] is. So now I should provide the total
[04:37] cost for all guests and confirm the
[04:39] availability. And that's exactly what it
[04:40] did down here. It responded to us with
[04:42] that information. All right. So now that
[04:44] you guys have seen a demo of the tool in
[04:45] action, let's break down what the think
[04:47] tool is, how it works, and how it can be
[04:50] helpful for your workflows. If you guys
[04:52] don't know me, my name is Nate. I've
[04:53] been building AI agents and AI workflows
[04:55] in NAND for a long time now. I run a
[04:57] school community that you can join for
[04:58] free that has all of the resources from
[05:00] any of my YouTube videos in there
[05:02] including all the templates that we're
[05:03] going to go over today in NIN. And then
[05:05] I also run a paid community that has a
[05:06] full course on building AI agents in
[05:08] NADN as well as other deep dive topics
[05:10] as you can see right here. Um we also
[05:12] have five live calls per week and it's
[05:13] just a community of a thousand other
[05:15] members who are building with NN. So the
[05:17] link for both of those communities will
[05:18] be down in the description. But let's
[05:19] get back to the video. So what we're
[05:20] talking about today is this think tool.
[05:22] It just got released in N8N version
[05:24] 1.88. So if you don't see it, you
[05:26] probably need to update your instance.
[05:27] So as you can see in the demo, the think
[05:28] tool is basically just a tool like a
[05:31] notepad for the agent to write things
[05:33] down while it's trying to solve through
[05:35] problems and call different tools. And
[05:36] this was inspired by Anthropic's
[05:38] article, as you can see right here,
[05:40] about the think tool, which enables
[05:42] Claude to stop and think in complex tool
[05:44] use situations. As you can see here, it
[05:46] is a new tool that improves Claude's
[05:48] complex problem solving performance. I'm
[05:50] not going to read through this whole
[05:51] article. I'll link it down below, but
[05:52] let's cover a few of the key points
[05:53] here. So, what is the think tool? It
[05:55] gives Claude the ability to include an
[05:57] additional thinking step complete with
[05:59] its own designated space as part of
[06:00] getting to its final answer. So, imagine
[06:02] you were trying to work through some
[06:03] sort of complex math problem, but you
[06:05] had no scratch paper and just nowhere to
[06:07] write down your thoughts and sort of
[06:08] reason. So, if you've ever played around
[06:10] with some of the reasoning models,
[06:11] you'll notice when you ask them a
[06:12] question, they basically will think out
[06:14] loud and they'll they'll read what
[06:16] they're saying and then they'll use that
[06:17] to reason to get to the best answer. So
[06:19] essentially what we have the ability to
[06:20] now do in NAN with our agents is turn
[06:23] almost any model into a reasoning model
[06:25] because they now have the ability to
[06:26] write down their thoughts and reflect
[06:28] upon them. So anyways the think tool is
[06:30] similar to extended thinking but it's a
[06:32] different concept. Extended thinking is
[06:34] all about what claw does before it
[06:35] starts generating response. What we're
[06:37] doing here with this thinking tool is
[06:39] the ability to after it's done
[06:41] processing different tool outputs then
[06:42] think about it or in the middle of these
[06:44] tool calls think about what it needs to
[06:46] do next. As you can see down here, this
[06:47] is particularly helpful when performing
[06:49] long chains of tool calls or in long
[06:51] multi-step conversations with the user.
[06:53] Anyways, down here it gives a sample
[06:55] implementation of this tool. The name is
[06:57] think. Here's the description. And this
[06:58] is actually going to be the prompt that
[07:00] is in the default native think tool.
[07:03] You'll see that in a sec. And then of
[07:05] course they also did some testing in
[07:06] here. So here is cloud 3.7 performance
[07:08] on a task where it has just a baseline.
[07:11] It has the ability to think, it has
[07:12] extended thinking, but then up here it
[07:14] has thinking plus a prompt. And so, as
[07:16] you can see, it says that the best
[07:17] performance was achieved by pairing the
[07:19] think tool with an optimized prompt
[07:21] about how to use the Think Tool and what
[07:22] it's going to do. So, here's an example
[07:24] of using the Think Tool, a sample prompt
[07:26] that you can feel free to look through
[07:27] if you go to this article. Anyways,
[07:29] there's a lot more details in here, but
[07:30] there's two more things I wanted to
[07:31] touch on, which is when to use the Think
[07:32] tool and when not to use the Think Tool.
[07:34] So, they provide three scenarios to when
[07:36] to use it. First one is tool output
[07:37] analysis. The second one is policyheavy
[07:40] requirements, and then the third one is
[07:41] sequential decisionmaking. And then
[07:43] there's the whole conversation of when
[07:45] not to use the think tool, which I think
[07:46] is something really important to quickly
[07:48] cover because in this AI space that's
[07:50] moving so quick and you have new models
[07:51] dropping and new tools dropping and new
[07:53] frameworks dropping, um I feel like we
[07:55] all tend to get caught up in the hype
[07:56] bubble and just want to shove all these
[07:58] different tools into everything. And the
[08:00] reality is a lot of times that's just
[08:02] going to hurt the performance and it's
[08:03] just overengineering. So when not to use
[08:05] this think tool would be nonsequential
[08:07] tool calls and if it's very simple
[08:09] instruction following. So, quick
[08:11] disclaimer. These examples I'm going to
[08:13] show today in Naden may not always be
[08:15] the most optimized of when to use the
[08:16] Think Tool or when not to. But what I
[08:18] wanted to do is get in here, show you
[08:20] guys kind of how it works with different
[08:21] models, how you may think about
[08:22] prompting your agents to use the Think
[08:24] Tool, and just in general what it looks
[08:26] like and and what it does. So, I'm going
[08:28] to start off in this workflow with just
[08:29] a few simple examples, and we'll compare
[08:31] 41 Mini and Claw 3.5 Sonnet with the
[08:33] Think Tool. And then we'll get into a
[08:34] bit more of a complex one that looks
[08:36] like this. We'll compare some more
[08:37] models, and we'll just kind of see how
[08:39] it goes. Cool. Cool. So, just starting
[08:40] off, let's test its thinking ability
[08:42] with some riddles. So, the first one I'm
[08:44] going to ask is, I speak without a mouth
[08:46] and hear without ears. I have no body,
[08:48] but I come alive with the wind. Who am
[08:50] I? And so, we'll see what happens. Then,
[08:52] we'll look at the prompt that we have in
[08:53] both the agent as well as the think
[08:54] tool. As you can see, Claude just called
[08:56] the think tool. And then the answer down
[08:58] here is an echo, which is correct. So,
[09:00] first things first, let's look at the
[09:02] system prompt for the thinking agent.
[09:04] All I said was, "You're a helpful
[09:05] assistant." And then I just gave it
[09:06] access to its tool which is think. And I
[09:08] just said use this tool to think deeply.
[09:10] If we click into the logs over here, we
[09:12] can see what it did is it immediately
[09:13] hit its think tool because it knew this
[09:15] was sort of like a riddle. And it said
[09:17] let me analyze the key characteristics
[09:18] given in the riddle. One speaks without
[09:20] a mouth. Two hears without ears. Three
[09:22] has no body. Four comes alive with wind.
[09:24] These characteristics point to something
[09:26] that makes sound without physical vocal
[09:28] organs. Blah blah blah. So as you can
[09:29] see it's thinking about how it actually
[09:32] needs to approach solving this riddle.
[09:33] And it looks like the thinking tool
[09:35] actually came to the conclusion of an
[09:36] echo. Now, it's interesting because if
[09:38] we were to disconnect claw 3.5 sonnet
[09:41] and plug in GPT4.1 mini, which is
[09:43] phenomenal, by the way, we will send
[09:45] this off again and we'll see what
[09:46] happens here. So, this one got the
[09:49] answer right much quicker. You can see
[09:51] it responded echo, but it didn't call
[09:53] the think tool. So, here's an example
[09:55] where they choose to use the tools
[09:57] differently. the prompting is a little
[09:58] bit different, but um I think it's
[10:00] really powerful to understand how the
[10:02] models will use the Think Tool and
[10:03] connecting different ones. And also just
[10:05] wanted to show you guys GPT 4.1 versus
[10:07] GPT 40. As you can see, first of all,
[10:10] GPT41 and all of its family models, so
[10:13] the mini and the nano have a 1.05
[10:15] million context length. Whereas over
[10:17] here with 40, we only have 128,000. And
[10:19] this basically just means how many
[10:20] tokens the model can look at in one try.
[10:22] And over here with the 4-1 family, we
[10:24] have it's basically eight times higher.
[10:27] Anyways, when it comes to pricing, you
[10:28] can also see that 41 is significantly
[10:31] cheaper. So, it's going to be $2 for a
[10:33] million input tokens and $8 for a
[10:35] million output tokens. And over here,
[10:37] it's basically tripled and basically
[10:39] doubled. So, GPT41 and its subm models
[10:42] are going to be a lot cheaper. Anyways,
[10:43] let's feed one more riddle to 41 mini
[10:46] and see how it handles it. A man lives
[10:47] on the 10th floor. Oh, wow. It already
[10:49] answered that really quick. Basically,
[10:51] the man lives on the 10th floor and he
[10:52] takes the elevator all the way down. But
[10:54] when he's going back up to the 10th
[10:55] floor, he stops at seven and then walks
[10:56] the rest of the way up the stairs. And
[10:58] the answer is that the man is short and
[11:00] can only reach the seventh button.
[11:01] Anyways, let's real quick just plug this
[11:03] into 3.5 sonnet and see if it uses the
[11:06] think tool. Cuz as you noticed right
[11:07] there, GPT41 mini did not use the Think
[11:10] Tool. So, we'll see if Claude 3.5 sonnet
[11:12] needs to. Or maybe it's not even needs
[11:14] to. I have a feeling if we disconnected
[11:15] the Think Tool, Claw 3.5 Sonnet would
[11:17] still get this riddle right. But it's
[11:19] cool just to see that Claude likes to
[11:20] rely on the think tool in order to
[11:22] reason through the the riddle. So we can
[11:24] see right here it calls it and it
[11:26] analyzes the whole situation. And as you
[11:28] can see it came to its conclusion in the
[11:29] think tool. The man must be short in
[11:31] stature. He can reach the button for the
[11:32] seventh floor but cannot reach any
[11:33] higher buttons in the elevator. And also
[11:35] one thing to keep in mind is the actual
[11:37] prompt within this think tool. There's
[11:39] nothing really to set up. This default
[11:41] think tool description is to use it to
[11:42] think about something. It's not going
[11:44] to, you know, obtain new information or
[11:46] change the database. And this right here
[11:48] is pretty much exactly what we saw in
[11:49] the anthropic article for the
[11:51] description of the tool right here. And
[11:52] for the sake of this video, I'm not
[11:54] touching this. I'm leaving this as
[11:55] default. But remember, you can optimize
[11:58] the way that your agents use this think
[11:59] tool if you put a different type of
[12:01] description in here. Okay. Now, moving
[12:03] into a different example with the think
[12:04] tool. We have a few more tools here. We
[12:06] have a pine cone vector store that has
[12:08] information about a knowledge base. We
[12:10] have the send email tool, the contacts
[12:11] tool, and then also a web search tool.
[12:13] And right here is our handydandy think
[12:15] tool. So, what I did in this agent is we
[12:17] just prompted it with the different
[12:18] tools it has and when to use them. And
[12:20] then I just gave it an important
[12:21] message, which was never make up an
[12:23] email address. If you can't find it,
[12:24] think about what you should do. So,
[12:26] we're going to fire off this message
[12:27] that's asking the agent to send an email
[12:29] to Michael Scott about dogs. And we
[12:30] wanted to research dogs online first.
[12:32] So, it just hit the web search tool.
[12:34] It's hitting the contacts tool, and then
[12:36] it went to send email after it had
[12:38] Michael Scott's contact information. So,
[12:40] as you can see, an email has been sent
[12:41] to Michael Scott about dogs. Let's go
[12:43] check on that in our email. Okay, not
[12:44] going to read this, but as you can see,
[12:46] the email did come through. But what we
[12:47] noticed here is that it didn't use the
[12:48] think tool, and it's probably because of
[12:50] the way we prompted it. So in the system
[12:51] prompt, when we said, "Hey, you have a
[12:53] tool called think." We said, "Use this
[12:54] tool to think deeply when you run into
[12:56] an issue before asking the user." So it
[12:58] didn't run into any issues. So maybe
[13:00] that's why it didn't call it. Let's
[13:01] throw a situation where it may have run
[13:03] into an issue. Okay. So as you saw in
[13:04] the previous example, it was using
[13:06] contacts air table base right here to
[13:08] get contact information. So here was
[13:10] Michael Scott's email. What we're going
[13:11] to do now is ask it to send an email to
[13:13] Jennifer Martin that Jennifer Martin
[13:15] doesn't exist in our air table. So even
[13:17] though this response came back
[13:19] successful, it's not going to have
[13:20] Jennifer Martin's email. As you can see,
[13:21] it just called the think tool and it
[13:23] says I couldn't find Jennifer Martin's
[13:25] email address in our contacts database.
[13:27] So let's click into the logs to see what
[13:29] it tried to do. First thing it tried to
[13:30] do is hit the contacts database, pulled
[13:33] back all the contacts, and then it knew,
[13:34] okay, I also need to be giving her
[13:36] internal policy information. So I'm
[13:38] going to search the knowledge base for
[13:39] internal policies. And then it realized,
[13:40] oh shoot, I don't actually have Jennifer
[13:42] Martin's email. So it said Jennifer
[13:44] Martin's contact is not found. I should
[13:46] ask the user if they have Jennifer
[13:48] Martin's email address or if they want
[13:49] to provide it before proceeding with
[13:51] sending the email. And so what I've
[13:52] typically found is if it didn't have
[13:54] access to write down its thoughts on
[13:55] this think tool or you didn't explicitly
[13:57] prompt it not to, it would probably just
[13:59] try to send an email and it would make
[14:00] up something like
[14:01] jennifermart@acample.com. So in this
[14:03] case, we avoided that because it used
[14:04] the think tool. And so this is very much
[14:06] just an experimental video to show you
[14:08] guys what this kind of stuff looks like.
[14:09] As you can see, if we were to hook up
[14:10] Claude 3.5 sonnet and try this again,
[14:12] we'll see if it acts the same or if it
[14:14] acts differently. Every model kind of
[14:16] behaves differently as far as the way
[14:18] they interpret a system prompt, but also
[14:19] the way they'll interpret these tools.
[14:21] So here it didn't even use the think
[14:22] tool, which is interesting because in
[14:24] the previous example for a riddle, it
[14:26] was using the think tool more than
[14:27] GPT4.1 was. But in this case, they came
[14:29] to the same conclusion, which is that
[14:31] Jennifer Martin's email address does not
[14:32] exist in the air table record. And then
[14:34] even more interestingly, if we plug in a
[14:36] free model like 2.0 Flash and we send
[14:38] off this same thing, we'll see here that
[14:41] it basically just immediately says,
[14:42] "Sure, what do you want to say in the
[14:43] email?" So, let's just make this query a
[14:44] little better. Okay, so I'm just going
[14:45] to ask it to send an email about our
[14:47] refund policy. So, now it's searching
[14:48] contact database and it said, "I don't
[14:50] see an email from Jennifer Martin in the
[14:51] contacts." So, it didn't use the think
[14:53] tool either. Okay, so just to see how we
[14:55] can maybe affect this behavior, I added
[14:57] a section in the system prompt called
[14:58] instructions where I said call the
[15:00] necessary tools based on the user's
[15:01] request. Then use the think tool to
[15:04] verify you took the right steps. And I'm
[15:06] actually just going to make this capital
[15:07] and I'm going to put this in quotes so
[15:08] it knows that I'm referring to a tool.
[15:10] You probably don't need to, but that's
[15:11] kind of the way I like to do it. And
[15:13] anyways, I'm going to save this and we
[15:14] plugged in claw 3.5 sonnet once again.
[15:17] And we're just going to try to send off
[15:19] this query again. So we'll see that
[15:21] cloud's thinking. It's going to the
[15:22] contact base. It's probably There you
[15:24] go. It hit the think tool right away.
[15:26] Now it's hitting the knowledge base.
[15:27] This is interesting the order of
[15:29] operations here, but we'll just let it
[15:30] finish up and then we'll address it.
[15:32] Anyways, it noticed two issues. The
[15:33] first one is Jennifer Martin's not in
[15:34] the contact database. And then I guess
[15:36] this is not an issue, but it did find
[15:38] refund policy. So, it found that
[15:40] information, but basically just said,
[15:41] "Hey, we don't have Jennifer's email
[15:43] information." Now, I'm curious what will
[15:44] happen if we ask that original query
[15:46] that actually did work. and we'll see if
[15:48] it wants to think about it at the end
[15:49] because we told it to call the think
[15:51] tool in order to verify it took the
[15:53] right steps. So, it scraped the web.
[15:56] It's getting the information. It's going
[15:57] to send an email. And then hopefully it
[15:59] thinks about it. It may not. We may have
[16:01] to get more, you know, specific with the
[16:03] prompting. There you go. It did call the
[16:04] Think tool. And now it says, "I've
[16:06] completed your request. I researched
[16:08] interesting facts about dogs. I found
[16:09] Michael Scott's email address. And then
[16:11] I composed and sent an informative email
[16:13] about dogs to Michael Scott." So if we
[16:15] click into the logs of the agent, we
[16:17] will see that it exactly did that. It
[16:18] called the web search tool. It found the
[16:21] contact information. It sent the email
[16:23] and then finally it thought about it and
[16:24] it said, "Hey, here's what I've done.
[16:26] This is what the user asked me to do.
[16:28] Um, looks like we're good to go. All
[16:30] steps have been completed successfully."
[16:33] So in this case, if it came to the
[16:34] conclusion that, hey, I actually need to
[16:36] do something still. It would probably go
[16:37] back and call those tools. Okay. So you
[16:40] may recognize this workflow from the
[16:41] demo. What it's doing is it's just an
[16:43] agent where I hooked up these two fake
[16:44] tools, a quoter and availability. Here's
[16:46] the quot, here's the availability. We're
[16:48] just hitting an LLM to give sample data.
[16:50] I just wanted to prove the concept of
[16:52] calling tools and then thinking about
[16:53] it. The prompt in here is I just gave an
[16:56] instruction. So I said send the
[16:57] requested dates to the availability
[16:58] tool. Send the requested dates and
[17:00] number of guests to the quoter tool. And
[17:02] then you use the think tool to verify
[17:03] everything has been done. There should
[17:04] be a quote and an availability status.
[17:07] So, what I did in the demo to break this
[17:09] automation is rather than feeding it
[17:10] into the quotot, I just plugged it into
[17:12] this node that basically just says no
[17:15] quote provided. We'll save this. We'll
[17:17] come back into here and ask a question.
[17:18] So, I'm asking if me and three friends
[17:20] can stay from April 15th to 20th. We're
[17:22] going to see it's going to think it
[17:23] realizes that no quote was provided and
[17:25] then it tries to hit the quot again and
[17:27] then we obviously didn't get a quote.
[17:29] And so, if you want to download this
[17:30] template and any of the other ones I've
[17:31] shown earlier for free, you can do so by
[17:33] joining my free school community. Just
[17:35] keep in mind this is mainly just kind of
[17:36] an experiment. So what's going on in
[17:38] here is all I did is I said, "Hey, you
[17:40] know, you're going to get a number of
[17:42] guests and a date and you just need to
[17:43] provide a fake quote." So I said, you
[17:45] know, the quote should be $500 per
[17:46] person per day. And then the exact same
[17:48] thing with the availability tool. All I
[17:50] did is I'm feeding in the query that's
[17:52] going to get passed through. I said, you
[17:54] know, provide availability. Assume all
[17:55] dates are currently available. So like I
[17:57] said, just for testing. And now you can
[17:59] see if I was to go back into the quotot
[18:01] and we take away this edit fields and we
[18:03] plug in the actual quotot. We'll save
[18:05] this. We'll quickly just do that exact
[18:07] same example and we should see that it's
[18:08] going to work. It'll call these two
[18:10] tools. It'll think about it and then
[18:11] we'll say you know we're good and then
[18:13] I'll just quickly walk through how we're
[18:15] actually sending data to these
[18:16] workflows. There you go. It just called
[18:17] the think tool and now we got a response
[18:19] which is here's your quote 12K. But what
[18:21] we're doing is these tools are called
[18:23] call another end workflow. And so when
[18:25] you click into that, all you have to do
[18:27] is you have the ability to link a
[18:29] different workflow from your
[18:30] environment. So as you can see, this one
[18:31] I'm linking to the tool called quotot.
[18:33] And then in this one, I'm linking to the
[18:34] tool called availability, which as you
[18:36] guys know are these two workflows we set
[18:38] up. Here's the quot and then here's the
[18:39] availability. And then what happens is
[18:41] within each of these two subworkflows,
[18:43] they're getting live data sent over. So
[18:45] the example we just did was April 15th
[18:47] through 20th, um, you know, four people
[18:50] total. And so what it sent over was
[18:51] April 15th to 20th. And so actually this
[18:53] is incorrect. should have sent over four
[18:55] total guests as well, but whatever. And
[18:58] then similarly, if we click at the
[18:59] executions of the availability tool, we
[19:01] should see that it sent over the dates
[19:04] once again, which was April 15th to
[19:05] 20th. And then the actual LM responded
[19:08] with the dates are currently available.
[19:10] So, I know I might have went over some
[19:11] of this stuff kind of fast. The main
[19:13] purpose of this video was just to focus
[19:14] on the Think tool, how it works, and how
[19:16] different models may interpret it with
[19:17] prompting. If this whole concept of
[19:19] sending data between workflows was a
[19:20] little confusing, I made a video where I
[19:22] pretty much talked about that the whole
[19:23] time. So, I'll link that right up here.
[19:25] And if you're looking to accelerate your
[19:26] journey with learning nodn and connect
[19:28] with other people who are also doing so,
[19:29] I would definitely recommend checking
[19:30] out the paid community. Like I said,
[19:32] we're growing really fast. We just hit a
[19:33] thousand members and everyone in here is
[19:35] pretty much dedicated towards learning
[19:37] and building AI solutions with n. So,
[19:39] any questions you may have, you'll be
[19:40] able to get them answered in here. But
[19:42] that's going to do it for this video. I
[19:43] hope you guys enjoyed. I hope you
[19:45] learned something new. If you did,
[19:46] please give it a like. Definitely helps
[19:47] me out a ton. And as always, I really
[19:49] appreciate it. And I appreciate you guys
[19:50] making it to the end of this video. I'll
[19:52] see you in the next one. Thanks
[19:53] everyone.