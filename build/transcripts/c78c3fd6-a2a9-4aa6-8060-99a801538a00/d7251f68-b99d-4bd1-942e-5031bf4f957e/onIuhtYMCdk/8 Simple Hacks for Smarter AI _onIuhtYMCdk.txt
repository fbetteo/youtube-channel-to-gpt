Video Title: 8 Simple Hacks for Smarter AI Agents in 8 Mins
Video ID: onIuhtYMCdk
URL: https://www.youtube.com/watch?v=onIuhtYMCdk
View Count: 13,056

[00:00] I've never talked about these settings
[00:01] before, and most people don't even know
[00:02] they exist. But changing just one for
[00:04] your AI agent can help it actually
[00:06] behave the way you want it to. So, in
[00:08] this video, we're going to be breaking
[00:09] down these eight hidden settings that
[00:10] you need to start using when you're
[00:11] building AI agents. Okay, so let's just
[00:13] get right into it. We're looking at
[00:15] these eight different settings that you
[00:16] can actually find within the chat models
[00:18] or the brains for your AI agents. So
[00:20] right here if I click into open router
[00:21] and I click on add option you can see
[00:23] that we have these eight options right
[00:24] here which are frequency penalty maximum
[00:27] number of tokens response format
[00:29] presence penalty sampling temperature
[00:32] timeout max retries and top p. So we're
[00:35] going to break these down and before we
[00:36] start breaking them down I just wanted
[00:37] to highlight something real quick. So
[00:39] right here we're looking at like claw
[00:40] sonnet 4 right and we're doing this
[00:42] through open router no matter which
[00:43] different model we use. So, let's say we
[00:45] go up here to um you know this random
[00:47] one. We still have all those same eight
[00:49] options. And if we go back to like a GPT
[00:51] 4.1, we're still going to have those
[00:53] eight options. But if we were using
[00:54] Anthropics models through the anthropic
[00:56] chat model, we have different options.
[00:58] So, that's just something to keep in
[00:59] mind. Here you can see we only have
[01:00] five. Open AAIs, we have the same eight.
[01:03] Deep Seeks, I think we have the same
[01:04] eight. And Google's we have only five
[01:06] once again. So, I like to use Open
[01:08] Router. I like to keep all of my billing
[01:09] information in one consistent place. I
[01:11] get the analytics there. And now I just
[01:13] have like the same eight settings for
[01:14] all my different chat models. Anyways,
[01:16] just wanted to get that out the way. So,
[01:17] we're going to be talking about open
[01:18] router today. And let's just get started
[01:20] with the first one, which is frequency
[01:21] penalty. And by the way, if you want to
[01:23] download this doc where I have articles
[01:24] about each of these settings and a
[01:26] summary table, when to use them, that
[01:27] sort of stuff. You can get this in my
[01:28] free school community. Link for that
[01:30] will be down in the description. And
[01:31] once you join the free school, just come
[01:32] up here and search for the title of this
[01:34] video and then you'll find that PDF
[01:36] right there. But anyways, the first one
[01:37] is frequency penalty. And what this does
[01:39] is it discourages the model from
[01:41] repeating the same words or phrases. If
[01:43] I click into this and we add frequency
[01:44] penalty, you can see that by default
[01:46] it's at zero, which means that
[01:48] repetition is likely. And the highest
[01:49] you can go up to is two. So if I put
[01:51] three, it would just go to two. But when
[01:53] you're at high or closer to two, it
[01:55] makes the repetition less likely while
[01:57] negative values encourage repetition. So
[01:59] you could even put in something like -2.
[02:01] And now the model is encouraged to
[02:02] repeat words and phrases. But like I
[02:04] said, the default here is just going to
[02:06] be zero. Okay. Okay, so the next one
[02:07] we're looking at is maximum number of
[02:08] tokens. And what this does is it sets
[02:10] the maximum length for the model's
[02:12] response. So that way you can kind of
[02:14] control if you don't want them to write
[02:16] a whole novel for you every time and
[02:17] blow through your tokens and your
[02:18] credits, then you can set what's the
[02:20] maximum that they can actually output.
[02:21] So maybe you have like, you know, you're
[02:23] trying to send a message through
[02:24] Telegram and it only has a,000 token
[02:26] limit in Telegram. Then you can say, oh,
[02:27] okay, this agent needs to only output
[02:29] 1,000 tokens max. And we can just add
[02:31] that option, maximum number of tokens,
[02:33] and put that in right there. And here
[02:34] you can see the default is negative one,
[02:36] which means if it's set here, it
[02:38] basically just has no limit. The actual
[02:40] limit depends on the AI model that we're
[02:41] using. But if we want shorter, more
[02:43] concise answers, we could say, okay, I
[02:44] only want you to output 200 tokens,
[02:46] which really isn't that much. As you can
[02:48] see, a token is roughly four characters
[02:49] or 3/4 of a word. All right, next up, we
[02:52] have response format, which is basically
[02:54] how do we want the AI model to respond.
[02:56] There's only really two options here,
[02:57] which are either text or JSON. And
[03:00] pretty much always, I think you're going
[03:01] to leave this on text. If you want them
[03:02] to output JSON, usually, you know, what
[03:04] you would do is you just come into the
[03:05] AI agent, require a specific output
[03:07] format, and then just connect a JSON
[03:09] kind of structured output parser. I
[03:10] think that's a little bit easier to set
[03:12] up, but it's cool to see that you do
[03:13] have that option in here as a response
[03:16] format. But I guess what's cool about
[03:17] this is that it guarantees the message
[03:19] model comes out in valid JSON. So maybe
[03:21] if your model is sending you a lot of
[03:23] like new lines and it's breaking the
[03:25] body or quotation marks, this may be
[03:27] able to help with that a little bit. All
[03:28] right, the next one is presence penalty.
[03:30] So this basically penalizes the model
[03:33] for using a word that it's already used.
[03:35] So kind of similar to the frequence
[03:37] penalty of like repeating things but a
[03:39] little bit different. So if you want
[03:40] highly varied language for like
[03:42] brainstorming or creative writing, you
[03:44] would basically decrease this presence
[03:46] penalty. So basically this is going to
[03:48] encourage diverse word choices. And
[03:50] what's also cool about this is you don't
[03:52] have to look at this doc. You could also
[03:53] hover over the question mark on each of
[03:55] these options to see what they do. So
[03:56] right here it says positive values
[03:58] penalize new tokens based on whether
[03:59] they appear in the text so far
[04:01] increasing the model's likelihood to
[04:03] talk about new topics. So let's see what
[04:05] the max here would be. If I put in 10,
[04:06] it just goes to two. And if I put in
[04:09] -10, it's probably going to go to -2. So
[04:11] very similar to frequence penalty here.
[04:13] And of course, default will be zero. All
[04:14] right. So the next one is sampling
[04:16] temperature. This controls the
[04:17] randomness in the output. So this is
[04:19] probably the one that you may have heard
[04:20] of before, which is kind of just known
[04:22] as temperature. It ranges from 0 to 1.
[04:25] And like I said, it controls randomness.
[04:27] So if you want responses to be more
[04:28] predictable, you're going to go more
[04:30] towards zero. And if you want them to be
[04:31] a little bit more random and creative,
[04:33] you're going to go towards one. And by
[04:34] default, it is set at 0.7. Because
[04:37] essentially, when the large language
[04:38] model is trying to choose and think
[04:39] about the next word to use, it has like
[04:41] all of these different options and it
[04:43] picks one. And if we decrease the
[04:44] temperature, it basically decreases the
[04:46] amount of words that it generates as in
[04:48] like what should I say next? And if we
[04:50] increase the temperature, it just kind
[04:51] of increases that range of words to
[04:53] choose from next. All right. All right.
[04:54] So, the next one we have is timeout,
[04:56] which sets the maximum time in
[04:57] milliseconds that the system will wait
[04:59] for a response before actually just
[05:00] giving up. So, here by default the
[05:02] timeout is 360,000 milliseconds, which
[05:05] is 360 seconds, which is 6 minutes. So,
[05:08] that's like the default, but you can go
[05:10] ahead and increase that or decrease
[05:12] that. The next one we have is max
[05:13] retries, which sets the number of times
[05:15] that the system is able to actually
[05:17] retry a failed request before giving up.
[05:19] So, it's set to default by two. I'm sure
[05:21] sometimes you wanted it to kind of
[05:22] iterate more than just two times if it's
[05:24] a complex thing. So you could go ahead
[05:26] and raise that. Or you could say, you
[05:27] know, if you fail on the first time, I
[05:28] don't want you to try again. Just tell
[05:30] me that you can't do it. So I'm not sure
[05:32] what the limit is. I went to 100 and it
[05:34] didn't change me. I went to 500 and it
[05:36] didn't change me back. So you can set a
[05:38] maximum number of retries. Just be
[05:40] careful because if the system does
[05:41] decide it needs to retry 500 times, that
[05:44] could add up as far as cost goes. And
[05:46] then finally, what we have down here is
[05:47] top P, which is a little bit confusing
[05:49] because it sounds very similar to
[05:50] temperature. And just as a quick note,
[05:52] it's generally recommended to change
[05:54] either the temperature or the top P, but
[05:56] probably not both. And as you can see
[05:58] right here, this says that it controls
[05:59] the diversity via nucleus sampling. 0.5
[06:02] means half of likelihood weighted
[06:04] options are considered. And we generally
[06:06] recommend altering this or temperature,
[06:07] but not both. So like I said, it feels a
[06:09] little bit redundant with sampling
[06:10] temperature. So I would probably just
[06:12] opt for changing this rather than top P.
[06:15] Okay. All right. So, some of these
[06:16] options are kind of hard to demo and
[06:17] probably pretty boring, like maximum
[06:19] number of tokens or max retries. I think
[06:21] temperature is a pretty cool one to play
[06:22] around with. But let's just go ahead and
[06:24] try something with presence penalty,
[06:25] which if we remember, if we go back to
[06:26] the doc real quick, this one penalizes
[06:28] the model for using any word that's
[06:30] already used. So, higher values are
[06:32] going to be less repetitive, more
[06:33] creative, and lower values will be more
[06:35] repetitive. So, let's just try real
[06:37] quick and go with a negative -2. And
[06:39] let's just ask it to make us a poem. All
[06:41] right. So, I'm asking it to write me a
[06:42] poem about nature. And I told it to make
[06:44] it repetitive wording even though the
[06:45] presence penalty is negative -2. So,
[06:47] it's by nature going to be a little bit
[06:48] more repetitive. But, as you can see, we
[06:50] have again, again, again, again, the
[06:51] sun, again, the sky. Okay, so that's
[06:53] really interesting. Now, let's go in
[06:55] here and make this two, which is going
[06:57] to be the most creative and most random.
[06:59] We'll just go ahead and let me copy
[07:01] this, but reset the session, and we'll
[07:02] do it again. And so, this time, while
[07:04] it's going through, it's going to
[07:05] penalize words that it's already used,
[07:07] and it's not going to repeat them as
[07:09] often. So, this is a much less
[07:10] repetitive poem even though we did tell
[07:12] it to make it repetitive wording. So,
[07:14] like I said, I don't want this video to
[07:15] go too long and I'm not going to demo
[07:16] all of them. Definitely feel free to go
[07:18] grab that PDF. So, you can go ahead and
[07:19] look at how they all work. Um, some
[07:21] sample use cases of them, a nice summary
[07:23] table, stuff like that. But hopefully,
[07:24] if you didn't know about these settings,
[07:25] you now have an idea of, okay, you know,
[07:27] I had an agent where it was repeating
[07:29] stuff and I want to be able to, you
[07:30] know, fine-tune that behavior a little
[07:31] bit. So, definitely get in there, play
[07:33] around with these settings with your AI
[07:34] agents and let me know if it helps. So,
[07:36] that's going to do it for this one.
[07:37] Hopefully you guys enjoyed and learned
[07:38] something new. If you did, please give
[07:40] it a like. Definitely helps me out a
[07:41] ton. And as always, I appreciate you
[07:43] guys making it to the end of the video.
[07:44] I'll see you on the next one.