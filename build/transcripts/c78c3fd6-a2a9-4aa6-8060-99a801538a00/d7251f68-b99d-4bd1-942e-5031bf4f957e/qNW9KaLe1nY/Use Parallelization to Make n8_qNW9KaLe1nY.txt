Video Title: Use Parallelization to Make n8n Workflows Faster & Scalable
Video ID: qNW9KaLe1nY
URL: https://www.youtube.com/watch?v=qNW9KaLe1nY
View Count: 32,991

[00:00] Today I'm going to be talking about an
[00:01] automation technique that I hardly see
[00:03] anyone talking about on YouTube and I've
[00:04] actually never covered it on this
[00:06] channel either. It's a technique called
[00:07] parallelization and it lets you run
[00:09] items at the same time so that your
[00:11] workflow finishes a lot quicker and it's
[00:12] a lot more effective. Understanding
[00:14] parallelization and subworkflows is
[00:16] really going to level up your automation
[00:17] game. So in this video we're going to be
[00:19] talking about what that is. We're going
[00:20] to see some examples of how you can
[00:21] actually set that up. We'll talk about
[00:23] the drawbacks and considerations and
[00:24] we're also going to talk about why and
[00:26] when you would want to set up a
[00:27] subworkflow because parallelization is
[00:29] only one of the many reasons why you may
[00:31] want to cut down a big workflow into
[00:32] some subworkflows. So I don't want to
[00:34] waste any time. I'm just going to run a
[00:35] quick visual demo so that you guys can
[00:37] see how slow this is when you run
[00:39] something sequentially rather than in
[00:41] parallel. So right now we're doing
[00:42] Plexity research on four different tools
[00:44] that are stored in a Google sheet. That
[00:46] happened pretty quick because
[00:47] Perplexity's API can actually do that in
[00:49] parallel. But now this AI agent is going
[00:51] to process each tool and each research
[00:54] one at a time. So right now it's working
[00:55] on the first tool.
[00:58] As you can see, it just finished up the
[00:59] first one and now it's going on to the
[01:00] second one. And we can tell because if I
[01:02] move this stuff down, you can see it's
[01:04] only done one item. Now it's going to do
[01:06] the second one. Now it's going to do the
[01:07] third one and so on. But if we would
[01:09] have had this agent running in parallel,
[01:10] this would have finished up like 20
[01:12] seconds ago already. So there we go. It
[01:14] finally just finished up. And that was
[01:15] only with four items. Imagine if you had
[01:17] 10 or 20. So, we're going to dive into
[01:18] this actual flow right here, and I'm
[01:20] going to turn this into a parallel
[01:21] running workflow. But before we do that,
[01:23] let's real quick hop into an Excel draw
[01:25] so I can explain what parallelization
[01:27] really is. So, what parallelization is,
[01:29] besides a mouthful, is that you're
[01:31] running multiple tasks at the same time
[01:33] instead of one after another. And this
[01:35] really doesn't natively happen within
[01:37] Nen. We get pretty lucky here because
[01:39] Perplexity's API takes all four items
[01:41] and it basically on their server does
[01:44] research about all four at the same
[01:45] time. So, that's great. But not every
[01:47] API does that. We basically just found
[01:49] out that Perplexity does. But as you can
[01:50] see, when we're calling the chat model,
[01:52] it processed these one at a time, and it
[01:54] was very slow. And just in case it
[01:55] hasn't quite stuck yet, pretend that we
[01:57] want to cook these three turkeys and
[01:58] they each will take 30 minutes to cook.
[02:01] But we only have one oven to use. So
[02:03] without parallelization, we'd cook the
[02:05] first one, and then we'd cook the second
[02:06] one, and then we'd cook the third one,
[02:08] and it would take a total of 90 minutes.
[02:10] But if we're able to use
[02:11] parallelization, which means we would
[02:13] have three ovens that can all run at the
[02:15] same time, the whole cook time of these
[02:17] three turkeys would only be 30 minutes,
[02:19] and it's just going to be a lot more
[02:20] efficient. So hopefully that demo shows
[02:22] you guys it's basically a no-brainer in
[02:24] some cases to use parallelization. So
[02:27] let's hop back into Nitn and I'm just
[02:29] going to show you guys how easy it is to
[02:30] get this set up. So, in a recent update
[02:32] of Nadn, they added this cool feature
[02:34] where you can basically select nodes,
[02:36] rightclick, and then say, I want to
[02:38] convert these seven nodes to a
[02:39] subworkflow. So, if you don't see this
[02:41] option, just try to update your instance
[02:42] of NDN, and then you should see it. And
[02:44] what happens is when I click on this
[02:46] button, it's basically going to tell me
[02:47] to name my workflow. I'm just going to
[02:48] keep it as my sub workflow for now. And
[02:50] when I hit confirm, it turns all of that
[02:52] into a subworkflow. And it's already
[02:54] linked up right here and being called by
[02:57] this guy. And so if I click into this,
[02:58] you can see that it's passing in the
[03:00] name of the tool, which is pretty cool.
[03:01] It's already configured. And I can click
[03:03] on this button right here, which will
[03:05] basically take us to that subexecution.
[03:07] And we should see that everything is
[03:08] still set up and should run just fine
[03:10] for us. But we're not done yet because
[03:12] all this is doing so far is passing all
[03:14] four of these tools, which would be
[03:16] Fireflies, Excaladraw, Loom, and Napkin
[03:18] into the subworkflow. And then the sub
[03:20] workflow over here would capture all
[03:22] four and do the exact same thing that we
[03:23] just saw. So what we did is we turned
[03:25] this into a subworkflow which is great
[03:27] but we don't yet have parallelization
[03:29] because it would still run all four
[03:30] through here and it would go 1 2 3 4. So
[03:34] to make this actually run in parallel we
[03:36] want all four items to run as four
[03:38] individual executions and we want them
[03:40] all to run at the same time. So what I
[03:41] have to do is click into the node and
[03:43] the first thing I'm going to do is
[03:44] rather than run once with all items I'm
[03:46] going to choose run once for each item
[03:48] which means it's going to call the sub
[03:50] workflow individually for each item. And
[03:52] then once we've changed that, we have to
[03:53] do one tricky thing where we add an
[03:55] option and we want to turn off wait for
[03:57] subworkflow completion. If we left this
[03:59] checked on, it would basically send the
[04:01] first one and then wait for that one to
[04:03] finish and then send the second one and
[04:04] wait for that one to finish. So that
[04:05] would be the same as basically looping
[04:07] through our list of items. And that
[04:08] would be even slower than the way we're
[04:10] currently doing it with sending all four
[04:12] items over there at once. So now that we
[04:14] have this configured properly, I'm going
[04:16] to go ahead and run this one once again.
[04:18] We're going to go into the sub workflow
[04:19] and click on executions. And you're
[04:21] going to see that all four items are
[04:22] running at the exact same time and
[04:24] they're all going to finish up a lot
[04:25] quicker than the way we previously did
[04:27] it. So we'll go ahead and execute the
[04:28] workflow. It's going to grab our tools.
[04:30] It's going to pass them into the sub
[04:31] workflow right there. I'll click into
[04:33] the sub workflow and click on
[04:34] executions. And you can see right here
[04:36] we now have all four of them running
[04:38] completely in parallel. And they're all
[04:39] going to finish up a lot quicker than if
[04:41] the AI agent was doing an analysis on
[04:43] each of them individually. So you guys
[04:45] can see that all four of these finished
[04:46] up and it took a total of 22 seconds. We
[04:49] can go into our Google sheet and see
[04:50] that we now have our features and our
[04:52] analysis on each of these four tools.
[04:53] And just to show you guys that it really
[04:55] is quicker, if I go back into the main
[04:56] workflow and we click on executions and
[04:58] we click on to one of the ones that ran
[05:00] when it was running all four at the same
[05:02] time, which I believe was this one. This
[05:04] took 51 seconds. So, we cut the time of
[05:07] processing in half, even more than half.
[05:09] And I know it doesn't make a huge
[05:11] difference when we're only running four
[05:12] items, but think about if you started to
[05:14] run 10, 20, 30. You're going to save so
[05:16] much time in that automation. And just
[05:18] remember in the node where you're
[05:19] calling that subworkflow in order for
[05:21] things to run in parallel you have to
[05:23] have it run once for each item and you
[05:25] have to turn off wait for subworkflow
[05:26] completion. Okay, so it was that easy to
[05:28] see how we can set up parallel runs. Now
[05:30] let's quickly talk about drawbacks and
[05:33] considerations of using parallelization.
[05:35] So initially these four came to mind
[05:37] which were resource limits. So, for
[05:38] example, in our flow where we're calling
[05:40] perplexity to do some research, we may
[05:42] have a limit where you can't do more
[05:44] than 20 per minute or something like
[05:46] that. And if you do run into an issue
[05:47] like that, then you do have to consider
[05:49] looping individually through each item
[05:51] or sending over batches of 20 or
[05:53] something like that. You can't process a
[05:55] 100 items all in parallel. Another thing
[05:57] to think about would be error handling.
[05:59] So, if you're running 50 items in
[06:00] parallel and one of them errors, how do
[06:02] you handle just that one that aired? And
[06:04] what do you want to do with the rest of
[06:06] the ones? or if they all errored at the
[06:08] same time. How do you basically cue them
[06:09] back up or process ones that errored?
[06:12] You've also got workflow dependencies.
[06:13] So in this example, it was completely
[06:15] fine. But sometimes if you have a pretty
[06:16] sequential flow where you can't do step
[06:19] C until step B is done, then you do want
[06:21] to have the logic actually make sense so
[06:24] that if there are dependencies, you're
[06:26] still good. And then finally, a little
[06:27] bit similar to number two is
[06:29] troubleshooting and debugging. just
[06:30] because if you have 50 items running as
[06:32] 50 different executions, you will have
[06:34] to sort of be able to find the right
[06:36] executions when you need to make
[06:37] changes. And right now, I'm just trying
[06:39] to throw you guys some highle points. If
[06:41] you want to see this document that I
[06:42] compiled that has way more detail about
[06:44] subworkflows and parallelization and
[06:46] some stuff like this, then definitely
[06:47] check out my free community. I will link
[06:49] that in there in the post associated
[06:51] with this video. In the free community,
[06:52] you can also get this template if you
[06:54] just want to play around with it a bit.
[06:55] And all you have to do is come into
[06:57] here, search for YouTube resources or
[06:59] search for the title of the video up
[07:00] here in the search bar and like let's
[07:02] say this was the video that you're
[07:04] looking at right now. There will be the
[07:06] PDF of this document that I'm talking
[07:07] about and then also JSON file for the
[07:10] workflow. So the link for the free
[07:11] community is down in the description.
[07:13] Okay, so that's drawbacks and
[07:14] considerations of parallelization. And
[07:16] so clearly you don't always want to go
[07:18] straight for parallelization, but it's
[07:20] really important to understand still to
[07:21] use subflows because subworkflows are
[07:23] really valuable. So here are four
[07:25] reasons of why and when to break them up
[07:27] into subworkflows. The first one is
[07:29] repeated logic. One cool thing about
[07:30] nitn is you can build these reusable
[07:32] components. And a reusable component
[07:34] sort of like this one where we're going
[07:35] to pass in something to be researched
[07:37] and then an agent to analyze it. This
[07:39] subflow can be linked up to 100
[07:41] workflows. It doesn't matter. It can be
[07:43] linked up to as many as we want. And so
[07:44] if we know we're creating something that
[07:46] has repeated logic or could be reused,
[07:48] if we package it up as a subworkflow, we
[07:51] can then call it from all these
[07:52] different places. And if we ever need to
[07:54] go in there and change a model or change
[07:56] the HTTP request or change the
[07:58] prompting, we can just do it in one spot
[08:00] rather than having to go change it in a
[08:01] 100 different spots if we link that up
[08:03] to 100 different workflows. We also have
[08:05] workflow complexity. So if your workflow
[08:07] is getting really large and you just
[08:09] want to clean it up sort of based on
[08:10] different modules, it's a really good
[08:12] idea to just split it into a
[08:13] subworkflow, especially with how easy it
[08:15] is now to just highlight something and
[08:17] convert it to a subflow. especially if
[08:19] it's not just you working on a workflow
[08:20] because then you have this whole
[08:22] workflow hygiene thing and everything
[08:23] can be labeled and it will make a lot
[08:25] more sense to someone else who needs to
[08:26] come in there and debug or improve it
[08:28] whatever it is. Obviously we have
[08:30] parallel processing which is what we
[08:31] just talked about. And then we have
[08:33] error handling and recovery. So maybe
[08:35] you can have things always go into a
[08:36] subworkflow if there was an error and
[08:38] then that can just basically handle all
[08:39] the notifications and whatever else you
[08:41] need to do. But it's also way easier to
[08:43] tell which sections of your workflow are
[08:44] erring if you have different components
[08:46] split out into different flows. And then
[08:48] you can just basically understand, okay,
[08:50] you know, this research section of my
[08:52] workflow is failing a lot more than the
[08:53] data manipulation section. And now you
[08:55] know a little bit quicker what you need
[08:56] to work on and what you need to refine.
[08:58] It just helps keep everything split up
[09:00] and separated and really clean and
[09:02] organized. But once again, you can dive
[09:03] a little bit deeper into breaking up
[09:05] workflows in this document that I have
[09:07] prepared for you guys. And just to show
[09:08] you guys one more quick example of how
[09:10] this node works. Let's say we have this
[09:12] flow which kind of looked like this. And
[09:14] we decide, okay, this section right here
[09:15] is going to be a really good reusable
[09:17] component. So we're going to turn that
[09:18] into a subflow, which we did right here.
[09:21] But let's say we're hitting rate limits
[09:22] on our Google sheet node or our
[09:24] perplexity node. So we don't want to
[09:26] process them parallelly, but we still
[09:28] want to keep that subworkflow to clean
[09:29] it up. All we'd have to do is we could
[09:31] come into our main workflow. We could
[09:33] click into here and then we could just
[09:35] turn this on which says wait for
[09:37] subworkflow completion. And this
[09:38] basically does the same thing as if we
[09:40] added a loop over here and we were just
[09:42] processing all four of these items one
[09:44] at a time. So just to show you guys
[09:46] that, I'm going to go in here and I'm
[09:47] going to clear out the data. I'm going
[09:49] to set all of these statuses to ready so
[09:51] that they'll actually be pulled into the
[09:53] automation. And then we'll come back in
[09:54] here. I'll hit save and then I'm going
[09:57] to execute this workflow. It's going to
[09:59] send all four over here. But you're
[10:00] going to see it's going to process these
[10:02] one at a time. So if I go into the
[10:03] actual subflow and I click into
[10:04] executions, you can see what we have is
[10:07] just one of them running right now.
[10:08] That's it. Once this one finishes up, it
[10:11] will then trigger off and fire the
[10:12] second one. So hopefully you guys can
[10:14] see how this is basically turning these
[10:16] into batches of one and just sending
[10:17] them off one at a time. So you can see
[10:19] the first one just finished up and now
[10:20] it's processing the second one. So this
[10:23] is going to take about 20 seconds for
[10:24] each one. So it's going to take a
[10:26] minute, which is a lot longer of an
[10:28] execution than having them all finish up
[10:29] in 20 seconds at the same time. But this
[10:32] is a way that we can protect ourselves
[10:33] from hitting certain rate limits. So if
[10:35] you guys found this video interesting
[10:37] and you want to dive deeper into
[10:38] different automation techniques that are
[10:39] a little bit more advanced, then
[10:40] definitely check out my paid community.
[10:42] The link for that is down in the
[10:43] description. We've got a great community
[10:44] of members who are building with any
[10:46] every single day, always sharing their
[10:47] resources and sharing the challenges
[10:48] that they're running into. We've got two
[10:50] full courses in here. One's called Agent
[10:52] Zero, which is perfect for beginners.
[10:53] It's like the foundations of a
[10:54] automation. And then you move from that
[10:56] into 10 hours to 10 seconds where you
[10:58] learn how to identify, design, and build
[11:00] time-saving automations. So, if any of
[11:02] this intrigues you and you want to be
[11:03] surrounded by over 2,000 like-minded
[11:05] individuals, then definitely check out
[11:07] the community. And I was able to do that
[11:09] entire school plug while this was still
[11:11] running sequentially, which just shows
[11:12] how much quicker it is to do parallel.
[11:14] Anyways, that's going to be it for this
[11:15] one, guys. If you appreciated the video
[11:17] or learned something new, please give it
[11:18] a like. It definitely helps me out a
[11:19] ton. And as always, I appreciate you
[11:21] guys making it to the end of the video.
[11:22] I'll see you on the next one. Thanks
[11:24] guys.