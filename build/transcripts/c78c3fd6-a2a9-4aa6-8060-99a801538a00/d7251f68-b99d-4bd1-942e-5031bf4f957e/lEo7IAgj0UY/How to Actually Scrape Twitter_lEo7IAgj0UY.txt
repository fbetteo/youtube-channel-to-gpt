Video Title: How to Actually Scrape Twitter/X Data with n8n
Video ID: lEo7IAgj0UY
URL: https://www.youtube.com/watch?v=lEo7IAgj0UY
View Count: 39,477

[00:00] today I'm going to be showing you guys
[00:01] this n workflow that I built that helps
[00:02] me scrape unlimited amount of tweets
[00:04] from X there's a ton of good stuff that
[00:06] you can find on X whether it's market
[00:08] analysis competitor analysis or just
[00:10] staying up to date with the latest news
[00:11] and Trends in a specific industry so the
[00:14] system is going to help you do all of
[00:15] that and as always I'm giving away the
[00:16] workflow as well as the Google sheet
[00:18] template for free so all you need to do
[00:20] is download those things plug them in
[00:22] hit run and you'll be scraping Twitter
[00:24] all you have to do to get those
[00:25] resources is join my free school
[00:26] Community the link for that's down in
[00:27] the description but let's not waste any
[00:29] time let's hop into a live demo and then
[00:31] I'll show you guys how to set up this
[00:32] API step by step as you can see here's
[00:35] the workflow that we're going over today
[00:36] quick disclaimer I don't know if this is
[00:39] optimal the way we have these different
[00:40] sets and code noes so if any of you guys
[00:42] are programmers please let me know I can
[00:44] um make this more efficient but anyways
[00:47] it works right so we're scraping X up
[00:49] here we're going to be checking the
[00:51] count and right now I'm just having it
[00:52] only go three runs through um if you
[00:54] wanted to increase that you'd have to
[00:55] change the number here as well as
[00:57] increase the sort of count code right
[00:58] here but if we haven't gone through or
[01:00] three times it's going to come down here
[01:01] we're just going to be setting the
[01:02] variables increasing the count we have
[01:04] to set the page ination and then we're
[01:05] going to loop back through and we're
[01:07] just going to keep scraping Twitter
[01:08] until we've done that three times and
[01:10] then that's the end of the process but
[01:11] as you can see we're updating a Google
[01:13] sheet right here which has these
[01:15] specific columns like the Tweet ID the
[01:16] URL the content likes retweets replies
[01:19] quotes views and the date of the Tweet
[01:22] so we're going to hop back into the
[01:23] workflow I'm going to hit test workflow
[01:24] and we'll see this happen live so right
[01:26] now it's hitting Twitter or X now we're
[01:29] extracting the info adding it to the
[01:30] sheet and then you're going to see it
[01:31] Loop all the way back down here we're
[01:33] once again hitting the API again we're
[01:35] doing the second round of scraping
[01:37] adding that to the sheet as you can see
[01:38] there's 38 total items so there's 38
[01:40] tweets and this is going to be the last
[01:41] run we have 58 tweets and then it's
[01:43] going to go off this way because we are
[01:45] done so that just finished up we can
[01:47] click into our Google sheet and you can
[01:49] see now that we have 58 tweets to look
[01:51] through each of them of course have the
[01:53] URL so if I clicked into this one we
[01:55] could see we have a tweet right here
[01:57] open AI your turn can you give this man
[01:59] a luscious full head of hair and a nice
[02:01] haircut looks great so as you can see if
[02:03] I was to scroll down we would see that
[02:05] we in fact have 58 tweets all of them
[02:07] have an ID along with the links so if we
[02:09] clicked into this one we can see this
[02:11] was on March 11th and it has almost
[02:13] 31,000 views so if we click into it and
[02:15] we wait for it to load it's on March
[02:16] 11th and it has almost 31,000 views so
[02:18] we know that we're actually getting real
[02:20] information into our data sheet and um
[02:23] yeah so let's break down what's going on
[02:25] okay so I told you guys that we were
[02:26] going to walk through setting up that
[02:27] API call step by step so we're going to
[02:29] do that and then I'll walk through this
[02:31] actual execution right here and we'll
[02:33] take a look what was going on all right
[02:34] so this is the API that we're going to
[02:35] be using it's called Twitter api. and
[02:38] I'm sure you guys are wondering about
[02:39] the price it's really not too bad as you
[02:41] can see it's sort of a pay as you go
[02:42] model and you can get a th000 tweets for
[02:45] 15 cents so it's really not too bad also
[02:47] I have a link for this down in the
[02:48] description and if you use that specific
[02:50] link you'll get $6 to start with I think
[02:52] if you sign up normally you only get one
[02:54] so you'll get five free extra dollars to
[02:56] play with anyways this is the API we're
[02:58] going to be using to access the Twitter
[02:59] data I'm going to click on docs which is
[03:01] the API documentation for the different
[03:03] endpoints that we can hit and basically
[03:05] the different functionality of what we
[03:07] can do using this API so let's take a
[03:09] quick glance at the left we have user
[03:11] endpoint actions which would mean we
[03:12] were looking at a specific user wanting
[03:14] to get their tweets their followers
[03:16] their mentions we have tweet endpoints
[03:18] which means that we can um grab an ID of
[03:20] a tweet so over here you see for every
[03:22] tweet we have an ID we could grab you
[03:24] know tweets by ID we could grab their
[03:26] replies their retweeters or what we were
[03:28] doing in the demo was just doing an
[03:29] advanced search where we were searching
[03:31] for tweets based on a query okay so I
[03:33] know that API documentation and setting
[03:35] up these requests can be kind of
[03:36] intimidating going to try to break it
[03:38] down as simple as possible okay the
[03:40] first thing I want to point out is
[03:41] whenever you're looking at API
[03:42] documentation if you see curl commands
[03:44] on the right which would look like this
[03:46] you're going to want to copy that and go
[03:48] into a new workflow type in HTTP request
[03:52] and all you're going to want to do is
[03:53] hit import curl paste that in there and
[03:55] when you click import it's going to
[03:57] populate the fields that you need so
[03:59] it's going to be really handy this one's
[04:01] not too bad because it's a simple get
[04:02] request with pretty much just
[04:03] authorization here but in the case of
[04:06] you know sending over a post request and
[04:08] you have to do a full Json body um and
[04:10] setting up those parameters it's going
[04:11] to be really helpful if you're able to
[04:12] just import that curl and everything set
[04:14] up okay so the first thing that we
[04:16] notice is our method and our URL so if I
[04:18] hop back into the documentation we can
[04:20] see that right here we have a method
[04:22] which is G and then this is sort of the
[04:23] endpoint so if we were to copy this
[04:25] endpoint and come back into here and
[04:27] paste it it would give us that full
[04:29] endpoint so I just pasted exactly what
[04:30] we copied um and basically what happens
[04:32] is we have like sort some sort of Base
[04:34] URL so we're accessing the Twitter api.
[04:37] API and then every single function has a
[04:40] different endpoint so because right now
[04:41] we're doing advanced search that's what
[04:43] it looks like if we were doing um you
[04:45] know get user info the end point would
[04:47] be Twitter
[04:49] userinfo so as you can see all of these
[04:51] are going to have different endpoints
[04:53] which basically just says hey we're
[04:54] reaching out to this server and we want
[04:55] to do something different so then what
[04:57] comes next is going to be authorization
[04:59] and that just means you know you made an
[05:01] account you have an API key you're
[05:03] paying for this search not someone else
[05:05] so right here we can see authorization
[05:07] we have sort of a key value pair the key
[05:10] is going to be x- api-key and then the
[05:12] actual value is going to be your API key
[05:15] and what's important to notice here is
[05:16] that this is a header off sometimes
[05:18] they're query offs sometimes they're
[05:20] headers in this case we have a header
[05:22] and so what you need to do is go to your
[05:23] dashboard in the top right you'll click
[05:25] on your profile your dashboard and then
[05:27] you'll have an API key right there to
[05:28] copy copy that and then we'll bring it
[05:30] into NN so as you remember the key was
[05:32] x- ai- key and the value was your actual
[05:35] API key so this is basically saying this
[05:36] is a placeholder this is where you'll
[05:38] put in your API key now what we can do
[05:40] that's a really cool tip with NN is
[05:42] instead of filling it out here in the
[05:43] header parameters we're going to do this
[05:45] up here under the authentication tab
[05:47] which basically just means we're able to
[05:49] save this authentication and use it for
[05:50] later and this is why you needed to
[05:52] remember that this is going to be a
[05:53] header off so I'm going to click on this
[05:55] button I'm going to click on General
[05:56] credential type and then within the
[05:58] General off type we're going to be
[06:00] choosing header because that's what we
[06:01] saw in the documentation so header and
[06:04] now all we have to do is as you can see
[06:05] mine is already configured but I'm going
[06:06] to pretend like I'm creating a new one
[06:08] we have a key value pair like we talked
[06:10] about so in this case it was x-
[06:13] api-key
[06:15] um and all caps and then for the value
[06:17] you're just going to paste in your API
[06:19] key that you just grabbed from Twitter
[06:21] api. and then you can just basically
[06:23] save this so then you have it forever so
[06:26] I'm just going to call this one Twitter
[06:27] demo we're saving the credential it's
[06:29] connect successfully and now I as you
[06:31] can see I have all these different apis
[06:33] that I already have saved so when I want
[06:34] to set up a request in the future I
[06:36] don't have to go find it put it in here
[06:37] as a header off I just have it saved
[06:39] already so I'm going to flick off send
[06:40] headers because we're sending a header
[06:42] right here and now let's go back to the
[06:44] documentation and see what else we need
[06:45] to configure okay so we're back to the
[06:47] advanced search endpoint we can see that
[06:49] we have two required fields that we need
[06:51] to put in which is going to be a query
[06:53] and a query type so the query is like
[06:56] what we're actually searching Twitter
[06:57] for so in that first example my query
[06:59] was open Ai and I'll show you guys that
[07:01] later but that means it's going to be
[07:02] searching Twitter for open Ai and then
[07:05] we have a query type which basically
[07:06] means you have two options you can
[07:08] either say latest tweets or top tweets
[07:10] so what I did in the demo was top tweets
[07:11] as you can tell they were all um very um
[07:14] very high performing with with the views
[07:16] and the likes but they're still going to
[07:17] be pulling recently so these were all
[07:19] tweets you know still within here's
[07:20] March 8th so that was about a week ago
[07:23] um but mainly they're still going to be
[07:24] pretty recent as you can see okay so
[07:26] anyways for query we have a string and
[07:28] for query type we have a string but but
[07:29] we only have two options so what I'm
[07:31] going to do is I'm going to flick on
[07:32] send query parameters and we know the
[07:34] first one was called query and for this
[07:36] example let's just do Manis because you
[07:38] know that dropped and everyone's talking
[07:39] about it and then we're going to add
[07:41] another one which we know was query type
[07:44] I think with a capital T let's just go
[07:45] make sure query type with a capital T
[07:47] and it has to be either latest or top so
[07:49] for this example let's do latest rather
[07:52] than doing top okay so that's what we
[07:54] have here um and then as you can tell in
[07:56] the demo we have one more option which
[07:58] is cursor
[08:00] um and we're not going to set that up
[08:01] right now but this is basically how you
[08:03] paginate through requests up here it
[08:05] says that um each page returns exactly
[08:08] exactly about that's kind of weird
[08:10] wording but each page returns about 20
[08:12] tweets and if you want more like in the
[08:14] demo we got 58 because we went through
[08:15] three times so we're going to leave that
[08:17] blank for now and we should be set up so
[08:19] I'm going to hit test step we're going
[08:21] to see it's going to be searching now
[08:22] through Twitter and we're going to get
[08:24] one item and if I just move this over a
[08:26] little bit we can see we have one item
[08:29] with
[08:30] this is a total
[08:31] of looks like this only got a 16 total
[08:34] tweets because this is um number 15 and
[08:37] computers start counting from zero but
[08:39] anyways this one got us 16 tweets so I'm
[08:41] just going to pin this data so we don't
[08:43] have to rerun the API we have this to
[08:45] play with and let's just take a look at
[08:46] one of the tweets so here we have the ID
[08:48] of the tweet and the URL let's search
[08:50] Google for this and we'll see that it
[08:51] should be a recent tweet about Manis um
[08:54] let's translate this I can't fetch the
[08:56] translation so let's try another it's
[08:58] from a user name Manis Eric so maybe
[08:59] that's what happened okay maybe let's
[09:01] try something else I'm going to type
[09:03] in college basketball and we'll try this
[09:05] and it's going to ask me if I want to
[09:06] unpin the data yes I do so we can do
[09:08] another run and then we'll see if we
[09:11] just want to validate some tweets okay
[09:12] let's let's go over here and pin this
[09:14] and we will copy this link right here
[09:17] and go to x and see what we got so
[09:19] college basketball betting this one came
[09:21] out 541 which is right now is current
[09:24] time so that's the latest tweet okay so
[09:26] we have a ton of data coming back right
[09:28] and it's all in one item so we want to
[09:29] do is clean this up and extract the
[09:31] fields that we're looking for so I'm
[09:32] going to paste in this code node right
[09:34] here which you can get by joining my
[09:35] free school Community the link for
[09:36] that's in the description you'll click
[09:38] on YouTube resources click on the post
[09:40] associated with this video and I'll have
[09:41] a text file right here for the actual
[09:43] code that's within the code node or of
[09:45] course you could download the workflow
[09:47] um where you download this Json come
[09:49] into nadn hit import from file up here
[09:51] and then you'll have the whole workflow
[09:53] with all the code nodes and everything
[09:55] so of course this is the workflow that
[09:56] you'll actually be downloading and if
[09:57] you want to really understand what's
[09:58] going on with this workflow and the the
[10:00] looping and the setting Fields then I
[10:02] would definitely recommend you join my
[10:03] paid Community the link for that is also
[10:05] down in the description it's really just
[10:06] a more Hands-On approach to learning
[10:08] nadn and having deep discussions about
[10:09] what's going on we have a full classroom
[10:11] about building agents Vector databases
[10:12] apis and HTTP requests as well as
[10:14] step-by-step builds this is definitely
[10:16] not a place for experts only my whole
[10:18] goal of the channel is to make things as
[10:19] simple as possible so um if this sounds
[10:21] like something that you're interested in
[10:22] then definitely hop in here okay anyway
[10:24] so I'm just going to plug in the code
[10:25] node and then it's already configured
[10:27] basically what we're saying is out of
[10:28] this item with it could have 15 tweets
[10:31] it could have 20 tweets every time what
[10:33] we want to do is just basically pull all
[10:35] the objects out and get what we want so
[10:37] actually in this case we have 23 tweets
[10:40] um so this one's different than that
[10:41] first one right and as you can see for
[10:43] each one we've now extracted a tweet ID
[10:45] a URL the actual content the like count
[10:48] the view count and as you can see all of
[10:49] these were just recently posted so
[10:51] they're very low on views except for
[10:52] this one actually kind of went crazy
[10:54] this one was from March 6th so not sure
[10:56] what happened there that was almost 2
[10:57] weeks ago now but um any anyways this is
[11:00] our Twitter data so what's next from
[11:01] here is putting that into a Google sheet
[11:03] so I'm going to grab a Google sheet node
[11:06] all the way down here um we're going to
[11:08] do a pen row and sheet or it's going to
[11:11] be a pen rows but we will choose our
[11:13] actual sheet which is going to be
[11:14] Twitter data we'll choose the sorry the
[11:16] document now we're choosing the sheet
[11:17] which is sheet one and now we have to do
[11:19] is map The Columns so because we were
[11:22] able to extract all of these columns
[11:23] that we want it's going to be super easy
[11:25] it's just as simple as dragging in the
[11:27] values that we need to send over to the
[11:29] colums in our Google sheet so what I'm
[11:31] going to do real quick is I'm going to
[11:32] delete all 58 items over here so we can
[11:35] just start from scratch and now we can
[11:36] see we have to basically tell naden what
[11:38] values am I putting in each of these
[11:40] cells right here so back in the naden
[11:43] workflow I'm going to grab tweet ID from
[11:45] the left from the code node and just
[11:46] drag it into the Tweet ID column I'm
[11:49] going to grab URL drag it into the URL
[11:51] column and just going to do all that all
[11:52] the way down we made it in this order so
[11:54] it's just really intuitive to drag um
[11:57] exactly like I said in this order order
[11:59] so you can also get the template for
[12:01] this Google sheet in the free school
[12:02] Community um just so you can basically
[12:04] plug this thing in right away and get
[12:05] going but we have um pretty much
[12:08] everything one thing also I did in the
[12:10] code node was we formatted the date to
[12:12] look a little more human readable but
[12:13] now we have that done and it's going to
[12:15] be doing that for all 23 items coming
[12:17] through and if I hit play and then we'll
[12:19] go over to here we'll basically just
[12:21] watch all 23 tweets pop into this
[12:24] workflow or sorry the sheet so as you
[12:26] can see there they are we have all of
[12:27] the links are clickable so let's click
[12:29] into this one real quick just to verify
[12:31] there we go we have some um looks like
[12:33] women's college basketball nice bucket
[12:35] there
[12:36] anyways that is pretty much the first
[12:39] step of we scraped we extracted and we
[12:42] put it into a Google sheet so from there
[12:44] I was thinking okay that's cool but we
[12:46] only got 23 items what if we you know
[12:48] want to put this on a scheduled trigger
[12:50] where every morning we are scraping you
[12:51] know AI news and we want every morning
[12:54] to just get like 100 tweets put into a
[12:55] Google sheet what we had to do was look
[12:57] at how the page in nation works so as
[12:59] you remember in the API documentation it
[13:01] says use cursor for pagination if we see
[13:04] the cursor parameter it says the cursor
[13:06] to paginate through the results first
[13:08] page is just basically an empty string
[13:10] this is basically what we just built out
[13:12] these three nodes where we're getting
[13:13] tweets extracting info and adding it to
[13:15] the sheet but now what we have to do is
[13:18] set up a parameter in here that is the
[13:20] cursor and it's not just a simple page
[13:23] ination where it's like page zero page
[13:24] one page two we have to grab a value and
[13:27] so what we're we're grabbing here is the
[13:29] output of the actual tweet extraction or
[13:31] tweet scraping there's a value called
[13:34] Next cursor so item one basically says
[13:37] this was the first page and if you put
[13:38] this page in another request you get
[13:40] page two and then on page two we get a
[13:42] different cursor which basically says
[13:44] okay now you can put this cursor in and
[13:45] you'll get page three and as you can see
[13:47] they basically get longer each time so I
[13:49] think that it's just adding a chunk on
[13:51] each time and saying hey let's get the
[13:52] next page now so we had to bacon some
[13:55] other logic here so I'm just going to
[13:56] break down what's going on in order okay
[13:58] so the first thing that we did was we
[14:00] set a count and basically I came in here
[14:02] and I hardcoded count to equal number
[14:05] one and this is important because we
[14:06] need this number to work off of and this
[14:08] is where I said you know if you're a
[14:10] coder or a programmer and this is not
[14:12] the way to do it let me know but this is
[14:14] how I got it to work so anyways we're
[14:16] setting number one and then in the next
[14:17] node which is the counter we're feeding
[14:19] in the actual count from the previous
[14:21] node as well as the cursor which will
[14:24] come from down here So eventually the
[14:26] count and cursor will both come from
[14:27] down here but to start with on the first
[14:29] first run we're just grabbing count from
[14:31] the previous node which would be one and
[14:32] then we're going to feed that in the
[14:34] rest of the process okay then we move on
[14:35] to the API call where we're going to be
[14:37] scraping Twitter first thing that you
[14:39] notice is it's pretty much the same
[14:40] right here as the step-by-step example
[14:42] we have our endpoint we have our method
[14:44] we have our credential we have our query
[14:46] which was open Ai and we have our query
[14:48] type which was top rather than or then
[14:51] we have our query type which was top so
[14:53] searching for top tweets and then we
[14:55] have our cursor which we didn't have in
[14:56] the step-by-step example what I'm going
[14:58] to show you guys is if we go to to run
[14:59] number one there would have been no
[15:01] cursor being fed in so on the left you
[15:03] can see what was fed in which was
[15:04] counter was one at this point and cursor
[15:07] was null so basically it was saying okay
[15:08] regular request I'm just hitting um
[15:11] looking for top results for open AI then
[15:14] if we go to run two we can see on the
[15:16] left what happened is we now have a
[15:17] cursor and we now have the counter
[15:19] equals two so this is run number two
[15:21] we're feeding in the cursor and we're
[15:22] getting different tweets over here and
[15:24] finally run number three on the left we
[15:26] can see the counter went up to three the
[15:27] cursor is now much longer and we feed
[15:29] that back into the request right away
[15:32] because we're able to always say json.
[15:34] cursor which means we're always looking
[15:35] here and so this is a concept that's
[15:37] kind of hard to explain we're basically
[15:39] looping everything back together because
[15:41] otherwise we'd be referencing some sort
[15:42] of absolute node which it'd be hard to
[15:44] say we want the most recent cursor not
[15:48] the first time we got one so that's why
[15:49] we have to have this counter node which
[15:51] is really important that says okay
[15:53] whatever I'm getting is going to be the
[15:54] most recent count and also the most
[15:56] recent cursor so as you know we were
[15:58] getting big it coming out of the API
[16:00] call and then we have to extract them so
[16:02] exact same code that we used in the step
[16:03] by step we are getting three different
[16:05] Runs run one had 18 Run Two at 20 and
[16:08] run three at 20 which is a total of 58
[16:10] and then we're just adding to them to
[16:11] the Google sheet exact same way we did
[16:13] that earlier except for we're doing it
[16:15] one at a time so 18 first all the way
[16:17] back 20 all the way back then 20 more
[16:20] what's going on over here when we're
[16:21] checking for the count is we basically
[16:23] just have a simple if and we're trying
[16:25] to check if the count is equal to three
[16:26] then we're going to basically end the
[16:28] process and the way we're able to do
[16:29] this is once again we're referencing
[16:31] that counter node which is this one that
[16:33] we're feeding back for the most recent
[16:35] count and cursor so we're able to look
[16:37] here as you can see this one ran three
[16:38] times run one um it was it was false
[16:42] because the counter was one run two it
[16:44] was false because the counter was two
[16:46] and then on run three it finally became
[16:47] true because the counter on this run was
[16:50] three so we're just sending it off to a
[16:52] no operation node which literally just
[16:53] does nothing and if they're false it's
[16:56] going to Loop all the way back down here
[16:57] so the first thing I'm doing here is
[16:58] just setting it to one item only because
[17:02] whatever leaves this branch is going to
[17:03] be either 18 items 20 items however many
[17:05] tweets were pulled back so I just wanted
[17:06] to set it to one to keep things cleaner
[17:09] then what I did was I set the increase
[17:11] so basically I grabbed um the counter
[17:14] from from earlier and which which would
[17:16] be the most recent count so we're
[17:18] setting it back to two but we're setting
[17:19] it dynamically here so that the code
[17:21] node can bump it up by one and then
[17:24] exact same thing with cursor we're
[17:25] grabbing cursor from the G tweets node
[17:28] um in order to feed get back in later so
[17:30] here you can see run one the counter was
[17:31] one um and then later gets bumped up to
[17:33] two on run two what was coming in was
[17:36] two and then it gets bumped up to three
[17:38] and you notice each time the cursor also
[17:39] increased from there it's a code node
[17:41] that obviously disclaimer all of these
[17:43] code nodes I had claw 3.7 right for me
[17:46] so like I said maybe not optimal but
[17:49] it's working we have the counter coming
[17:51] in at two and then we're outputting it
[17:53] called count and it's going up one as
[17:55] you can see so on run one it was coming
[17:57] in as one it came out as two on run two
[17:59] coming in as two came out as three from
[18:03] there all we're doing is we're setting
[18:04] it one more time so I know there's lots
[18:06] of sets going on but we're setting it
[18:08] because we need to be able to pull it in
[18:10] dynamically and always have this node be
[18:12] able to reference it as json. count or
[18:14] json. cursor because remember earlier we
[18:18] passed it in as a hard-coded variable so
[18:19] it needs to be able to say okay I can
[18:21] either look here or here based on
[18:23] whichever one has most recently been
[18:25] activated all right so that's basically
[18:26] it I'm definitely aware that this
[18:28] concept of dynamically setting all these
[18:30] things is a little bit confusing but
[18:32] what I would definitely recommend is You
[18:33] Know download this template run it and
[18:36] just look in there and explore and see
[18:38] how it's actually being able to
[18:39] reference things the key thing to
[18:41] remember here is that when you're
[18:42] referencing a variable like let's let's
[18:44] just go back into here when you're
[18:46] referencing something and you use dollar
[18:47] sign Json it's looking for whatever is
[18:50] the most immediate node coming
[18:52] beforehand and otherwise if you're doing
[18:54] something like right here where in this
[18:56] case we're referencing the counter node
[18:58] or we're referencing the get tweets node
[19:00] that's a lot different because it's like
[19:01] an absolute reference so when we use a
[19:03] dollar sign Json it's just going to give
[19:04] us a lot more flexibility by being able
[19:06] to reference whatever came previously
[19:08] before so we know we're getting the most
[19:10] upto-date information so I hope you guys
[19:12] enjoyed this one I hope you guys learned
[19:13] something new as always if you did
[19:15] please give it a like it definitely
[19:16] helps me out a ton and I always
[19:18] appreciate you guys making it to the end
[19:19] of the videos definitely let me know in
[19:21] the comments what else you guys want to
[19:22] see some other use cases as you know
[19:23] there's a ton more we can do with this
[19:25] Twitter API because we can now that we
[19:28] have the IDS of both the users and the
[19:30] actual tweet we can look up so much
[19:32] other stuff so generating lead list
[19:34] stuff like that but yeah that's going to
[19:35] be it really appreciate you guys once
[19:37] again and I'll see you in the next video
[19:39] thanks