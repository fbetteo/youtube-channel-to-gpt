Video Title: n8n AI Agent Masterclass | AI Nodes Made Simple
Video ID: EzS2PIjyeQQ
URL: https://www.youtube.com/watch?v=EzS2PIjyeQQ
View Count: 30,743

[00:00] all right everyone welcome to the nadn
[00:01] AI agents Master Class I've been
[00:03] building out AI agents in NN for a month
[00:05] now and when I was first getting started
[00:07] I remember being really confused on
[00:09] configuring certain nodes and trying to
[00:11] figure out when to use each one you know
[00:12] I found myself getting stuck in my
[00:14] workflows over the tiniest most specific
[00:16] problems that were hard to find
[00:17] tutorials on so in this video I'm going
[00:19] to cover everything that I wish I'd
[00:21] known from the start the tips the tricks
[00:23] and answers that have taken me hours to
[00:25] figure out so that you don't have to
[00:26] spend that time like I did like I
[00:29] mentioned this stuff can be really
[00:30] confusing at first you've got all these
[00:31] different AI nodes to choose from and
[00:33] then once you click on a node you've got
[00:34] all these different actions within each
[00:36] node and then you have like AI agents
[00:38] and when you click onto an AI agent
[00:39] you've got different types of Agents
[00:40] Within These agents and then you've got
[00:42] different options you've got different
[00:43] settings so by the end of this video all
[00:46] of this will be much clearer to you and
[00:47] you'll be more confident setting up more
[00:49] sophisticated agent workflows if you
[00:52] clicked on this video you probably
[00:53] understand what AI agents are and sort
[00:55] of how they work already but if not I'm
[00:57] just going to go through really really
[00:58] quick what they are and sort of how they
[01:00] work so think of an AI agent as an
[01:03] employee who has Perfect Memory follows
[01:06] exact instructions never sleeps cost a
[01:09] fraction of hiring a human and more so
[01:12] don't want to oversimplify it we'll
[01:13] leave it at this for now how do they
[01:15] work essentially we have an entity so
[01:18] think of an employee the human is the
[01:20] entity and it has the brain and the
[01:23] ability to think and reason and use
[01:25] logic in order to use the tools that you
[01:27] give it access to like Excel LinkedIn
[01:30] Google word slack Google Drive that sort
[01:33] of stuff so the entity is able to think
[01:35] about which tool to use in order to get
[01:36] a specific action done and so don't
[01:39] oversimplify it it's the exact same
[01:40] thing with an AI agent except for the
[01:42] agent is the entity that has the ability
[01:44] to think reason use logic and then it's
[01:47] going to use the tools that you give it
[01:48] access to in order to take action on
[01:51] your behalf so first thing I wanted to
[01:53] talk about is just the basics setting up
[01:55] an AI agent workflow in nadn you can
[01:57] pass information to an AI agent and you
[01:59] can trigger in multiple ways and each
[02:01] one is going to be suited to a specific
[02:03] use case so as you can see in this
[02:04] example we have um you know a Google
[02:06] Sheets trigger we've got a webhook
[02:08] trigger you can chat with it Gmail or
[02:11] execute by other workflow um and
[02:13] obviously this is just five examples
[02:15] there's so many ways you can have agents
[02:16] talk to each other or you can have
[02:18] different triggers for an agent to you
[02:20] know start doing its job but I just
[02:22] wanted to quickly highlight this last
[02:23] one real quick which is execute workflow
[02:25] trigger this is basically saying that
[02:27] this agent will start working when it's
[02:29] called by another agent and um this is
[02:31] super super powerful it lets you stack
[02:33] and chain agents which allows you to
[02:35] create super powerful multi-step
[02:37] processes where each agent can focus on
[02:39] a specific task passing data along the
[02:41] chain so for example a conversational
[02:44] agent could gather information from a
[02:46] user pass it to a planning and execute
[02:48] agent to create an action plan who would
[02:50] then pass it to different tools agents
[02:52] in order to take different actions you
[02:54] know each tools agent specifies or
[02:55] specializes in a different thing and it
[02:57] can be passed around in order to do the
[02:59] job
[03:00] this approach lets you build really
[03:01] sophisticated workflows where agents can
[03:03] interact and collaborate in order to
[03:05] achieve um objectives autonomously and
[03:08] as they're doing so they're going to be
[03:09] learning how to behave and getting
[03:11] smarter at talking to each other and
[03:12] sort of problem solving and real quick I
[03:14] just wanted to plug my free school
[03:16] Community the link for that will be down
[03:17] in the description um couple benefits of
[03:20] this are whenever I make a video on
[03:21] YouTube pretty much I'll upload the
[03:23] workflows on there so you can download
[03:24] the NAD template and plug it right in
[03:26] I'll have my prompts on there any code
[03:28] Snippets I use will be on there as well
[03:30] and then also just being able to have
[03:31] discussions about um you know what's
[03:33] going on in the AI space and if if you
[03:35] want help troubleshooting stuff or you
[03:37] want to collaborate there's um you know
[03:38] a ton of like-minded people in there so
[03:40] definitely feel free to join and I'll
[03:41] see you guys in there okay now we're
[03:43] going to sort of dive into that AI agent
[03:45] node so this is what it's going to look
[03:46] like um within ID end you'll click on it
[03:48] and it will drag into your canvas and
[03:50] then when you open it up to configure
[03:52] the node you're going to see this where
[03:53] you have to choose the type of agent as
[03:55] you can see we've got tools
[03:56] conversational open AI plan and execute
[03:58] and then there are two more below this
[03:59] so this is you know it can be
[04:01] overwhelming you have to figure out what
[04:02] is the goal of this agent which one
[04:03] should I be choosing um all that kind of
[04:05] stuff it can be confusing so that's what
[04:07] we're going to talk about real quick
[04:08] within nadn there are six types of
[04:10] Agents you can choose from we've got a
[04:12] tools agent conversational plan and
[04:14] execute react SQL and then open AI
[04:17] functions and to be honest I've only
[04:19] been really using these main three so
[04:21] we'll dive into these in the most detail
[04:23] but I'll still talk about what these
[04:24] ones do and when you should use them but
[04:27] these are the three go-to that I always
[04:28] use okay so the first one is a tools
[04:30] agent this tools agent is a flexible
[04:33] agent that can use external tools to
[04:34] perform specific actions you can attach
[04:37] different tools like calculators HTTP
[04:39] requests um custom code you can allow it
[04:41] to perform a variety of tasks and you
[04:43] want to use this tools agent when you
[04:45] need an AI agent to access other
[04:47] applications or perform processes that
[04:49] go beyond conversation such as calling
[04:51] apis or doing calculations running
[04:54] specific code or you know calling a
[04:56] different workflow for example if you
[04:58] want an agent that can receive user's
[05:00] input look up information from a web API
[05:02] and then send an email with all those
[05:04] results the tools agent would be perfect
[05:06] for this use case next we have a
[05:08] conversational agent this one is
[05:10] designed for human-like conversations
[05:12] it's going to maintain context over
[05:14] multiple interactions making it ideal
[05:16] for something like a chatbot application
[05:18] um you're going to want to use this one
[05:20] when you want your agent to engage in
[05:22] back and forth conversation with a user
[05:24] like a chatbot that's going to remember
[05:26] what the user said earlier and be able
[05:27] to reference it and it will make more
[05:29] sense then having a tools agent have no
[05:31] idea of what's going on previous context
[05:34] a perfect example of when you would want
[05:35] to use a conversational agent would be
[05:37] something like a customer support
[05:38] chatbot that can answer questions keep
[05:40] track of conversations context and
[05:42] respond based on what the user
[05:43] previously mentioned as if they were
[05:45] talking to a human the third one we have
[05:48] here is a plan and execute agent this
[05:50] agent is going to be ideal for handling
[05:51] complex tasks that need to be broken
[05:53] down into steps it's going to plan out
[05:55] what needs to be done and then execute
[05:57] each step one by one you're going to
[05:59] want to use use this agent when you have
[06:00] a complex task that requires multiple
[06:02] actions or stages or phases to complete
[06:05] and you need each step to be executed in
[06:07] sort of a structured organized sequence
[06:09] great example of when to use a plan and
[06:11] execute agent would be something like an
[06:13] agent that helps with project planning
[06:14] where it's going to break down a task
[06:15] like organizing an event into individual
[06:18] steps and it's going to execute each
[06:19] step in order or send it off to a tools
[06:21] agent to execute that step without
[06:23] needing to remember previous steps so
[06:25] it's was just going to take the plan and
[06:27] execute and then for the other three
[06:29] agents like I said I don't use these
[06:30] ones too often but the first one's a
[06:32] react agent this one is designed for
[06:34] tasks that require reasoning and
[06:36] deciding what actions to take based on
[06:38] logic rather than conversation history
[06:40] so it's good for situations where the
[06:41] agent needs to think through a problem
[06:44] um you could use this when you need an
[06:46] agent to survey to analyze survey
[06:47] responses to decide followup actions
[06:49] something like that then we have the SQL
[06:52] agent this one is a specialized agent
[06:54] that can interface with SQL databases
[06:56] it's going to translate natural human
[06:58] language into to SQL queries which will
[07:01] allow users to interact with it without
[07:03] needing SQL knowledge in order to go get
[07:05] stuff from a database so you obviously
[07:07] want to use this when you want to
[07:09] retrieve or manipulate data in the
[07:10] database using natural language
[07:12] especially if you're not familiar with
[07:14] SQL and then finally we have the open AI
[07:17] functions agent which is one that I
[07:18] think is really cool and I definitely
[07:19] want to start playing around with them
[07:20] more but this one is going to leverage
[07:23] open AI function calling capabilities
[07:25] allowing the agent to select and execute
[07:27] specific tools based on the requirements
[07:29] of the task that you're giving it so
[07:31] this agent is going to be really useful
[07:32] when you need structured outputs from
[07:33] open AI models as it can precisely pick
[07:36] the right tool for you okay so now I
[07:38] wanted to talk about JavaScript
[07:40] variables and functions within NN
[07:42] because this is something that's super
[07:43] important to understand and when you
[07:45] first get in there it may seem sort of
[07:46] intimidating but it's really not so I
[07:48] just want to break this down as simply
[07:50] as I can so in NN we have variables and
[07:53] these are this is just how data is going
[07:54] to flow from one node to the next
[07:56] especially when it's going to change
[07:57] each time based on user input form
[08:00] responses something like that and you're
[08:02] able to reuse these variables and sort
[08:04] of modify them so on the left here we
[08:07] you can see the schema and this is just
[08:08] something that you'll see how um agents
[08:11] are inputting or outputting data we have
[08:14] you know three different labels here
[08:15] we've got the name Andrew Smith email
[08:17] Andrew Smith gmail.com and then we have
[08:18] the age and so the Json over here which
[08:21] just stands for JavaScript object
[08:23] notation it's a very simple way to
[08:25] organize data um for humans and
[08:27] computers to read and it's very very
[08:28] commonly used so it's definitely
[08:30] important just to be able to understand
[08:31] what you're seeing here and it's not
[08:33] that bad it's just key value pairs so
[08:34] we've got the key of name Andrew Smith
[08:36] is the value and same thing over here so
[08:38] as you can see the similarities between
[08:39] the Json and the
[08:41] schema um and obviously this would be
[08:43] changing each time so if you have 15
[08:45] form responses these keys will be the
[08:47] same and the schema over here the keys
[08:49] will be the same but then the obviously
[08:51] the actual values coming through will be
[08:52] different based on which form you're
[08:53] looking at now here's a live look at the
[08:55] actual variables that you'll be seeing
[08:57] within nadn so the example from the last
[08:59] last slide with the schema we have the
[09:00] name andreww Smith and his email and
[09:02] what you would do is you take this right
[09:04] here and you just drag it into a certain
[09:06] field and then you'd see this is the
[09:08] variable with the two curly brackets
[09:10] dollar sign json. name and all that's
[09:12] referring to is the name from the
[09:13] previous node and then you'll see the
[09:15] result come through so this would change
[09:17] obviously like we just said for each
[09:18] form or whatever it is how it's coming
[09:20] through same thing with email json.
[09:22] email we've got email up here so that's
[09:24] how it's being referenced and we have
[09:26] the result and like I said this will
[09:27] change each time and we can reference it
[09:30] later so that's why it's sort of you
[09:31] know that green JavaScript variable and
[09:33] if it's red it just means that it's not
[09:35] valid um it's undefined so if you typed
[09:37] email right here with a capital e um it
[09:40] would not come through it would be R
[09:42] because that's not exactly how it's
[09:43] being referenced right here so
[09:45] definitely wanted to throw that in there
[09:46] cuz I remember one of my first agent
[09:47] builds I was having a problem where it
[09:49] was trying to read in um you know chat
[09:52] input and the i in chat input was
[09:55] supposed to be capitalized and I didn't
[09:56] have it capitalized and it just wasn't
[09:57] working and it couldn't read anything
[09:59] and I was getting pretty flustered but
[10:01] then I just realized okay well it's just
[10:02] not being referenced correctly so make
[10:04] sure that this is sort of being
[10:05] referenced the same way that it comes
[10:07] through okay now just one quick slide on
[10:09] functions and like I said after this
[10:11] we'll get into NN and I'll show some of
[10:12] this actual live and I'll show the
[10:14] dragon drop and it will make more sense
[10:16] but functions are other ways that you
[10:18] can have um things happen within
[10:20] JavaScript so normally it's like json.
[10:23] something if you're calling um you know
[10:25] a v a variable from a previous node but
[10:27] here we have actual functions Within
[10:29] nadn so this is where it could seem a
[10:30] little Cody but it's not too bad so one
[10:33] thing that I was very confused about is
[10:34] how do I get the agent to know the
[10:36] current date and time and it's a super
[10:38] simple function you just do the um two
[10:40] curly brackets dollar sign now and as
[10:42] you can see it's going to display the
[10:44] current date and time right here and
[10:45] then that's obviously not how you know a
[10:47] computer can read this but a human
[10:48] doesn't want to look at this so you can
[10:50] dot format and then this will
[10:51] automatically pop up and you can see it
[10:53] displayed differently so this is like
[10:55] the year the month and the day but you
[10:57] can also edit this and you can change
[10:58] how you want you could have it actually
[10:59] say November you could actually have it
[11:01] you know say the day in a different way
[11:03] so I'll get in N Inn and show you guys
[11:04] that in a sec but then um you can see
[11:08] here's an example of different functions
[11:09] so this one is if and you've got
[11:11] conditions value of true value of false
[11:13] almost similar to like an Excel
[11:15] function but
[11:17] um you really don't have to use these
[11:19] too much because you're able to sort of
[11:22] like use the large language model to
[11:24] determine if you know value of true
[11:26] value of false so you really don't have
[11:27] to look at this it's nice that if you
[11:29] type in two curly brackets it'll pull up
[11:31] um sort of like a list of JavaScript
[11:33] functions you have available to you and
[11:35] it will sort of explain how to use them
[11:36] but to be honest I hardly touch these I
[11:38] really only use like now and I format
[11:41] now so like I said let's hop in nnn and
[11:44] just sort of look at this kind of stuff
[11:46] all right we are now in Ann and I've
[11:47] just set up a super simple agent it
[11:49] really doesn't do anything but just
[11:50] wanted to talk about um the JavaScript
[11:52] variables and how they come in and out
[11:54] of like an agent so in here we can see
[11:56] this workflow would be executed by um a
[11:59] different workflow and this is the PIN
[12:01] data we have which was just in the
[12:02] slides the name email and the age and
[12:05] right now we're in the schema we can go
[12:06] to table and we can see how simple it is
[12:08] um and like I said if you had multiple
[12:10] responses they'd be down here just like
[12:11] a just like an Excel table with you know
[12:13] column names and then the values and
[12:16] then if you want to look at the Json
[12:17] we've got um the name email and age so
[12:20] you can kind of choose how you want to
[12:21] see your data I usually go with schema
[12:23] um because that's how you can actually
[12:24] drag and drop this stuff and I'll show
[12:25] you guys that in a sec but this is the
[12:28] data that's coming through
[12:29] and so then we open up the AI agent and
[12:32] let's say that we wanted to have um The
[12:34] Prompt is what we wanted to give him
[12:36] this information and I'll I'll get more
[12:38] into like what we're looking at in the
[12:39] screen in a sec but for now this is what
[12:41] we want and let's say that we wanted to
[12:43] give this agent information about a
[12:46] client so we said here is the client
[12:50] information and then we'd go um name and
[12:54] all we would do is we'd come over here
[12:55] to name and we' drag in right here so as
[12:58] you can see now that is a
[12:59] um JavaScript variable it turned green
[13:01] json. name it's referencing this and the
[13:03] name is Andrew Smith we can do the same
[13:06] for oops um we can do the same for email
[13:09] and it's simple drag and drop with
[13:10] schema we come in here json. email and
[13:12] we get the email down here and then
[13:14] finally we would be giving it the age
[13:17] right here drag that in it's all green
[13:19] we're all good and as you can see this
[13:20] is the real information coming through
[13:22] and this is just one item so if we had
[13:24] 15 it would be different for each one
[13:26] but these variables would always be the
[13:27] same so that's kind of the point of of
[13:29] the variables okay now let's say we
[13:31] wanted to you know give the agent
[13:33] context of here is the current um you
[13:36] know date and time so we just come in
[13:39] here and if I type in the two curly
[13:40] brackets we'll see the um JavaScript
[13:42] functions available will pop up so
[13:44] obviously this it looks pretty Cody like
[13:46] I said I don't really touch this stuff
[13:47] too much um but I could go to now right
[13:50] here and that's how we're going to get
[13:52] the current date and time and then like
[13:54] I said if you type in do format and just
[13:57] hit enter it's going to automatically do
[13:58] this format for you so you can see it's
[14:00] automatically done y y y- capital m
[14:03] capital M and you get this is how you
[14:06] get it back but let's say we didn't like
[14:07] that um you could come in
[14:09] here and so you can see how it changes
[14:11] so now it's just the year but let's just
[14:13] say if I did capital m capital M um now
[14:17] we're getting no no now we're getting
[14:19] November and um now we're just getting
[14:21] an N you could come in here and do a
[14:23] capital D you're getting the date like
[14:26] that if you go again you'll get
[14:27] something different you go again you'll
[14:28] get that I'm not sure what we get there
[14:30] now we even get the the day of the week
[14:32] so let's say that's how we wanted the
[14:33] agent to be able to understand what day
[14:35] it is we would just do that and then
[14:37] this variable would always stay the same
[14:39] but this would change every single day
[14:41] based on the time um that sort of stuff
[14:43] so that's a really cool one um but then
[14:46] like I said yeah you can just type this
[14:47] in and you'll be able to see all it's
[14:49] available to you you'll see sort of how
[14:50] to configure it but um I don't touch
[14:52] that because let's say we wanted to do
[14:54] an if you know if the person's age is
[14:57] greater than you know 20
[14:59] so we would just say instead of doing a
[15:01] function we could just say do you know
[15:04] whatever it is if you want them to send
[15:05] an email so send an email if
[15:09] [Music]
[15:10] um right here we just drag in if
[15:13] age is greater than 25 and we could
[15:17] leave that value hardcoded or we could
[15:19] have that be a different variable but
[15:20] it'd be coming through as like you know
[15:22] let's actually just change this for the
[15:23] sake of the video we would say oops we
[15:26] would say only send an email if 32 is
[15:31] greater than 25 and obviously we would
[15:33] change it a little bit make it more easy
[15:35] for the agent to actually understand
[15:36] like if um Andrew Smith's email is
[15:38] greater than 25 we would just have to
[15:40] drag in the name and we could sort of
[15:42] configure it right so um if Andrew
[15:48] Smith's oops okay that's inside the
[15:50] variable we just want to make sure it's
[15:51] not inside the JavaScript
[15:53] functions so we could just say like if
[15:55] Andrew Smith's age
[15:59] which is 32 is greater than 25 so I
[16:02] don't know that's just a quick example
[16:03] of how you could do it and why you don't
[16:04] really need to use all these you know
[16:06] Codi JavaScript functions but this now
[16:08] one is definitely very useful and um
[16:11] that's something that I had to figure
[16:12] out okay now let's break down what we're
[16:15] seeing here so obviously this is where
[16:16] you choose the agent like we talked
[16:18] about we have all these these six
[16:19] different types and we talked about when
[16:21] to use each one so right now we'll just
[16:22] stay on a tools agent um and then we
[16:25] have the prompt so you could either
[16:26] Define below like we just did where you
[16:27] talked to it or you could take from
[16:29] previous node automatically which will
[16:31] look for an input field called chat
[16:32] input um and this is you know a lot of
[16:35] times this is when you're actually
[16:37] talking to the agent where you're using
[16:38] chat message so um that's how that would
[16:42] work and it would take like the chat
[16:43] input field from over here json. chat
[16:45] input or you could have it defined below
[16:47] obviously so that's what we just did and
[16:50] this is the prompt for the main
[16:51] instruction or the question that the a
[16:53] agent is going to respond to so this is
[16:55] the direct command that that the agent
[16:58] is expected to look through and act on
[17:00] so that's a little different from like
[17:02] an actual prompt this is kind of like um
[17:05] its instructions for this use case but
[17:07] then if you come down to options you
[17:10] would go to system message and this is
[17:12] where you actually want to give it um
[17:14] the background the instructions how it
[17:17] should behave the context this is where
[17:19] you would set you know the tone
[17:21] personality and um all the context of
[17:24] like how it should act so this is like
[17:26] it's instructions and then this is like
[17:28] the current message so if like for
[17:30] example The Prompt here was you are you
[17:32] know a client onboarding agent and
[17:35] you're going to take information from
[17:37] the forms you're going to um run them
[17:39] through the CRM you're going to email
[17:40] them that sort of thing and then up here
[17:42] we would actually just be feeding in the
[17:43] information about the client so if that
[17:45] makes sense um that's the difference
[17:49] between like this prompt and then the
[17:50] system message and so think of this as
[17:52] like you know giving it the personality
[17:55] and the job description down here so we
[17:58] we've covered that system message
[17:59] there's also other options we can look
[18:00] at so we've got Max iterations um this
[18:03] one is going to control how many times
[18:05] the agent can go back and forth in a
[18:06] conversational Loop so it's going to
[18:08] limit how many turns the agent can take
[18:10] in a conversation you probably want to
[18:12] use this when you want to limit the
[18:14] conversation length especially for use
[18:16] cases where you want to avoid an endless
[18:18] loop so an example of that would be if
[18:20] you're setting it to three it means the
[18:22] agent would only um try to reply up to
[18:25] three times and then we have return
[18:27] immediate steps so if you turn this one
[18:29] on it is going to allow you to see all
[18:31] the steps that the agent took to reach
[18:33] its final answer and it's going to show
[18:34] all of this immediately um after the
[18:37] decision you want to enable this when
[18:39] you want to help you know debug
[18:41] troubleshoot figure out what's going on
[18:43] when you want detailed transparency on
[18:44] how the agent arrived at its answer and
[18:46] you could look at the logs for that and
[18:47] we'll get into logs later but it's
[18:50] helpful to understand the flow of the
[18:51] conversation and it can be really
[18:53] helpful when you want to refine your
[18:54] prompt and you want to see how you can
[18:56] change it to make the agent act the way
[18:58] you want it to and then the final option
[19:00] down here is automatically pass through
[19:02] binary images so when this one is
[19:04] enabled it's going to take any binary
[19:06] image images like image files or um you
[19:09] know PDFs from a previous node and it's
[19:11] going to automatically pass through the
[19:12] AI agent node without being changed so
[19:14] this is going to allow you to keep your
[19:15] image files flowing through your
[19:16] workflow alongside other data without
[19:19] the AI agent trying to process it or
[19:20] interpret it as plain text all right so
[19:23] I haven't yet touched on require
[19:25] specific output format I'll get back to
[19:27] this in one sec but first I just want to
[19:28] want to highlight how you know different
[19:30] agents have different options so that
[19:31] was tools conversational has the same
[19:33] four options as you can see but if we go
[19:35] to a plan and execute agent and we hit
[19:37] add option it's just going to add this
[19:39] it doesn't pull up that list of four
[19:40] options so let's just quickly talk about
[19:42] what a human message template does this
[19:45] is just a way to guide the AI through
[19:47] each step of a complex task so we talk
[19:49] about how a plan execute agent will
[19:50] break down a task into individual steps
[19:52] and then work to execute those sort of
[19:54] in order so this is like a script that
[19:56] keeps the AI focused on what it's
[19:57] already done what it needs to do now and
[20:00] then any notes that um it needs to help
[20:03] remember that process so um obviously
[20:05] we' got previous step this is going to
[20:07] be you know what it already has done and
[20:08] so that it doesn't lose track of you
[20:10] know where it needs to be and what it
[20:12] needs to do next current objective
[20:14] pretty much same thing this is what the
[20:15] AI is focusing on right now and it's
[20:17] going to keep it focused on executing
[20:19] this task without getting distracted by
[20:20] other data or other steps that are
[20:22] coming through so it's just going to be
[20:23] a clear instruction and then the scratch
[20:25] Pad just a space where the AI can sort
[20:27] of jot down any information
[20:29] that it might need later it might need a
[20:30] reference back later um and usually the
[20:33] best case for when you're using a plan
[20:35] and execute agent is to not touch this
[20:37] stuff you can pretty much just leave it
[20:39] as is this was already hardcoded into
[20:41] this agent configuration so I would just
[20:43] leave this as is um I've tried to play
[20:46] around with it before and it kind of
[20:47] just doesn't really add any value and it
[20:49] even makes it mess up more so just leave
[20:51] this as is because it's just going to
[20:52] take what it's already done what it's
[20:54] doing now and then write down notes and
[20:56] stuff like that so that's what this
[20:58] option does does okay and now we have
[21:00] the settings tab and this one is pretty
[21:01] much the same for all different types of
[21:03] Agents conversational tool plan and
[21:05] execute so the first thing we have is
[21:07] always output data this is going to
[21:09] ensure that um the node outputs data
[21:12] even if it didn't execute a specific
[21:14] action so you want to enable this if you
[21:16] want the workflow to keep flowing
[21:17] regardless of whether the AI agent
[21:19] returned a result or not then we have
[21:22] execute once so when this one's turned
[21:24] on it's going to restrict the agent to
[21:25] only running once during the workflow
[21:27] you want to use this to prevent the
[21:29] agent from being triggered multiple
[21:30] times which is helpful for controlling
[21:32] execution in Loops or repetitive tasks
[21:34] stuff like
[21:35] that then we have retry on fail so when
[21:38] you're using this one um it's going to
[21:40] automatically retry running the agent if
[21:42] it fails the first time so you'll use
[21:43] this to enable it for scenarios where
[21:45] you may have like network issues or
[21:47] temporary errors that may cause a
[21:49] failure so it's going to allow your
[21:50] agent to continue retrying before it
[21:52] gives up and obviously you can you can
[21:54] set up how many times you want it to try
[21:56] and um wait between tries and stuff like
[21:59] that so that's how you can kind of be in
[22:01] control of its retrying and then you can
[22:03] set up what you want it to do on air so
[22:05] the default option is that it's just
[22:06] going to stop the workflow um even if
[22:08] there's stuff after the agent it's just
[22:09] going to stop but you can choose to have
[22:11] it continue so if you do this obviously
[22:13] it's going to um just pass an error
[22:16] message through but another cool way to
[22:18] do it is continue using an error output
[22:20] so um this will sort of break off two
[22:22] different branches off of the agent so
[22:24] as you can see there's a success branch
[22:25] and an error Branch so this is cool
[22:27] because if it's working successfully you
[22:30] could have the um agent continue going
[22:32] down this logic over here but if it
[22:34] doesn't work then you could have the
[22:37] agent um you know send some sort of
[22:39] email saying that it didn't work or you
[22:40] could have it um respond to the agent
[22:42] saying you know what the error was so
[22:44] this is a cool way to sort of um have
[22:46] that air handling built in but as you
[22:48] can see if you turn that off in settings
[22:51] just with the stop or continue we'll do
[22:53] continue it's going to take away that
[22:55] branch and so now we only have one
[22:57] option to to go through no matter what
[22:59] so um that is just another cool way to
[23:02] make the agents more powerful and
[23:04] smarter and then back in the settings
[23:05] for this agent we have notes so um I
[23:08] used to think that this was something
[23:09] like within a prompt where you could
[23:11] tell the agent very specific things on
[23:12] how to ask on how to act like I like
[23:14] saying please answer in all caps or
[23:16] something like that but that's not the
[23:18] case this is pretty much just notes for
[23:21] you to remember stuff about this agent
[23:22] or for if you're collabor collaborating
[23:24] with someone on this workflow like if I
[23:26] said um prompt needs refining that's
[23:29] just notes for me and my team to look at
[23:32] and then you could also display the note
[23:33] inlow so then it would just come right
[23:35] here prompt needs refining but if we
[23:37] turn that off then it will just say um
[23:39] like plan an execute agent so that's
[23:41] that's what it is and it has nothing to
[23:43] do with the actual um way that the agent
[23:46] will act or the way that it's in you
[23:48] know taking in data it's just for you to
[23:50] see pretty much okay now the last thing
[23:53] I wanted to touch on is the require
[23:54] specific output format because we didn't
[23:56] talk about this so if I turn this off
[23:58] off um we only have the tool option at
[24:01] least for this agent um a conversational
[24:03] or a tools one would have the memory as
[24:05] well but we just have tools now we turn
[24:07] this back on it's going to say connect
[24:08] an output parser to the canvas to spec
[24:11] specify the output format you require
[24:13] and as you can see now we have an output
[24:14] parser over
[24:16] here um so yeah pretty much this is just
[24:19] going to allow you to control the format
[24:20] of your response that you receive from
[24:21] the AI when it's enabled it's going to
[24:24] have you specify exactly how the output
[24:26] should be structured such as if you
[24:28] require it to be in a particular format
[24:30] you know a particular Json format so
[24:31] that you can pass it through into the
[24:33] next Fields um but it will just let you
[24:36] list or you know specify the structure
[24:38] that you want from the agent when it
[24:40] goes off this way um so if you click an
[24:43] Alpa parser you have three options autof
[24:45] fixing item list and structured output
[24:47] parser and let's just hop back into the
[24:49] slides real quick in order to talk about
[24:51] these things all right so output parsers
[24:54] we very briefly touched on it but I just
[24:56] want to reiterate the two reasons you
[24:57] want to use them for consistency and for
[25:00] reliability so it's going to ensure that
[25:01] your ai's response is in a predictable
[25:03] structure helpful for workflows that
[25:05] need to process the output in a specific
[25:07] way and it's also going to make it
[25:08] easier to handle and pass data to other
[25:10] nodes after your agent in the workflow
[25:13] since you know the format is going to
[25:14] match exactly what you specified so the
[25:16] first one we have here is an autof
[25:17] fixing output parser um this one is
[25:20] going to automatically adjust the output
[25:21] if it's not in the expected format this
[25:23] one will use um some sort of large
[25:25] language model to look at and fix by
[25:27] itself
[25:28] and this is good for when you need the
[25:30] flexibility but you still want reliable
[25:31] formatting because the parser is going
[25:33] to sort of fix the response in to fit
[25:35] the required structure then we have the
[25:38] item list output parser this one is
[25:40] going to separate the output into
[25:41] individual items so you would use this
[25:43] when you're expecting to get a list back
[25:45] of results and you want each item to be
[25:47] handled separately like multiple rows
[25:49] within a spreadsheet and then finally we
[25:52] have the structured output parser which
[25:54] is going to force the output to follow
[25:56] some sort of strict Json structure that
[25:57] you fine in that output parser so for
[26:01] example um like the stuff we were
[26:04] looking at earlier with the name and the
[26:05] email you could have you know you could
[26:08] have the Json put in there in a very
[26:09] specific structure with name email age
[26:12] and then it will always come through
[26:13] from that agent matching those those uh
[26:16] keys to those values if that makes sense
[26:18] okay back in nadn now I just wanted to
[26:20] show what it looks like when you
[26:21] actually attach these different output
[26:22] parsers so um obviously we talked about
[26:24] what these do and so wanted to show this
[26:27] because it may be a little confusing
[26:28] when you add the autof fixing one um you
[26:31] can see that you have to add a chat
[26:32] model obviously so we'll connect this
[26:33] real quick but then also you see that
[26:36] you need to add some sort of output
[26:37] parser so in here once again it opens up
[26:40] the same output parser so um you know we
[26:42] choose from list or structured and this
[26:44] is kind of confusing because it's like
[26:47] why would you add this here if you could
[26:49] just add um you know this structured one
[26:52] right away and so that's like what was
[26:54] going through my mind basically it's
[26:56] going to be more flexible because the
[26:57] autof fixing one is going to correct any
[26:59] small formatting errors with the AI
[27:01] response first of all because the AI
[27:04] occasionally will return data that's
[27:06] almost but not quite in the required
[27:07] format to feed straight into the
[27:09] structured output parser um and so after
[27:12] the autof fixing step the structured
[27:13] parser will confidently enforce the Json
[27:17] format that we had you know specified
[27:19] with less errors than if we had just
[27:20] connected this straight away to the
[27:23] agent and so basically it's just an
[27:25] extra step right here to make sure that
[27:27] the agents respond
[27:28] is being fed in properly because we want
[27:30] it to come through as you know specific
[27:32] Json right here is State and cities and
[27:34] as you can see it would sort of list out
[27:35] the stuff correctly um and if we were to
[27:38] do the item list um this one doesn't
[27:41] even really need to be configured but
[27:42] once again just wanted to sort of lay
[27:45] that out because I was always thinking
[27:48] why would I even add that autof fixing
[27:50] one and have it connect to a large
[27:52] language model if I could just throw in
[27:54] the Json right here but hopefully that
[27:56] made sense and that's why you would do
[27:58] it like that okay now we're going to be
[28:00] doing just a live build of an AI agent
[28:03] in nadn we're just going to build out a
[28:04] really simple one give it access to a
[28:06] few tools and just to show some of the
[28:08] aspects of looking at the agent logs um
[28:11] looking at memory seeing the differences
[28:13] in prompting and how that's going to
[28:14] work so we're just going to do a simple
[28:16] agent here our trigger is going to be on
[28:18] chat message because we will be chatting
[28:20] with this agent in NN just right here in
[28:22] this chat window obviously we talked
[28:24] about some of the triggers you can
[28:25] connect it to um you know your SMS if
[28:27] you wanted to text that you could have
[28:29] it on Gmail or if you want to talk to it
[28:30] through some sort of website or
[28:31] different app obviously you can trigger
[28:32] there but we're doing just chatting with
[28:35] it in NN and we're going to go into the
[28:37] next node and grab an advanced AI
[28:39] obviously a ton of AI nodes we'll talk
[28:41] about these ones later but just wanted
[28:43] to show a quick agent build so we're
[28:45] going to grab an AI agent we're going to
[28:47] leave this one as tools and we will
[28:49] configure this stuff in a sec after we
[28:51] give it access to some other stuff so
[28:53] the first thing obviously is to connect
[28:55] a chat model sort of the brain um it
[28:58] will think about what we said based on
[29:00] this chat model so we'll come in here
[29:03] and click um we'll take 40 and as you
[29:06] can see we got this right here so it'll
[29:08] think about what we say and then it will
[29:09] hit the chat model and then go back to
[29:11] the agent in order to interpret what we
[29:13] asked and to figure out which tools to
[29:15] send it off to so before we add tools um
[29:18] we're going to go with the window buffer
[29:20] memory this is sort of the NN builtin
[29:22] one it's super easy to configure because
[29:24] you don't even have to but we've got
[29:25] other options for memory we've got
[29:27] postgress um you can use all this other
[29:29] stuff but going to use window buffer
[29:31] memory here going to leave it as is
[29:33] leave it as five context window length
[29:36] and basically when we're chatting with
[29:38] it in NN it'll remember five of our past
[29:40] interactions and it can see how it
[29:41] answered and it can see what we're
[29:42] referencing um and I'll I'll show a live
[29:44] example of using this and then I'll show
[29:46] where we take it out and we'll see but
[29:48] that is how that works and it's super
[29:50] easy to configure and we just throw that
[29:51] in there and so for tools we are just
[29:54] going to grab a simple Wikipedia tool
[29:56] right here don't even have to configure
[29:57] that one one and then we'll grab a
[30:00] calculator as well don't have to
[30:02] configure that one and then obviously
[30:03] this is where you would be able to um
[30:06] call some sort of nadn tool so let's say
[30:08] this agent was supposed to be helping
[30:10] send emails we could choose from
[30:11] workflow and I could grab like a send
[30:14] email tool right here so I could
[30:15] configure this one have it set up right
[30:17] here and then when it figures out it
[30:19] needs to send an email it would hit this
[30:20] tool because it's referencing a
[30:22] different workflow that goes through
[30:23] logic to send an email so we'll get rid
[30:25] of this for now but that is just sort of
[30:27] how that could work and super cool
[30:29] stuff so these are just the tools that
[30:32] we're going to give it access to in the
[30:33] model and the
[30:34] memory and now we need to see what goes
[30:37] on in here so we don't need to require a
[30:39] specific output format we'll leave it as
[30:41] a tools agent and I'll switch this later
[30:43] to conversational and we can sort of see
[30:44] the difference but the prompt right now
[30:47] is going to be taken from the previous
[30:49] note automatically it's going to be
[30:50] looking for a field called chat input so
[30:52] once we chat with it um the field of our
[30:56] query whatever we type in will be right
[30:57] here under you know a Json key or schema
[31:01] of um chat input or it'll be json. chat
[31:06] input but I'll show you guys that in a
[31:07] sec so we've got that and then we have
[31:10] um a system message so right now it's
[31:13] just going to say you're helpful
[31:14] assistant we'll save it as that and then
[31:16] we'll ask what is the capital of
[31:21] Illinois it will so it's going to go
[31:24] through here it's going to store it in
[31:25] the memory and then let's see what it
[31:27] said it told us the capital of Illinois
[31:29] is Springfield so now we can go into the
[31:31] log we can look right here actually so
[31:34] the first thing that it did was it
[31:35] received our message and then it updated
[31:37] in the window buffer memory what is the
[31:39] capital of Illinois it ran through its
[31:41] prompt and how to format and then it hit
[31:44] the open AI
[31:46] model um the capital what is the capital
[31:49] of Illinois we can see the input and it
[31:50] actually just answered from this model
[31:52] so that's pretty cool because it didn't
[31:54] even have to reference the Wikipedia
[31:55] node so we'll do one where it has to
[31:57] reference that to show that but um then
[32:00] it finally just updates the response in
[32:02] the window buffer memory so you can see
[32:04] it this is what we asked what's capital
[32:06] Illinois and then it responded the
[32:07] capital of Illinois Springfield so then
[32:09] I could reference it again and just say
[32:10] like what about um
[32:12] Florida and it will know that we're
[32:14] asking about the capital because we just
[32:16] pretty much talked about that so the
[32:17] capital of Florida is Tallahassee if we
[32:19] were to remove this window buffer memory
[32:22] so now it has no context and I said what
[32:25] about um California it's probably not
[32:28] going to give us the capital it's going
[32:30] to say could you please specify what
[32:31] you'd like to know about California see
[32:34] um that's sort of how that works but
[32:36] yeah so we'll add this memory back in
[32:38] here um right here just going to go back
[32:41] in
[32:42] here and now what we should try is um
[32:46] we'll see the difference in a prompt so
[32:47] helpful assistant is good but now let's
[32:50] say you should always
[32:54] respond very friendly and use joke
[32:58] and emojis okay so that's super simple
[33:01] prompt obviously that's not how you
[33:03] prompt I want to make a video on
[33:04] prompting so if that's something you
[33:05] guys interested in please let me know
[33:08] but now we will see if we ask another
[33:10] question so like um what is or like can
[33:14] you um what does the
[33:17] company Nike do it will search Wikipedia
[33:20] for Nike and then it will give us the
[33:22] information and it should do it friendly
[33:24] maybe a joke or two in there with Emojis
[33:26] so Nike is this like the superhero of
[33:28] the sportsware world Emoji they are the
[33:31] largest supplier of athletic shoes
[33:32] apparel globally blah blah blah their
[33:34] product lineup includes various Brands
[33:36] like Nike golf Air Jordan Converse um
[33:38] and at the at the bottom it says so if
[33:40] you're looking for some sporty swag
[33:41] Nike's got you covered so it's friendly
[33:43] it's got emojis we didn't see really a
[33:45] joke yet but we will in the future um
[33:48] but let's look at the log so obviously
[33:50] it updates the window buffer memory with
[33:51] what we said and as you can see it has
[33:53] the past interactions it has capital of
[33:55] Illinois Florida so it knows what we
[33:57] talked about previous L and then it
[33:59] gives the input to the chat model it's
[34:01] going to come hit Wikipedia so the query
[34:04] for Wikipedia was literally just Nike
[34:06] comma Inc so it took that based on what
[34:08] we originally asked it which was what
[34:09] does the company Nike do and so it used
[34:11] the chat model to figure out that it's
[34:12] going to send the query of Nike Inc to
[34:14] Wikipedia we got this big response it
[34:16] pretty much scraped the page of
[34:17] Wikipedia but then the chat model is
[34:19] coming back and it is taking that and
[34:22] then formatting an output that's easier
[34:23] for us to read because it's more concise
[34:25] and that's where it threw in the Emojis
[34:26] and um the sort of joke at the end and
[34:29] then finally it's just going to update
[34:30] the window buffer memory so now we see
[34:32] what we asked and um finally what it
[34:35] answered to us so let's try um we'll try
[34:40] to get the calculator to involve so
[34:43] what what is 12 * um
[34:49] [Music]
[34:51] 52 we'll be able to look at the log of
[34:53] what it did so the result of 12 multiped
[34:55] 52 is 624 math can be so much fun can't
[34:58] it so there's another um the
[35:00] friendliness coming out of the prompting
[35:01] that we gave it so once again it updates
[35:04] the window buffer memory it looks
[35:06] through it it hits the chat model with
[35:08] what we asked which is what is 12 * 52
[35:11] and then it hits the calculator query 12
[35:13] * 52 response 624 it takes that input
[35:16] from the calculator and then makes this
[35:17] output for us and then finally outputs
[35:20] the um response in the window buffer
[35:23] memory so it has all this context of
[35:24] what we've talked about so
[35:26] far so the logs are very important to be
[35:28] able to look through and understand
[35:29] what's going on you can see step by step
[35:31] which is good for you know once you have
[35:33] multiple tools and you need to
[35:34] troubleshoot you know the prompt you
[35:35] need to redefine it so it's taking
[35:37] action in the right step or it's hitting
[35:39] the right tools um sort of as necessary
[35:42] and now let's see if we were to switches
[35:44] to a conversational agent let's just see
[35:46] if it changes anything like that we will
[35:48] come in here and add a system message
[35:49] once again which it has this default one
[35:51] which is huge um assistant is a large
[35:55] language model trained by open AI
[35:57] designed to be able to assist with a
[35:59] wide range of tasks constantly learning
[36:00] improving so this one's going to
[36:01] hopefully give us a humanlike message so
[36:04] I won't even add anything right now
[36:05] about like emojis or anything like that
[36:07] we will just chat with it again to see
[36:09] um any difference so let's do another
[36:11] one where it will talk to us so can you
[36:16] um give us some key
[36:19] highlights from Apple in the past year
[36:24] let's see what it says here so it should
[36:25] be looking through Wikipedia and we
[36:28] didn't even really prompt it in that it
[36:29] has that tool but it should still be
[36:31] able to go through it so it did Apple
[36:34] Inc has been actively announcing new and
[36:35] redesigned products as well as upgrades
[36:37] most announcements take place at
[36:38] high-profile press events that draw
[36:40] significant media attention um so we can
[36:44] take a look at the log obviously it
[36:46] updates hits the chat model to figure
[36:47] out what to do the output is action
[36:50] Wikipedia Apple Inc 2023 highlights and
[36:54] then it search Wikipedia got that big
[36:55] response and then it formed the response
[36:58] with um wow that's a huge chunk but then
[37:01] it gave out the final answer and then
[37:02] this is what we see right over here so
[37:05] that's how that one work works with the
[37:08] log see if there's anything else that we
[37:10] could take a look at in here um I guess
[37:12] you can see right here so right here you
[37:15] know how it says take prompt from
[37:16] previous node automatically um it's
[37:18] looking for an input field called chat
[37:20] input so that's what's right here chat
[37:22] input but we could also just Define
[37:23] below and we could drag that in so it's
[37:25] going to be referencing the same thing
[37:27] which is we asked can you give us some
[37:28] key highlights from Apple in the past
[37:30] year and so this is where you could also
[37:32] um give some more context as to what
[37:34] exactly you're looking for based on the
[37:36] user query that sort of thing um so
[37:41] let's see now let's just add at the
[37:42] bottom of this one
[37:44] to um make sure to
[37:48] always tell a joke at the end so we'll
[37:53] see what that one does with the
[37:54] conversational
[37:55] agent let's ask about
[37:59] um what should we ask about something
[38:02] that it would search Wikipedia for
[38:03] actually no let's just do another
[38:04] calculator thing so what
[38:06] is
[38:10] 2,784
[38:12] 2784 um divided
[38:15] by
[38:17] 16 see what we get here when it hits the
[38:20] calculator tool the result of 2784
[38:23] divided 16 is 174 oh wow is that
[38:26] actually an even number
[38:28] um good example I guess but um yeah I
[38:32] guess this one didn't actually give us a
[38:34] joke probably because of it has this
[38:36] huge other prompt
[38:38] but yeah that's just sort of the
[38:40] difference between um using these two
[38:42] and um we saw what happens if you take
[38:44] away the window buffer memory we saw the
[38:47] way you could look through logs in order
[38:48] to see what takes place and you can also
[38:51] in real time um see it happening so let
[38:53] me just show that real quick what is um
[38:57] Nvidia so we hit that and we can see
[39:00] it's hitting the brain it's up it
[39:01] updated this and now oh it actually
[39:04] answered just based on the model it
[39:05] didn't even go to Wikipedia um but this
[39:07] is the result we get so it pretty much
[39:09] gives us key areas where Nvidia excels
[39:12] um stuff like that and once again you
[39:14] can look through the log so we can see
[39:15] that it didn't even hit Wikipedia it
[39:17] would have gone through here then hit
[39:18] Wikipedia then came up so let's just do
[39:20] an example of that actually um can you
[39:24] provide information about Honda
[39:28] uh the car company so this one probably
[39:31] will go to um Wikipedia we'll see it
[39:34] hits the window bu for memory goes here
[39:36] and then it knows to search Wikipedia to
[39:37] get that information and now it's
[39:39] turning that information from Wikipedia
[39:41] into a response that we can digest
[39:43] easier so here's some information about
[39:45] Honda key highlights stuff like that we
[39:47] could obviously prompt it to make this
[39:49] more concise or to use emojis and jokes
[39:51] and stuff like that but hopefully that
[39:53] gives you a good understanding of you
[39:54] know the actual interfacing of agents
[39:56] and how they they work as far as their
[39:58] process so the next thing to do from
[40:00] here is we want to talk about um the
[40:02] other AI nodes so once you come in here
[40:04] and you click Advance AI you've got all
[40:06] this stuff obviously so we've kind of
[40:08] talked about the agents um we've seen
[40:09] some stuff with open AI but we'll talk
[40:10] about it real quick and then we have all
[40:12] this other stuff and then we have other
[40:14] AI nodes in here too so this is where
[40:16] back at the beginning I was talking
[40:17] about you don't always know when to use
[40:19] which one and it can be overwhelming so
[40:22] um we're going to hop back into the
[40:23] slides real quick and just sort of go
[40:24] over when you would want to actually use
[40:26] each of these a nodes all right now
[40:28] we're going to look at these seven other
[40:30] AI nodes in nadn open AI is the one that
[40:33] you'll probably use the most often
[40:34] that's the one that I use most often
[40:36] usually just to message a model but
[40:37] we'll sort of break that down and then
[40:39] we've got llm chain information
[40:41] extractor question and answer chain
[40:43] sentiment analysis summarization chain
[40:46] and then finally text classifier so
[40:48] let's hop into um this first one here
[40:51] which is going to be open AI so what
[40:54] this one does is it connects to open AI
[40:56] models like chat gbt um so that's where
[40:58] you can choose you know 40 4 mini 3.5
[41:01] turbo whatever you want that's where you
[41:03] can choose that and you can use this to
[41:05] generate text um you can talk to a model
[41:07] like chat gbt you can analyze images you
[41:10] can create audio or more based on the
[41:13] sort of the function you choose and the
[41:14] way that you instruct the node to work
[41:17] you're going to want to use this one
[41:18] when you need to interact with a general
[41:20] purpose AI assistant ask questions
[41:22] generate texts um create summaries or
[41:25] even analyze content from images and you
[41:28] know something like the the
[41:29] summarization um chain can also
[41:32] summarize but sometimes you need to have
[41:33] a multi-step thing within this node
[41:35] that's going to summarize and then
[41:36] extract certain things or summarize and
[41:38] then create new things so that's sort of
[41:40] why you'd want to use this it's very
[41:42] powerful so something like creating a
[41:44] customer support bot that can answer
[41:45] questions um based on like a summary of
[41:49] a huge report or something like that and
[41:52] then we have the basic llm chain so this
[41:54] one's going to provide a very simple
[41:56] streamlined way to send a prompt to a
[41:58] large language model llm and get a
[42:00] response so you want to use this when
[42:02] you're doing something very
[42:03] straightforward where you need an answer
[42:05] or output from an AI without complex
[42:08] configurations so if you're asking AI
[42:10] for a quick response to a user's
[42:11] question like what are the best
[42:13] practices for email marketing it would
[42:15] hit that llm and then answer you right
[42:18] away so then we have information
[42:20] extractor this one's going to pull
[42:21] specific pieces of information from a
[42:23] block of text that's usually organized
[42:25] in a structured format so you want to
[42:28] use this when you need to extract
[42:29] details from um text such as pulling out
[42:32] names dates locations um you know from
[42:35] emails or messages or documents
[42:38] something like extracting customer
[42:40] details from a email to pass it into
[42:42] your CRM
[42:46] automatically Okay so we've got four
[42:48] more these two question and answer chain
[42:50] tensent analysis for the first one Q&A
[42:52] chain you're going to use this um when
[42:55] you want AI to answer questions based on
[42:57] specific documents or you know content
[42:59] you provide so like a knowledge base
[43:01] rather than general llm internet
[43:03] knowledge you want to use it when you
[43:05] want to limit the ai's answers to
[43:07] specific documents um and a good example
[43:09] would be a Q&A assistant for employees
[43:11] that are um getting questions answered
[43:14] based on the company's policy documents
[43:16] or specific training
[43:18] material then we have sentiment analysis
[43:21] this one can analyze a motion or tone of
[43:23] a piece of text and it can categorize it
[43:25] as like positive negative or neutral so
[43:29] you want to use this to understand the
[43:30] mood or attitude behind um like a
[43:32] customer feedback or certain social
[43:34] media posts or reviews about your
[43:37] product or something like that um you
[43:40] know it's going to automatically analyze
[43:41] those emails and then sort of give you
[43:43] the tone that was it was written in sort
[43:45] of
[43:46] thing so then we have summarization
[43:49] chain um what this one does is it's
[43:51] going to condense long pieces of text
[43:53] into shorter summaries and it's going to
[43:54] capture those main points and highlights
[43:56] for you um you can use this to create
[43:58] summaries of lengthy documents obviously
[44:00] making them easier to read or pass
[44:02] through um quickly and then you can you
[44:05] know summarize meeting transcripts or
[44:08] feedback documents or policy documents
[44:10] in order to make sure that you're
[44:11] getting all the the key information from
[44:12] it and then finally we have the text
[44:15] classifier which categorizes pieces of
[44:18] text or a piece of text into predefined
[44:20] categories based on the content of what
[44:22] it's reading it's going to automatically
[44:24] label or classify texts such as sorting
[44:26] email into different departments or
[44:28] tagging um different types of customers
[44:31] or tagging different types of feedback
[44:33] um an example would be you know
[44:34] classifying incoming support tickets
[44:37] into categories you could have like
[44:39] billing tickets technical issues
[44:41] customer support General General inquiry
[44:43] and then you can route them based on
[44:45] this classifier into different logic of
[44:48] you know where it could get sent around
[44:50] so all these nodes obviously very
[44:52] powerful um it's important to understand
[44:54] when is the best use case for each one
[44:56] but they're all just ways that you can
[44:58] add um large language models you know
[45:00] artificial intelligence into your
[45:01] workflows in order to make them even
[45:03] more powerful than you know static
[45:05] automations all right we're sort of
[45:07] starting to wrap up the master class we
[45:09] just talked about these other Advanced
[45:10] AI nodes right here besides the agent
[45:12] we've got these ones that we just talked
[45:13] about in those slides and finally just
[45:15] wanted to end off by talking about these
[45:17] other AI nodes in here because once
[45:19] again it's a little overwhelming so
[45:22] first off I want to say that um in my n
[45:24] andn Master Class video If you guys
[45:26] haven't seen that I talk more in depth
[45:28] about the document loaders as well as um
[45:31] the text Splitters that are down here
[45:32] and then also sort of vector stores so
[45:35] for more in depth I would go watch those
[45:37] videos because we sort of get a workflow
[45:38] where we talk about what's going on in
[45:40] each of these sort of AI um steps but
[45:44] first one we have document loader it's
[45:46] just going to bring documents into the
[45:48] workflow for processing like um loading
[45:50] PDFs or text files and like I said if
[45:53] you need more information um this is
[45:55] pretty much how it looks and then you
[45:56] add the text splitter which are you know
[45:58] down here and I talk about these more in
[46:01] depth in the nadn master class video
[46:04] then we have language models so as you
[46:06] can see these are the different models
[46:07] that you can choose from um I pretty
[46:09] much use open AI the most of the time
[46:11] but these are just models I can
[46:12] understand generate language and they're
[46:14] used for tasks like answering questions
[46:15] or creating text when you want to talk
[46:17] to something like you know GPT or
[46:20] claw and then we have memory we've
[46:22] talked about this again um you've got
[46:24] the option for window buffery which we
[46:26] have right here or sorry window buffer
[46:28] memory we have right here and then these
[46:29] other options basically just storing
[46:31] information temporarily during a
[46:33] workflow so the AI can remember um these
[46:35] details to reference later then we have
[46:38] output parsers we talked about these
[46:40] earlier too um we have retrievers which
[46:43] search and pull relevant information
[46:45] from a specified Source um and this is
[46:47] just so that they can sort of assist in
[46:49] answering questions and finding details
[46:51] so we've got different options here um
[46:52] Vector store seems to be the one that
[46:54] I've used the most but you've got
[46:56] different options as um of course
[46:58] because there's just so many to choose
[47:00] from and then after retrievers we have
[47:02] text Splitters we talked about those um
[47:05] tools this is where you can either call
[47:07] n andn workflow or there's a ton of
[47:09] Integrations as you can see um and these
[47:10] are just going to give you um you know
[47:13] actions Within These popular systems
[47:14] that you probably use every
[47:16] day then we have our embeddings this is
[47:19] just going to convert text into a vector
[47:22] um format which helps tasks like um you
[47:25] know Rag and semantic search finding
[47:27] similar content so if you don't
[47:29] understand these embeddings I would also
[47:30] maybe go watch that NN masterclass video
[47:32] we talk about you know pushing data into
[47:35] a pine cone Vector database and it's
[47:37] going to push it into a document loader
[47:38] to load the data split it up embed it
[47:40] into the um Vector database uh in the
[47:44] vector stores down here which you can
[47:46] see Vector stores are going to store and
[47:48] retrieve um these Vector sort of
[47:50] numerical multi-dimensional repres
[47:53] representations of data or text and
[47:55] making it really easy to def find for
[47:57] the agent based on you know related
[48:00] content sort of based on the meaning of
[48:01] what's going on so these are the vector
[48:03] store options and then finally we've got
[48:05] miscellaneous which I don't even really
[48:07] knows in here it's open Ai and then chat
[48:10] memory manager so within the window
[48:11] buffer memory you have the option to get
[48:13] messages insert messages delete messages
[48:16] so that is sort of what's going on here
[48:18] um a lot of the times you won't be
[48:20] really coming in here to grab these
[48:21] nodes they'll be you know you'll be
[48:23] grabbing them from something else like
[48:25] you know when you need to text split or
[48:27] when you need to sort of load data stuff
[48:29] like that so I hope hopefully that at
[48:32] least gave you some general context on
[48:33] this and this because when you click
[48:35] into AI noes obviously um there's a ton
[48:37] that's going on in here so yeah I hope
[48:40] that that made sense so that is going to
[48:42] be it for this master class where we
[48:43] talked about a lot of the aspects of AI
[48:45] agents specifically within NN so let's
[48:47] just quickly go over what we covered we
[48:49] started with the basics of AI agents we
[48:51] explored how these agents can automate
[48:52] tasks we talked about their triggers we
[48:54] learned about setting up the workflow we
[48:56] took a deep dive into each of the agents
[48:58] because when you open up an agent you've
[48:59] got a tools conversational plan and
[49:01] execute um and more and each have sort
[49:04] of unique strengths and use cases so we
[49:05] talked about what they each do and when
[49:07] you would want to use each one then we
[49:10] talked about JavaScript functions and
[49:11] variables which are super important when
[49:13] you want to build agents or even just
[49:14] build workflows in Ann um they help make
[49:17] our workflows more Dynamic and
[49:18] customizable then we talked about some
[49:20] key settings within the agents like
[49:22] prompts and system messages and you know
[49:25] the other options and settings and how
[49:26] we want the agents to interact we talked
[49:28] about some output parsers to ensure that
[49:30] responses are structured in the way that
[49:31] we need them and then finally we just
[49:33] dove into some of the other AI nodes
[49:34] that are available within naden to
[49:37] extend the power of your workflows stuff
[49:39] like document loaders and beddings and
[49:41] um you know text classifiers sens
[49:43] analysis that sort of stuff so at this
[49:46] point um if you're excited about
[49:47] building AI agents within nnn there's so
[49:50] much more to explore feel free to check
[49:52] out other videos on my channel where I
[49:54] dive into you know building Advanced Air
[49:56] agent and different simple workflows and
[49:58] end in end step byep or even live builds
[50:01] um you know some more real life examples
[50:03] and more in-depth looks at each of the
[50:05] tools so um finally I just wanted to say
[50:09] um feel free to join the school
[50:10] Community there's you know it's growing
[50:11] really fast and a lot of people are in
[50:12] there you know talking about n Talk NAD
[50:15] talking about other ways to you know
[50:17] improve workflows or getting help on
[50:18] certain workflows and I would love to
[50:20] just you know see all you guys in there
[50:22] cuz it's really nice to interact and you
[50:23] know see what people are working on see
[50:25] what people want to see from future
[50:27] videos um but that's really all I've got
[50:29] for today um if you made it this far
[50:32] really appreciate you guys taking the
[50:33] time out of your day to watch this video
[50:34] and I would really love some feedback
[50:36] cuz I hope these sort of longer form
[50:38] ones are um educational and at least you
[50:40] get a few things learned out of the
[50:42] videos but again I really appreciate all
[50:44] the support lately and um thank you so
[50:46] much for watching I'll see you guys in
[50:48] the next video