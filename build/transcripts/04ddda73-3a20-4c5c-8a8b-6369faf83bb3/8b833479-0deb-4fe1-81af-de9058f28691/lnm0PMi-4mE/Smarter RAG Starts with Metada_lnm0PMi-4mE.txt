Video Title: Smarter RAG Starts with Metadata (Beginner's Guide)
Video ID: lnm0PMi-4mE
URL: https://www.youtube.com/watch?v=lnm0PMi-4mE
View Count: 25,496

[00:00] Today I'm going to be talking about the
[00:01] different ways you can use metadata to
[00:02] make your rag agents much more
[00:04] intelligent. Not only does metadata let
[00:06] us organize and filter through our data,
[00:08] but it also lets us enrich it so that we
[00:10] can get way more context as to what
[00:12] chunks we're looking at. If it doesn't
[00:13] make too much sense to you yet, don't
[00:15] worry. We're going to break it all down.
[00:16] I'm going to run through some live
[00:17] examples and by the end there'll
[00:19] definitely be thinking about how you can
[00:20] apply metadata to your own use case. All
[00:22] right, so I don't want to waste any
[00:23] time. We're going to start off with a
[00:24] quick demo so I can show you guys what
[00:26] this metadata stuff looks like by
[00:27] talking to our YouTube transcript rag
[00:29] agent. So I'm going to send off this
[00:31] message that says, "What's the
[00:32] difference between a relational database
[00:33] and a vector database?" Our agent is
[00:35] going to search through Superbase. It's
[00:37] going to use a reranker to sort those
[00:39] results. And now we're going to get our
[00:40] answer based on a YouTube transcript.
[00:43] And you can see, not only was our agent
[00:44] able to search through and answer for
[00:46] us, it also gave us the exact YouTube
[00:48] video that it pulled this data from, the
[00:50] exact timestamp that it pulled the data
[00:52] from, and then we can also go right here
[00:54] and click into the link if we want to
[00:56] watch the full video.
[00:58] >> Imagine having a team of
[00:59] >> And the only reason it was able to give
[01:00] us this extra context is because we
[01:03] enriched the chunks with this
[01:05] information in the metadata. So that's
[01:06] the use case that we're going over in
[01:07] this video. But metadata essentially
[01:09] just means data about data. So, whatever
[01:11] you're loading in, you could enrich it
[01:12] with metadata like the date or the
[01:14] department or the author or the file
[01:16] name or the file size, whatever you
[01:18] want. Before I break down this pipeline
[01:20] where we're getting the transcript data,
[01:21] preparing it, standardizing it, and then
[01:23] enriching it with metadata to vectorize
[01:25] it, let's hop into a quick Excal so I
[01:26] can explain real quick why metadata is
[01:29] important and how it works with vector
[01:30] search. If you've been following my
[01:31] channel for a while now, you've probably
[01:33] seen this exact Excal visualization, but
[01:35] either way, let's run through it real
[01:36] quick. And I'm also going to keep this
[01:38] example relevant to the one that we'll
[01:39] be running through an Nitn just to keep
[01:41] everything consistent. So what we start
[01:42] off with over here is our transcript.
[01:44] And then what happens is we have to
[01:45] chunk it up and run it through an
[01:47] embeddings model in order to get turned
[01:48] into different vectors. And each of
[01:50] these dots or vector points is basically
[01:52] associated with one chunk of the
[01:54] transcript. So transcript could have 20
[01:56] chunks, 30 chunks, 50 chunks just
[01:58] depending on how long the video is. And
[02:00] so the issue here is without metadata,
[02:02] when we're looking back at this specific
[02:04] chunk, we actually don't know which
[02:06] YouTube video it came from. So if we put
[02:08] three YouTube video transcripts in our
[02:10] vector database, transcript A,
[02:12] transcript B, and transcript C, with
[02:14] metadata, we can give each chunk extra
[02:16] information like the title of the full
[02:18] video it came from, or the URL of the
[02:20] full video it came from, or the
[02:22] timestamp of this specific chunk in the
[02:25] transcript. And without this type of
[02:26] metadata, we would have no idea the type
[02:28] of insights we're getting back. And we
[02:30] honestly wouldn't have that much trust
[02:31] when our agent responds to us because it
[02:33] can't back it up with an actual source.
[02:35] And then one more thing to keep in mind
[02:37] is the metadata is just data about data.
[02:40] So the metadata has no effect on the
[02:43] actual meaning of the chunk. It has no
[02:45] effect on where the chunk gets placed in
[02:47] the vector database. So when we're
[02:48] searching through our vector database,
[02:50] the query gets embedded. It gets put
[02:51] into the vector database. and then the
[02:53] nearest chunk or nearest couple chunks,
[02:54] however you set it up, get pulled back
[02:57] just based on the actual transcript
[02:59] meaning. But then once we get the
[03:01] transcript back and we realize that it's
[03:03] relevant to the query, then we'll pull
[03:04] in the metadata and look at it for more
[03:06] context. And then of course you could do
[03:08] things like metadata filtering where you
[03:10] could say, okay, you know, I have
[03:11] transcript A, B, and C in my vector
[03:12] database, but for this specific query, I
[03:15] only want to look at transcript C
[03:16] because I know that's the video that I
[03:18] want some insights from. So the three
[03:20] big benefits of having metadata for your
[03:23] vector database rag is that you can get
[03:25] more context when you're pulling stuff
[03:27] back. You can keep your data a little
[03:29] more organized and segmented and then of
[03:31] course you can filter using metadata to
[03:33] get only what you want back. So now that
[03:35] that's out of the way, let's hop into
[03:36] Naden and we'll take a look at these
[03:38] different pipelines and how the metadata
[03:39] is actually working. Okay, so we just
[03:41] saw the chat functionality where we're
[03:43] talking with Subbase. Now let's look at
[03:45] the actual transcripts pipeline of how
[03:47] we're getting data into our Superbase
[03:50] vector store with metadata. So before we
[03:52] run this pipeline, let me just real
[03:54] quick go into Subbase to show you guys
[03:56] the transcript that's currently in here.
[03:57] This is this YouTube video that you guys
[03:59] saw in the demo. But as you can see each
[04:02] of these different chunks, we have the
[04:03] content which is just text. And then in
[04:05] the metadata of each one, we have the
[04:08] timestamp of where this chunk came from.
[04:10] We have the video title and then we have
[04:12] the video URL. So that's exactly what
[04:14] happens to every single YouTube video
[04:15] that we process into our database. Okay,
[04:18] so I have this set up to basically
[04:19] trigger on a form submission if we want
[04:21] to put a video into our database. So I'm
[04:24] going to hit execute workflow. It's
[04:25] going to pull up this form that prompts
[04:26] us to drop in a video title and a URL.
[04:29] So I'm going to drop in this video from
[04:30] Anthropic where they're sitting on a
[04:32] couch and they're talking about tips for
[04:34] building effective AI agents. What
[04:35] happens first is that we're going to
[04:37] Ampify in order to scrape the YouTube
[04:38] transcript. We're going to put together
[04:40] the full YouTube transcript. We're going
[04:42] to grab the timestamps, merge it all
[04:43] together, and then that's actually what
[04:45] gets vectorized, and then it also
[04:46] updates us in our Google sheet. So, we
[04:48] can come in here real quick and see
[04:50] which videos currently exist in our
[04:51] vector database. And now, let's dive
[04:53] into each of these nodes so you guys can
[04:54] see what's going on. The first one is
[04:56] our HTTP request to Ampify to scrape the
[04:59] transcript. It's a really simple
[05:00] request. We're basically just giving it
[05:02] the video URL, which came right here
[05:04] from the actual form submission. And
[05:06] then what it gives us is this kind of
[05:08] weird item where we have like hundreds
[05:11] of objects in here and each object has a
[05:14] start time, a duration, and then text.
[05:16] And so it's just really split up and not
[05:18] exactly the way that we wanted it. And
[05:20] by the way, if you guys download this
[05:21] template and you want to get set up, all
[05:22] you'll have to do is go to Apify and put
[05:24] in your own Aify API key right here.
[05:27] When you get to Appify, make sure to use
[05:28] code 30 Nate Herk for 30% off for 3
[05:31] months. And then you'll come down here
[05:32] to your settings. You'll click on API
[05:34] and integrations. And then right here is
[05:35] where you'll copy your personal API key
[05:38] and then you just have to paste it in
[05:39] like I said right here. Just make sure
[05:41] to keep that key private because it's
[05:42] kind of like your password. So from
[05:44] there, what I decided to do was I wanted
[05:46] to clean it up and get one string of the
[05:48] entire full context. So I decided to use
[05:50] a code node for this. Now what I did
[05:52] here in this code node just to show you
[05:54] guys my thought process. I come here and
[05:56] I realize, okay, I have this incoming
[05:58] JSON and I want to use code to clean it
[06:00] up. What I'm going to do is I'm just
[06:02] going to copy a bit of this JSON right
[06:04] here. just so I can understand the
[06:05] schema. And then I go over to Claude and
[06:07] I say, "Help me write a code node in
[06:09] Nitn that will receive this incoming
[06:11] JSON schema." I paste in the schema and
[06:13] then I say, I need it to split out all
[06:15] of the text as one giant string because
[06:17] we're processing a transcript. So I want
[06:19] all of the text to flow together as one
[06:21] string in one item. It spits out the
[06:23] code. I paste it in. I try it out and
[06:25] what do you know? We get one item, which
[06:27] is the entire transcript in one string.
[06:30] So that's exactly what we wanted. But
[06:31] that's not exactly the information that
[06:33] I wanted to actually vectorize and chunk
[06:35] up because just like we saw in that
[06:37] visualization, we then wouldn't know the
[06:40] timestamps of every area that got
[06:42] chunked up. So what I decided to do was
[06:45] another code node and this time I wanted
[06:47] to actually split it up but keep the
[06:49] timestamps relative to each chunk of
[06:52] transcript. So I did the exact same
[06:54] thing here. I copied the schema. I went
[06:55] into my claude and I basically said what
[06:58] I want to do now is lump together 20
[06:59] data objects at a time and I want the
[07:02] timestamps to actually stay relevant to
[07:04] each chunk. So that may not make a lot
[07:06] of sense. I'd rather just come into here
[07:07] and show you guys what I mean by that.
[07:09] So the transcript node outputs different
[07:12] objects called data and each data object
[07:15] has a start time, a duration, and then
[07:17] the actual text in that little object.
[07:20] So, what I'm basically telling Claude to
[07:22] do is I want to lump together 20 data
[07:24] objects at a time. And then when I lump
[07:26] together these data objects, I want you
[07:28] to collect and combine all of the text
[07:30] fields together. But in order to keep
[07:32] the time stamp relevant, I want to get
[07:34] the first start time from that first
[07:37] data object. And then from the last one,
[07:39] what I want to do is get the start time
[07:41] plus the duration, which would equal the
[07:42] end time. And then you can see on this
[07:44] right hand side, we get 25 different
[07:46] chunks of transcript. And each chunk has
[07:49] a formatted start time and a formatted
[07:52] end time. So this is why we're getting
[07:53] our nice chunks to vectorize that have
[07:56] our actual timestamps with them. So that
[07:58] worked beautifully. I'm now merging it
[08:00] all together and then we're setting it
[08:01] into subbase. And the only really thing
[08:03] that's special going on here is in the
[08:05] default data loader. This is where we're
[08:07] telling it what data to vectorize. And
[08:08] this is also where we're giving it the
[08:10] three metadata fields. Video title,
[08:12] timestamp, and video URL. So the first
[08:14] thing is the data to actually vectorize.
[08:17] I didn't want to do all input data. We
[08:19] wanted to load very specific data which
[08:21] would just be the text that actually got
[08:23] split up with timestamps. We didn't want
[08:25] to vectorize the entire combined text.
[08:28] So then for each of these chunks, we
[08:29] wanted to give the metadata. So for
[08:30] video title, I just went to the form
[08:32] submission and I dragged in the video
[08:34] title. For the video URL, I did the same
[08:36] thing from here. Dragged in the video
[08:37] URL. But then for time stamp, what I had
[08:39] to do was just go to our merge node
[08:41] where we pulled it all together. And
[08:42] then I wanted to pull in the formatted
[08:43] start time. And then I manually put a
[08:45] dash. And then I pulled in the formatted
[08:47] end time. So now we're getting like a
[08:49] range of 40 seconds for each of our
[08:51] chunks. And now that we have everything
[08:52] in our vector database and enriched,
[08:54] we're writing it into our Google sheet.
[08:56] So we can now see once again, here are
[08:57] the videos that exist in our vector
[08:59] database. And by the way, if you guys
[09:00] want to get a better feel for it and
[09:01] download this workflow, you can do so
[09:03] for free. You just have to join my free
[09:05] school community. The link for that will
[09:06] be down in the description. You'll then
[09:07] search for the title of this video or
[09:09] you can click on YouTube resources and
[09:11] then the post associated with this video
[09:13] will have the workflow right here to
[09:15] download. There will also be a link
[09:16] right here to this Google sheet template
[09:18] if you want to just import the exact
[09:19] same one so you don't have to set up
[09:21] anything else. Okay, cool. So now what
[09:23] we can do is chat with our agent and see
[09:25] if it actually can access that new
[09:27] video. So I'm going to send off this
[09:28] query that asks what does anthropics
[09:30] team say about building agents
[09:32] effectively. We'll see it look through
[09:34] Subbase and hopefully it's able to pull
[09:37] insights from that new video that we
[09:39] just put into our database. So, it just
[09:41] responded. Enthropics team shares
[09:43] several insights about building agents
[09:44] effectively. Here's the first tip from
[09:46] this video from the time stamp 12 to 42
[09:49] with our YouTube video link. You can see
[09:51] we get some more insights down here and
[09:52] this is from way later in that video.
[09:54] So, 1745 to 1816. And then, of course,
[09:57] it gives us a few more points, but you
[09:58] can see it's pulling from the right
[09:59] video and it's also showing us exactly
[10:01] where it got that data. All right. All
[10:02] right. So, now that you guys have seen
[10:04] how that works, before we get to the
[10:05] pipeline down here to actually
[10:07] automatically delete transcripts from
[10:09] our vector database, I wanted to talk
[10:11] about now that we have two YouTube
[10:13] videos in our vector database. What if
[10:15] we wanted to make a search and say, I
[10:17] only want you to look through this one
[10:19] video. Well, this is where we would use
[10:21] metadata filtering because we can
[10:22] literally say, search through our vector
[10:24] database for this, but only pull back
[10:26] rows where the video title equals this.
[10:29] So, in order to do that, I have a
[10:30] different setup over here, which is our
[10:32] rag agent with a form submission in
[10:34] order to do the metadata filtering. So,
[10:35] we talked to this one through a form
[10:37] submission. I'm going to open up the
[10:38] form right here, and it's going to ask
[10:40] us for a YouTube video to look through,
[10:42] and then we can give it a query. So, I'm
[10:44] saying that I only want to look through
[10:45] that tips for building AI agents video
[10:47] by anthropic. And I just want three key
[10:49] takeaways. So, I'll send that off. Our
[10:51] agent's going to take that search
[10:52] through Superbase only for chunks where
[10:55] video title equals what we just put in
[10:57] there. and then it's going to give us an
[10:59] answer. And also, this one isn't going
[11:00] to answer in the chat because we talked
[11:02] to it through a form submission. So, I
[11:03] just have to click into the agent to
[11:04] read the answer. So, here we go. Here
[11:06] are three key takeaways from the video.
[11:08] Tips for building AI agents. The first
[11:09] one is when building agents, it's
[11:11] important to design your product so that
[11:12] as the AI models improve, your product
[11:14] improves as well. The second tip is
[11:16] about developers and it comes from 517
[11:18] in that video. And then the last tip
[11:20] comes from that same video, of course,
[11:21] and from the 12second mark. So, just
[11:23] remember this isn't perfect. And if you
[11:25] really did want like three key takeaways
[11:27] or a big summary, you probably wouldn't
[11:29] want to take a vector search approach
[11:32] because the chunks still don't have
[11:34] holistic context of all the chunks
[11:36] combined. But the point I was trying to
[11:37] make here is this is how you can
[11:39] actually control which chunks you're
[11:40] looking through. Because in this subbase
[11:42] vector store node, we have a metadata
[11:44] filter. And this is where we're saying
[11:46] only pull back chunks if video title
[11:48] equals this dynamic input, which in our
[11:51] case is whatever we entered in that form
[11:53] submission. All right. And then the
[11:54] final part is we have this little
[11:56] pipeline to delete things if we don't
[11:57] want them in our vector database
[11:59] anymore. So let's say we're tired of
[12:01] chatting with this YouTube video from
[12:03] Nate and we want to get it out of our
[12:05] vector database. All we would do is we'd
[12:07] come in here and we would change the
[12:08] status to remove. And then this workflow
[12:10] is going to fire off if it's an active
[12:12] workflow because we updated a row. It's
[12:14] going to make sure it's only going to
[12:15] process rows where the status equals
[12:17] remove. So, let me just fire this off
[12:18] and show you guys. And then we set up
[12:20] the loop just in case you go in there
[12:22] and you mark off like five as it'll go
[12:24] ahead and process all five. So, you can
[12:27] see that was really quick. It already
[12:28] finished up. And if I go to that Google
[12:30] sheet, you can see that this now is
[12:32] marked as removed. And if I go to our
[12:34] vector database and we refresh this
[12:36] thing, we should see that this first
[12:38] chunk in here will not have the YouTube
[12:41] title of everything I learned about
[12:43] building agents, that video that just
[12:45] deleted. So now our vector database is
[12:47] up to date. So just to show you guys how
[12:49] this works, the trigger goes off when a
[12:50] row is updated, but it's still going to
[12:52] pull in everything on that sheet. So
[12:54] then what we do is we move into a filter
[12:56] and we just say if the status column
[12:58] equals remove, then we're going to keep
[13:00] it. And that's why it kept this video,
[13:02] but it didn't keep the tips for building
[13:03] AI agents. So, we're only going to
[13:05] actually remove the one that we marked
[13:07] as removed. Then we have the loop. We
[13:09] only have one item in this case, so it
[13:10] doesn't matter. But then we're doing a
[13:11] set just to make sure we set the URL of
[13:14] the video because that's what we're
[13:15] going to use as our metadata filter to
[13:17] delete vectors. So, we have our URL
[13:20] right here of the YouTube video. And
[13:22] then we go into a subbase node where
[13:23] we're going to delete a row. We're
[13:25] deleting it from that table that we had
[13:26] set up. And then we're basically saying
[13:28] only delete rows where in the metadata
[13:31] the field called video URL equals this,
[13:34] which is the dynamic
[13:36] YouTube video URL that we just set. And
[13:38] so it comes back to us and it says,
[13:40] okay, we found 32 vectors where the
[13:43] video URL metadata field equaled this
[13:46] string. And so we're just going to
[13:47] delete them all. It goes ahead and it
[13:49] deletes them. And then we update in our
[13:51] Google sheet that this URL was marked as
[13:55] removed. So, that's the full system I
[13:57] wanted to share with you guys today. If
[13:58] you download this template for free,
[14:00] you'll be able to play around with all
[14:01] of this and just test it out and see,
[14:03] you know, how could you start to use
[14:05] metadata for your specific use case
[14:07] because it's always going to be
[14:08] different for different types of data.
[14:10] But the key really is this rag pipeline
[14:13] right here where you're going to process
[14:15] different types of inputs, but for each
[14:17] input, you should know exactly what you
[14:18] want. So, in this case, I knew whenever
[14:20] I get a transcript, I want the full
[14:22] thing. I want the timestamps and then
[14:23] I'm going to know exactly what fields
[14:26] are going to be metadata. And so once
[14:27] you get that pipeline figured out where
[14:29] you're taking data, you're preparing it,
[14:30] you're standardizing it, everything's
[14:32] predictable, then you can really start
[14:33] to enrich all of this data in a database
[14:36] to pull it back. And if you're looking
[14:37] to deep dive into some of this stuff and
[14:39] you're looking for a more hands-on
[14:40] experience, then I definitely recommend
[14:41] checking out my paid community. The link
[14:43] for that is also down in the
[14:44] description. We've got a great community
[14:46] of members who are always sharing and
[14:47] building with an end every single day.
[14:49] And we have two full courses. One's
[14:51] called Agent Zero, which is the
[14:52] foundations for AI automation. And then
[14:54] we have 10 hours to 10 seconds, where
[14:56] you learn how to identify, design, and
[14:58] build time-saving automations. So, I'd
[15:00] love to see you guys in this community.
[15:01] But that's going to do it for this one.
[15:03] Hopefully, you all enjoyed or learned
[15:04] something new. If you did, I'd really
[15:06] appreciate a like. Helps me out a ton.
[15:08] And as always, I appreciate you guys
[15:09] making it to the end of the video. I'll
[15:11] see you all in the next one. Thanks
[15:13] guys.